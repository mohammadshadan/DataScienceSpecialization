---
title: "R - Correlation and Regression"
author: "MOHAMMAD SHADAN"
date: "May 20, 2017"
output: html_document
---

getwd()
```{r}
library(ggplot2)
library(dplyr)
```


### Exploring Anscombe

In 1973, Francis Anscombe famously created four datasets with remarkably similar numerical properties, but obviously different graphic relationships. The Anscombe dataset contains the x and y coordinates for these four datasets, along with a grouping variable set that distinguishes the quartet.

It may be helpful to remind yourself of the graphic relationship by viewing the four scatterplots:

ggplot(data = Anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)
Instructions
For each of the four sets of data points in the Anscombe dataset, compute the following in the order specified. Don't worry about naming any of the variables other than the first in your call to summarize().

 - Number of observations N
 - Mean of x
 - Standard deviation of x
 - Mean of y
 - Standard deviation of y
 - Correlation coefficient between x and y

```{r}
getwd()

Anscombe <- read.table("Anscombe.txt", header=TRUE)
head(Anscombe)
str(Anscombe)

library(ggplot2)
ggplot(data = Anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)

```

```{r}
# Compute properties of Anscombe
Anscombe %>%
  group_by(set) %>%
  summarize(N = n(), mean(x), sd(x), mean(y), sd(y), cor(x,y))
```

Perception of correlation (2)
100xp
Estimating the value of the correlation coefficient between two quantities from their scatterplot can be tricky. Statisticians have shown that people's perception of the strength of these relationships can be influenced by design choices like the x and y scales.

Nevertheless, with some practice your perception of correlation will improve. Toggle through the four scatterplots in the plotting window, each of which you've seen in a previous exercise. Jot down your best estimate of the value of the correlation coefficient between each pair of variables. Then, compare these values to the actual values you compute in this exercise.

Instructions
Each graph in the plotting window corresponds to an instruction below. Compute the correlation between...

OBP and SLG for all players in the mlbBat10 dataset.
OBP and SLG for all players in the mlbBat10 dataset with at least 200 at-bats.
Height and weight for each sex in the bdims dataset.
Body weight and brain weight for all species of mammals. Alongside this computation, compute the correlation between the same two quantities after taking their natural logarithms.

```{r}
# Correlation for all baseball players
mlbBat10 %>%
  summarize(N = n(), r = cor(OBP, SLG))

# Correlation for all players with at least 200 ABs
mlbBat10 %>%
  filter(AB >= 200) %>%
  summarize(N = n(), r = cor(OBP, SLG))

# Correlation of body dimensions
bdims %>%
  group_by(sex) %>%
  summarize(N = n(), r = cor(hgt, wgt))

# Correlation among mammals, with and without log
mammals %>%
  summarize(N = n(), 
            r = cor(BodyWt, BrainWt), 
            r_log = cor(log(BodyWt), log(BrainWt)))
```

## Interpretation of correlation - Video


Spurious correlation in random data
100xp
Statisticians must always be skeptical of potentially spurious correlations. Human beings are very good at seeing patterns in data, sometimes when the patterns themselves are actually just random noise. To illustrate how easy it can be to fall into this trap, we will look for patterns in truly random data.

The noise dataset contains 20 sets of x and y variables drawn at random from a standard normal distribution. Each set, denoted as z, has 50 observations of x-y pairs. Do you see any pairs of variables that might be meaningfully correlated? Are all of the correlation coefficients close to zero?

Instructions
Create a faceted scatterplot that shows the relationship between each of the 20 sets of pairs of random variables x and y. You will need the facet_wrap() function for this.
Compute the actual correlation between each of the 20 sets of pairs of x and y.
Identify the datasets that show non-trivial correlation of greater than 0.2 in absolute value.

```{r}
noise <- read.table("noise.txt", header = TRUE)
head(noise)

# Create faceted scatterplot
ggplot(noise, aes(x,y)) + geom_point() + facet_wrap(~z)



# Compute correlations for each dataset
noise_summary <- noise %>%
  group_by(z) %>%
  summarize(N = n(), spurious_cor = cor(x, y))

# Isolate sets with correlations above 0.2 in absolute strength
noise_summary %>%
  filter(abs(spurious_cor) > 0.2)
```

## Visualization of linear models - Video

The "best fit" line
100xp
The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a "best fit" line that cuts through the data in a way that minimizes the distance between the line and the data points.

We might consider linear regression to be a specific example of a larger class of smooth models. The geom_smooth() function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the data space. The method argument to geom_smooth() allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we'll set this argument to the value "lm".

Note that geom_smooth() also takes an se argument that controls the standard error, which we will ignore for now.

Instructions
Create a scatterplot of body weight as a function of height for all individuals in the bdims dataset.

```{r}
# Scatterplot with regression line
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

Uniqueness of least squares regression line
100xp
The least squares criterion implies that the slope of the regression line is unique. In practice, the slope is computed by R. In this exercise, you will experiment with trying to find the optimal value for the regression slope for weight as a function of height in the bdims dataset via trial-and-error.

To help, we've built a custom function for you called add_line(), which takes a single argument: the proposed slope coefficient.

Instructions
The bdims dataset is available in your workspace. Experiment with different values (to the nearest integer) of the my_slope parameter until you find one that you think fits best.

```{r}
library(readxl)
bdims <- read_excel("bdims.xlsx", sheet=1)
head(bdims)
dim(bdims)

mlbBat10 <- read_excel("mlbBat10.xlsx", sheet=1)
dim(mlbBat10)
names(mlbBat10)
mammals <- read_excel("mammals.xlsx", sheet=1)
dim(mammals)
names(mammals)
```


```{r}
add_line <- function (my_slope) {

  bdims_summary <- bdims %>%
    summarize(N = n(), r = cor(hgt, wgt),
              mean_hgt = mean(hgt), mean_wgt = mean(wgt),
              sd_hgt = sd(hgt), sd_wgt = sd(wgt)) %>%
    mutate(true_slope = r * sd_wgt / sd_hgt, 
           true_intercept = mean_wgt - true_slope * mean_hgt)
  p <- ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_point(data = bdims_summary, 
               aes(x = mean_hgt, y = mean_wgt), 
               color = "red", size = 3)
  
  my_data <- bdims_summary %>%
    mutate(my_slope = my_slope, 
           my_intercept = mean_wgt - my_slope * mean_hgt)
  p + geom_abline(data = my_data, 
                  aes(intercept = my_intercept, slope = my_slope), color = "dodgerblue")
}
```

```{r}
# Estimate optimal value of my_slope
add_line(my_slope = 1)
```

## Understanding the linear model - Viedo



Fitting a linear model "by hand"
100xp
Recall the simple linear regression model:
Y=b0+b1???X
Y=b0+b1???X
Two facts enable you to compute the slope b1b1 and intercept b0b0 of a simple linear regression model from some basic summary statistics.

First, the slope can be defined as:

b1=r(X,Y)*???sY/sX

where rX,YrX,Y represents the correlation (cor()) of XX and YY and sXsX and sYsY represent the standard deviation (sd()) of XX and YY, respectively.

Second, the point (x¯,y¯)(x¯,y¯) is always on the least squares regression line, where x¯x¯ and y¯y¯ denote the average of xx and yy, respectively.

The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (YY) as a function of height (XX). You might need to do some algebra to solve for b0b0!

Instructions
Print the bdims_summary data frame.
Use mutate() to add the slope and intercept to the bdims_summary data frame.

```{r}
#   N         r mean_hgt   sd_hgt mean_wgt   sd_wgt
# 507 0.7173011 171.1438 9.407205 69.14753 13.34576

bdims_summary <- read.table("clipboard", header = TRUE)
bdims_summary
```

```{r}
# Print bdims_summary
print(bdims_summary)

# Add slope and intercept
bdims_summary %>%
  mutate(slope = r*sd_wgt/sd_hgt, 
         intercept = mean_wgt - (r*sd_wgt/sd_hgt)*mean_hgt)
```

Regression to the mean
100xp
Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While "regression to the mean" and "linear regression" are not the same thing, we will examine them together in this exercise.

One way to see the effects of regression to the mean is to compare the heights of parents to their children's heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches.

The Galton_men and Galton_women datasets contain data originally collected by Galton himself in the 1880s on the heights of men and women, respectively, along with their parents' heights.

Compare the slope of the regression line to the slope of the diagonal line. What does this tell you?

Instructions
Create a scatterplot of the height of men as a function of their father's height. Add the simple linear regression line and a diagonal line (with slope equal to 1 and intercept equal to 0) to the plot.
Create a scatterplot of the height of women as a function of their mother's height. Add the simple linear regression line and a diagonal line to the plot.

```{r}
library(readxl)
Galton_men <- read_excel("Galton_men.xlsx", sheet=1)
dim(Galton_men)
Galton_women <- read_excel("Galton_women.xlsx", sheet=1)
dim(Galton_women)
head(Galton_men)
head(Galton_women)
```

```{r}
# Height of children vs. height of father
ggplot(data = Galton_men, aes(x = father, y = height)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm", se = FALSE)

# Height of children vs. height of mother
ggplot(data = Galton_women, aes(x = mother, y = height)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm", se = FALSE)
```

### "Regression" in the parlance of our time

In an opinion piece about nepotism published in The New York Times in 2015, economist Seth Stephens-Davidowitz wrote that:

"Regression to the mean is so powerful that once-in-a-generation talent basically never sires once-in-a-generation talent. It explains why Michael Jordan's sons were middling college basketball players and Jakob Dylan wrote two good songs. It is why there are no American parent-child pairs among Hall of Fame players in any major professional sports league."
The author is arguing that...

Possible Answers
Because of regression to the mean, an outstanding basketball player is likely to have sons that are as good at basketball as him.
Because of regression to the mean, an outstanding basketball player is likely to have sons that are not good at basketball.
Because of regression to the mean, an outstanding basketball player is likely to have sons that are good at basketball, but not as good as him. (Correct)
Linear regression is incapable of evaluating musical or athletic talent.


## Interpretation of regression coefficients - Video

Interpretation of coefficients
50xp
Recall that the fitted model for the poverty rate of U.S. counties as a function of high school graduation rate is:

poverty^=64.594???0.591???hs_grad
poverty^=64.594???0.591???hs_grad
Which of the following is the correct interpretation of the slope coefficient?

Possible Answers
Among U.S. counties, each additional percentage point increase in the poverty rate is associated with about a 0.591 percentage point decrease in the high school graduation rate.
Among U.S. counties, each additional percentage point increase in the high school graduation rate is associated with about a 0.591 percentage point decrease in the poverty rate. (Correct)
Among U.S. counties, each additional percentage point increase in the high school graduation rate is associated with about a 0.591 percentage point increase in the poverty rate.
Among U.S. counties, a 1% increase in the high school graduation rate is associated with about a 0.591% decrease in the poverty rate.

### Interpretation in context
50xp
A politician interpreting the relationship between poverty rates and high school graduation rates implores his constituents:

If we can lower the poverty rate by 59%, we'll double the high school graduate rate in our county (i.e. raise it by 100%).
Which of the following mistakes in interpretation has the politician made?

Possible Answers
Implying that the regression model establishes a cause-and-effect relationship.
Switching the role of the response and explanatory variables.
Confusing percentage change with percentage point change.
All of the above. (Correct)
None of the above.
Take Hint (-15xp)

### Fitting SLR
100xp
While the geom_smooth(method = "lm") function is useful for drawing linear models on a scatterplot, it doesn't actually return the characteristics of the model. As suggested by that syntax, however, the function that creates linear models is lm(). This function generally takes two arguments:

A formula that specifies the model
A data argument for the data frame that contains the data you want to use to fit the model
The lm() function return a model object having class "lm". This object contains lots of information about your regression model, including the data used to fit the model, the specification of the model, the fitted values and residuals, etc.

Instructions
Using the bdims dataset, create a linear model for the weight of people as a function of their height.
Using the mlbBat10 dataset, create a linear model for SLG as a function of OBP.
Using the mammals dataset, create a linear model for the body weight of mammals as a function of their brain weight, after taking the natural log of both variables.


```{r}
# Linear model for weight as a function of height
lm(wgt ~ hgt, data = bdims)

# Linear model for SLG as a function of OBP
lm(SLG~OBP, data=mlbBat10)

# Log-linear model for body weight as a function of brain weight
lm(log(BodyWt)~log(BrainWt), data=mammals)
```

Units and scale
50xp
In the previous examples, we fit two regression models:
wgt^=???105.011+1.018???hgt
wgt^=???105.011+1.018???hgt
and
SLG^=0.009+1.110???OBP.
SLG^=0.009+1.110???OBP.
Which of the following statements is incorrect?

Possible Answers
A person who is 170 cm tall is expected to weigh about 68 kg.
Because the slope coefficient for OBPOBP is larger (1.110) than the slope coefficient for hgthgt (1.018), we can conclude that the association between OBPOBP and SLGSLG is stronger than the association between height and weight. (Incorrect)
None of the above.
Take Hint (-15xp)


### Your linear model object - Video

The lm summary output
100xp
An "lm" object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.

The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2R2, adjusted R2R2, and the residual standard error. The summary of an "lm" object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.)

mod <- lm(wgt ~ hgt, data = bdims)
Instructions
The mod object is a linear model for the weight of individuals as a function of their height, fit to the data in the bdims dataset.

Use coef() to display the coefficients of mod.
Use summary() to display the full regression output of mod.

```{r}
mod <- lm(wgt ~ hgt, data = bdims)

# Show the coefficients
coef(mod)
#coefficients(mod)

# Show the full output
summary(mod)

```

Fitted values and residuals
100xp
Once you have fit a regression model, you are often interested in the fitted values (y^iy^i) and the residuals (eiei), where ii indexes the observations. Recall that:

ei=yi???y^i

The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable.

In this exercise, we will confirm these two mathematical facts by accessing the fitted values and residuals with the fitted.values() and residuals() functions, respectively, for the following model:

mod <- lm(wgt ~ hgt, data = bdims)
Instructions
mod (defined above) is available in your workspace.

Confirm that the mean of the body weights equals the mean of the fitted values of mod.
Compute the mean of the residuals of mod.

```{r}
# Mean of weights equal to mean of fitted values?
mean(bdims$wgt) == mean(fitted.values(mod))


# Mean of the residuals
mean(residuals(mod))
```

Tidying your linear model
100xp
As you fit a regression model, there are some quantities (e.g. R2R2) that apply to the model as a whole, while others apply to each observation (e.g. y^iy^i). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.

The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.

Instructions
The same linear model from the last exercise, mod, is available in your workspace.

Load the broom package.
Create a new data frame called bdims_tidy that is the augmentation of the mod linear model.
View the bdims_tidy data frame using glimpse().

```{r}
# Load broom
# install.packages("broom")
library(broom)

# Create bdims_tidy
bdims_tidy <- augment(mod)

# Glimpse the resulting data frame
glimpse(bdims_tidy)
head(bdims_tidy)
```

Making predictions
100xp
The fitted.values() function or the augment()-ed data frame provides us with the fitted values for the observations that were in the original data. However, once we have fit the model, we may want to compute expected values for observations that were not present in the data on which the model was fit. These types of predictions are called out-of-sample.

The ben data frame contains a height and weight observation for one person. The mod object contains the fitted model for weight as a function of height for the observations in the bdims dataset. We can use the predict() function to generate expected values for the weight of new individuals. We must pass the data frame of new observations through the newdata argument.

Instructions
The same linear model, mod, is defined in your workspace.

Print ben to the console.
Use predict() with the newdata argument to compute the expected height of the individual in the ben data frame.

```{r}
ben = data.frame(wgt=74.8, hgt=182.8)
print(ben)
```
```{r}
# Print ben
print(ben)

# Predict the weight of ben
predict(mod, newdata=ben)
```

Note that the data frame ben has variables with the exact same names as those in the fitted model.

Adding a regression line to a plot manually
100xp
The geom_smooth() function makes it easy to add a simple linear regression line to a scatterplot of the corresponding variables. And in fact, there are more complicated regression models that can be visualized in the data space with geom_smooth(). However, there may still be times when we will want to add regression lines to our scatterplot manually. To do this, we will use the geom_abline() function, which takes slope and intercept arguments. Naturally, we have to compute those values ahead of time, but we already saw how to do this (e.g. using coef()).

The coefs data frame contains the model estimates retrieved from coef(). Passing this to geom_abline() as the data argument will enable you to draw a straight line on your scatterplot.

Instructions
Use geom_abline() to add a line defined in the coefs data frame to a scatterplot of weight vs. height for individuals in the bdims dataset.

```{r}
# coefs <- data.frame('(Intercept)'=coef(mod)[1], slope=coef(mod)[2])
```


```{r}
# Add the line to the scatterplot
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(data = coefs, 
              aes(intercept = `(Intercept)`, slope = hgt),  
              color = "dodgerblue")
```

### Assessing model fit - Video

### RMSE
50xp
The residual standard error reported for the regression model for poverty rate of U.S. counties in terms of high school graduation rate is 4.67. What does this mean?

Possible Answers
The typical difference between the observed poverty rate and the poverty rate predicted by the model is about 4.67 percentage points. (Correct)
The typical difference between the observed poverty rate and the poverty rate predicted by the model is about 4.67%.
The model explains about 4.67% of the variability in poverty rate among counties.
The model correctly predicted the poverty rate of 4.67% of the counties.
Take Hint (-15xp)

### Standard error of residuals

One way to assess strength of fit is to consider how far off the model is for a typical case. That is, for some observations, the fitted value will be very close to the actual value, while for others it will not. The magnitude of a typical residual can give us a sense of generally how close our estimates are.

However, recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero. Thus, it makes more sense to compute the square root of the mean squared residual, or root mean squared error (RMSE). R calls this quantity the residual standard error.

To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. Thus,

RMSE=???ie2id.f.??????????????????=SSEd.f.??????????????????
RMSE=???iei2d.f.=SSEd.f.
You can recover the residuals from mod with residuals(), and the degrees of freedom with df.residual().

Instructions
View a summary() of mod.
Compute the mean of the residuals() and verify that it is approximately zero.
Use residuals() and df.residual() to compute the root mean squared error (RMSE), a.k.a. residual standard error.

```{r}
# View summary of model
summary(mod)

# Compute the mean of the residuals
mean(residuals(mod))

# Compute RMSE
sqrt(sum(residuals(mod)^2) / df.residual(mod))
```

### Assessing simple linear model fit

Recall that the coefficient of determination (R2R2), can be computed as
R2=1???SSE/SST=1???Var(e)/Var(y),

where ee is the vector of residuals and yy is the response variable. This gives us the interpretation of R2R2 as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model.

Instructions
The bdims_tidy data frame is the result of augment()-ing the bdims data frame with the mod for wgt as a function of hgt.

Use the summary() function to view the full results of mod.
Use the bdims_tidy data frame to compute the R2R2 of mod manually using the formula above, by computing the ratio of the variance of the residuals to the variance of the response variable.

```{r}
# View model summary
summary(mod)

# Compute R-squared
bdims_tidy %>%
  summarize(var_y = var(wgt), var_e = var(.resid)) %>%
  mutate(R_squared = 1-var_e/var_y)
```

This means that 51.4% of the variability in weight is explained by height.

### Interpretation of R^2

The R2R2 reported for the regression model for poverty rate of U.S. counties in terms of high school graduation rate is 0.464.

lm(formula = poverty ~ hs_grad, data = countyComplete) %>%
  summary()
How should this result be interpreted?

Possible Answers
46.4% of the variability in high school graduate rate among U.S. counties can be explained by poverty rate.
46.4% of the variability in poverty rate among U.S. counties can be explained by high school graduation rate. (Correct)
This model is 46.4% effective.
The correlation between poverty rate and high school graduation rate is 0.464.
Take Hint (-15xp)


### Linear vs. average
100xp
The R2R2 gives us a numerical measurement of the strength of fit relative to a null model based on the average of the response variable:
y^null=y¯
y^null=y¯
This model has an R2R2 of zero because SSE=SSTSSE=SST. That is, since the fitted values (y^nully^null) are all equal to the average (y¯y¯), the residual for each observation is the distance between that observation and the mean of the response. Since we can always fit the null model, it serves as a baseline against which all other models will be compared.

In the graphic, we visualize the residuals for the null model (mod_null at left) vs. the simple linear regression model (mod_hgt at right) with height as a single explanatory variable. Try to convince yourself that, if you squared the lengths of the grey arrows on the left and summed them up, you would get a larger value than if you performed the same operation on the grey arrows on the right.

It may be useful to preview these augment()-ed models with glimpse():

glimpse(mod_null)
glimpse(mod_hgt)
Instructions
Compute the sum of the squared residuals (SSE) for the null model mod_null.
Compute the sum of the squared residuals (SSE) for the regression model mod_hgt.


```{r}
mod1 <- lm(wgt ~ 1, data = bdims)
mod_null <- augment(mod1, bdims)
glimpse(mod_null)

mod2 <- lm(wgt ~ hgt, data = bdims)
mod_hgt <- augment(mod2, bdims)
glimpse(mod_hgt)

head(bdims)
```

```{r}
# Compute SSE for null model
mod_null %>%
  summarize(SSE = var(.resid))

# Compute SSE for regression model
mod_hgt %>%
  summarize(SSE = var(.resid))

#SSE/SST = Var(e)/Var(y) because mean of residual error is zeroa as guaranteed by Least Sum of Squares 
mod_hgt %>%   summarize(SSE = var(.resid)) / mod_null %>% summarize(SSE = var(.resid))

mod_hgt %>% summarize(SSE = sum(.resid^2)) / mod_null %>%  summarize(SSE = sum(.resid^2))

0.4854791
0.4854792
```

### Leverage

The leverage of an observation in a regression model is defined entirely in terms of the distance of that observation from the mean of the explanatory variable. That is, observations close to the mean of the explanatory variable have low leverage, while observations far from the mean of the explanatory variable have high leverage. Points of high leverage may or may not be influential.

The augment() function from the broom package will add the leverage scores (.hat) to a model data frame.

Instructions
Use augment() to list the top 6 observations by their leverage scores, in descending order.


```{r}
# Rank points of high leverage
mod %>%
  augment() %>%
  arrange(desc(.hat)) %>%
  head()
```

### Influence

As noted previously, observations of high leverage may or may not be influential. The influence of an observation depends not only on its leverage, but also on the magnitude of its residual. Recall that while leverage only takes into account the explanatory variable (xx), the residual depends on the response variable (yy) and the fitted value (y^y^).

Influential points are likely to have high leverage and deviate from the general relationship between the two variables. We measure influence using Cook's distance, which incorporates both the leverage and residual of each observation.

Instructions
Use augment() to list the top 6 observations by their Cook's distance (.cooksd), in descending order.

```{r}
# Rank influential points
mod %>%
  augment() %>%
  arrange(desc(.cooksd)) %>%
  head()
```


Removing outliers
100xp
Observations can be outliers for a number of different reasons. Statisticians must always be careful-and more importantly, transparent-when dealing with outliers. Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher R2R2 is not a good enough reason!

In the mlbBat10 data, the outlier with an OBP of 0.550 is Bobby Scales, an infielder who had four hits in 13 at-bats for the Chicago Cubs. Scales also walked seven times, resulting in his unusually high OBP. The justification for removing Scales here is weak. While his performance was unusual, there is nothing to suggest that it is not a valid data point, nor is there a good reason to think that somehow we will learn more about Major League Baseball players by excluding him.

Nevertheless, we can demonstrate how removing him will affect our model.

Instructions
Use filter() to create a subset of mlbBat10 called nontrivial_players consisting of only those players with at least 10 at-bats and OBP of below 0.500.
Fit the linear model for SLG as a function of OBP for the nontrivial_players. Save the result as mod_cleaner.
View the summary() of the new model and compare the slope and R2R2 to those of mod, the original model fit to the data on all players.
Visualize the new model with ggplot() and the appropriate geom_*() functions.


```{r}
# Create nontrivial_players
nontrivial_players <- filter(mlbBat10, AB>=10, OBP<0.500)

# Fit model to new data
mod_cleaner <- lm(SLG~OBP, data=nontrivial_players)

# View model summary
summary(mod_cleaner)

# Visualize new model
ggplot(nontrivial_players, aes(OBP, SLG)) + geom_point()+ geom_smooth(method="lm", se=FALSE)
```
High leverage points
100xp
Not all points of high leverage are influential. While the high leverage observation corresponding to Bobby Scales in the previous exercise is influential, the three observations for players with OBP and SLG values of 0 are not influential.

This is because they happen to lie right near the regression anyway. Thus, while their extremely low OBP gives them the power to exert influence over the slope of the regression line, their low SLG prevents them from using it.

Instructions
The linear model, mod, is available in your workspace. Use a combination of augment(), arrange() with two arguments, and head() to find the top 6 observations with the highest leverage but the lowest Cook's distance.

```{r}
# Rank high leverage points
mod %>% augment() %>% arrange(desc(.hat),desc(.cooksd)) %>% head()

```

