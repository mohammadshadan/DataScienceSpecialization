{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'qdap' was built under R version 3.3.3\"Loading required package: qdapDictionaries\n",
      "Loading required package: qdapRegex\n",
      "Warning message:\n",
      "\"package 'qdapRegex' was built under R version 3.3.3\"Loading required package: qdapTools\n",
      "Warning message:\n",
      "\"package 'qdapTools' was built under R version 3.3.3\"Loading required package: RColorBrewer\n",
      "\n",
      "Attaching package: 'qdap'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Filter\n",
      "\n",
      "Loading required package: NLP\n",
      "\n",
      "Attaching package: 'NLP'\n",
      "\n",
      "The following object is masked from 'package:qdap':\n",
      "\n",
      "    ngrams\n",
      "\n",
      "\n",
      "Attaching package: 'tm'\n",
      "\n",
      "The following objects are masked from 'package:qdap':\n",
      "\n",
      "    as.DocumentTermMatrix, as.TermDocumentMatrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(qdap)\n",
    "library(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Jumping into text mining with bag of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick taste of text mining\n",
    "\n",
    "It is always fun to jump in with a quick and easy example. Sometimes we can find out the author's intent and main ideas just by looking at the most common words.\n",
    "\n",
    "At its heart, bag of words text mining represents a way to count terms, or n-grams, across a collection of documents. Consider the following sentences, which we've saved to text and made available in your workspace:\n",
    "\n",
    "text <- \"Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\"\n",
    "Manually counting words in the sentences above is a pain! Fortunately, the qdap package offers a better alternative. You can easily find the top 4 most frequent terms (including ties) in text by calling the freq_terms function and specifying 4.\n",
    "\n",
    "frequent_terms <- freq_terms(text, 4)\n",
    "The frequent_terms object stores all unique words and their count. You can then make a bar chart simply by calling the plot function on the frequent_terms object.\n",
    "\n",
    "plot(frequent_terms)\n",
    "Instructions\n",
    "We've created an object in your workspace called new_text containing several sentences.\n",
    "\n",
    "Load the qdap package\n",
    "\n",
    " - Print new_text to the console.\n",
    " - Create term_count consisting of the 10 most frequent terms in new_text.\n",
    " - Plot a bar chart with the results of term_count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'qdap' was built under R version 3.3.3\"Loading required package: qdapDictionaries\n",
      "Loading required package: qdapRegex\n",
      "Warning message:\n",
      "\"package 'qdapRegex' was built under R version 3.3.3\"Loading required package: qdapTools\n",
      "Warning message:\n",
      "\"package 'qdapTools' was built under R version 3.3.3\"Loading required package: RColorBrewer\n",
      "\n",
      "Attaching package: 'qdap'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Filter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(qdap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text <- \"DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'character'"
      ],
      "text/latex": [
       "'character'"
      ],
      "text/markdown": [
       "'character'"
      ],
      "text/plain": [
       "[1] \"character\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses.\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAzMzNNTU1ZWVlo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\nEBIsEBIsEBIs+H+thqOcn9fXwwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print new_text to the console\n",
    "print(new_text)\n",
    "\n",
    "# Find the 10 most frequent terms: term_count\n",
    "term_count <- freq_terms(new_text,10)\n",
    "\n",
    "# Plot term_count\n",
    "plot(term_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some text\n",
    "100xp\n",
    "Text mining begins with loading some text data into R, which we'll do with the read.csv() function. By default, read.csv() treats character strings as factor levels like Male/Female. To prevent this from happening, it's very important to use the argument stringsAsFactors = FALSE.\n",
    "\n",
    "A best practice is to examine the object you read in to make sure you know which column(s) are important. The str() function provides an efficient way of doing this. You can also count the number of documents using the nrow() function on the new object. In this example, it will tell you how many coffee tweets are in the vector.\n",
    "\n",
    "If the data frame contains columns that are not text, you may want to make a new object using only the correct column of text (e.g. some_object$column_name).\n",
    "\n",
    "A word of warning: you'll be working with real tweets from real people in this course, so you may find some mild profanity from time to time.\n",
    "\n",
    "Instructions\n",
    "Create a new object tweets using read.csv() on the file coffee.csv, which contains tweets mentioning coffee. Don't forget to add stringsAsFactors = FALSE!\n",
    "Examine the tweets object using str() to determine which column has the text you'll want to analyze.\n",
    "Print out the number of rows in the tweets data frame.\n",
    "Make a new coffee_tweets object using only the text column you identified earlier. To do so, use the $ operator and column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t1000 obs. of  1 variable:\n",
      " $ text: chr  \"@ayyytylerb that is so true drink lots of coffee\" \"RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will \"| __truncated__ \"If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns+Coffee=#nosense\"| __truncated__ \"My cute coffee mug. http://t.co/2udvMU6XIG\" ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1000"
      ],
      "text/latex": [
       "1000"
      ],
      "text/markdown": [
       "1000"
      ],
      "text/plain": [
       "[1] 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import text data\n",
    "tweets <- read.csv(\"coffee.csv\", stringsAsFactors=FALSE)\n",
    "names(tweets) <- \"text\" #my addition\n",
    "\n",
    "# View the structure of tweets\n",
    "str(tweets)\n",
    "\n",
    "# Print out the number of rows in tweets\n",
    "nrow(tweets)\n",
    "\n",
    "# Isolate text from tweets: coffee_tweets\n",
    "coffee_tweets <- tweets$text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the vector a VCorpus object (1)\n",
    "\n",
    "Recall that you've loaded your text data as a vector called coffee_tweets in the last exercise. Your next step is to convert this vector containing the text data to a corpus. As you've learned in the video, a corpus is a collection of documents, but it's also important to know that in the tm domain, R recognizes it as a data type.\n",
    "\n",
    "There are two kinds of the corpus data type, the permanent corpus, PCorpus, and the volatile corpus, VCorpus. In essence, the difference between the two has to do with how the collection of documents is stored in your computer. In this course, we will use the volatile corpus, which is held in your computer's RAM rather than saved to disk, just to be more memory efficient.\n",
    "\n",
    "To make a volatile corpus, R needs to interpret each element in our vector of text, coffee_tweets, as a document. And the tm package provides what are called Source functions to do just that! In this exercise, we'll use a Source function called VectorSource() because our text data is contained in a vector. The output of this function is called a Source object. Give it a shot!\n",
    "\n",
    "Instructions\n",
    "- Load the tm package.\n",
    "- Create a Source object from the coffee_tweets vector. Call this new object coffee_source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load tm\n",
    "library(tm)\n",
    "\n",
    "# Make a vector source: coffee_source\n",
    "coffee_source <- VectorSource(coffee_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the vector a VCorpus object (2)\n",
    "\n",
    "Now that we've converted our vector to a Source object, we pass it to another tm function, VCorpus(), to create our volatile corpus. Pretty straightforward, right?\n",
    "\n",
    "The VCorpus object is a nested list, or list of lists. At each index of the VCorpus object, there is a PlainTextDocument object, which is essentially a list that contains the actual text data (content), as well as some corresponding metadata (meta). It can help to visualize a VCorpus object to conceptualize the whole thing.\n",
    "\n",
    "For example, to examine the contents of the second tweet in coffee_corpus, you'd subset twice. Once to specify the second PlainTextDocument corresponding to the second tweet and again to extract the first (or content) element of that PlainTextDocument:\n",
    "\n",
    "coffee_corpus[[15]][1]  \n",
    "\n",
    "Instructions\n",
    "- Call the VCorpus() function on the coffee_source object to create coffee_corpus.\n",
    "- Verify coffee_corpus is a VCorpus object by printing it to the console.\n",
    "- Print the 15th element of coffee_corpus to the console to verify that it's a PlainTextDocument that contains the content and metadata of the 15th tweet. Use double bracket subsetting.\n",
    "- Print the content of the 15th tweet in coffee_corpus. Use double brackets to select the proper tweet, followed by single brackets (or the $ notation) to extract the content of that tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<VCorpus>>\n",
      "Metadata:  corpus specific: 0, document level (indexed): 0\n",
      "Content:  documents: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<<PlainTextDocument>>\n",
       "Metadata:  7\n",
       "Content:  chars: 111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>$content</strong> = '@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! #Don\\'tDrinkNShoot'"
      ],
      "text/latex": [
       "\\textbf{\\$content} = '@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! \\#Don\\textbackslash{}'tDrinkNShoot'"
      ],
      "text/markdown": [
       "**$content** = '@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! #Don\\'tDrinkNShoot'"
      ],
      "text/plain": [
       "$content\n",
       "[1] \"@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! #Don'tDrinkNShoot\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## coffee_source is already in your workspace\n",
    "\n",
    "# Make a volatile corpus: coffee_corpus\n",
    "coffee_corpus <- VCorpus(coffee_source)\n",
    "\n",
    "# Print out coffee_corpus\n",
    "print(coffee_corpus)\n",
    "\n",
    "# Print data on the 15th tweet in coffee_corpus\n",
    "coffee_corpus[[15]]\n",
    "\n",
    "# Print the content of the 15th tweet in coffee_corpus\n",
    "coffee_corpus[[15]][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Make a VCorpus from a data frame\n",
    "\n",
    "Because another common text source is a data frame, there is a Source function called DataframeSource(). The DataframeSource() function treats the entire row as a complete document, so be careful you don't pick up non-text data like customer IDs when sourcing a document this way.\n",
    "\n",
    "Instructions\n",
    "In your workspace, there's a simple data frame called example_text from which we're only interested in the second and third columns (i.e. example_text[, 2:3]).\n",
    "\n",
    "Print example_text to the console\n",
    "Create df_source using DataframeSource() on columns 2 and 3 from example_text.\n",
    "Create df_corpus by converting df_source to a corpus object.\n",
    "Print out df_corpus. Notice how many documents it contains.\n",
    "Make another object vec_source using the VectorSource() function on just the third column from example_text.\n",
    "Create vec_corpus by converting vec_source to a corpus object.\n",
    "Print out vec_corpus. Observe whether it has the same number of documents as df_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  num                             Author1               Author2\n",
    "1   1        Text mining is a great time. R is a great language\n",
    "2   2     Text analysis provides insights       R has many uses\n",
    "3   3 qdap and tm are used in text mining     DataCamp is cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_text=data.frame(num=c(1,2,3), \n",
    "                        Author1=c(\"Text mining is a great time.\",\"Text analysis provides insights\",\"qdap and tm are used in text mining\"),\n",
    "                        Author2=c(\"R is a great language\",\"R has many uses\",\"DataCamp is cool!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>num</th><th scope=col>Author1</th><th scope=col>Author2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1                                  </td><td>Text mining is a great time.       </td><td>R is a great language              </td></tr>\n",
       "\t<tr><td>2                                  </td><td>Text analysis provides insights    </td><td>R has many uses                    </td></tr>\n",
       "\t<tr><td>3                                  </td><td>qdap and tm are used in text mining</td><td>DataCamp is cool!                  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " num & Author1 & Author2\\\\\n",
       "\\hline\n",
       "\t 1                                   & Text mining is a great time.        & R is a great language              \\\\\n",
       "\t 2                                   & Text analysis provides insights     & R has many uses                    \\\\\n",
       "\t 3                                   & qdap and tm are used in text mining & DataCamp is cool!                  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "num | Author1 | Author2 | \n",
       "|---|---|---|\n",
       "| 1                                   | Text mining is a great time.        | R is a great language               | \n",
       "| 2                                   | Text analysis provides insights     | R has many uses                     | \n",
       "| 3                                   | qdap and tm are used in text mining | DataCamp is cool!                   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  num Author1                             Author2              \n",
       "1 1   Text mining is a great time.        R is a great language\n",
       "2 2   Text analysis provides insights     R has many uses      \n",
       "3 3   qdap and tm are used in text mining DataCamp is cool!    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  num                             Author1               Author2\n",
      "1   1        Text mining is a great time. R is a great language\n",
      "2   2     Text analysis provides insights       R has many uses\n",
      "3   3 qdap and tm are used in text mining     DataCamp is cool!\n",
      "<<VCorpus>>\n",
      "Metadata:  corpus specific: 0, document level (indexed): 0\n",
      "Content:  documents: 3\n",
      "<<VCorpus>>\n",
      "Metadata:  corpus specific: 0, document level (indexed): 0\n",
      "Content:  documents: 3\n"
     ]
    }
   ],
   "source": [
    "# Print example_text to the console\n",
    "print(example_text)\n",
    "\n",
    "# Create a DataframeSource on columns 2 and 3: df_source\n",
    "df_source <- DataframeSource(example_text[, 2:3])\n",
    "\n",
    "# Convert df_source to a corpus: df_corpus\n",
    "df_corpus <- VCorpus(df_source)\n",
    "\n",
    "# Examine df_corpus\n",
    "print(df_corpus)\n",
    "\n",
    "# Create a VectorSource on column 3: vec_source\n",
    "vec_source <- VectorSource(example_text$Author2)\n",
    "\n",
    "# Convert vec_source to a corpus: vec_corpus\n",
    "vec_corpus <- VCorpus(vec_source)\n",
    "\n",
    "# Examine vec_corpus\n",
    "print(vec_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preprocessing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common cleaning functions from tm\n",
    "100xp\n",
    "Now that you know two ways to make a corpus, we can focus on cleaning, or preprocessing, the text. First, we'll clean a small piece of text so you can see how it works. Then we will move on to actual corpora.\n",
    "\n",
    "In bag of words text mining, cleaning helps aggregate terms. For example, it may make sense that the words \"miner\", \"mining\" and \"mine\" should be considered one term. Specific preprocessing steps will vary based on the project. For example, the words used in tweets are vastly different than those used in legal documents, so the cleaning process can also be quite different.\n",
    "\n",
    "Common preprocessing functions include:\n",
    "\n",
    "tolower(): Make all characters lowercase\n",
    "removePunctuation(): Remove all punctuation marks\n",
    "removeNumbers(): Remove numbers\n",
    "stripWhitespace(): Remove excess whitespace\n",
    "Note that tolower() is part of base R, while the other three functions come from the tm package. Going forward, we'll load the tm and qdap for you when they are needed. Every time we introduce a new package, we'll have you load it the first time.\n",
    "\n",
    "Instructions\n",
    "- Create an object, text, containing the sentence below:\n",
    "- <b>She</b> woke up at       6 A.M. It\\'s so early!  She was only 10% awake and began drinking coffee in front of her computer.\n",
    "- Apply each of the following functions to text, simply printing results to the console:\n",
    "tolower()\n",
    "removePunctuation()\n",
    "removeNumbers()\n",
    "stripWhitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'qdap' was built under R version 3.3.3\"Loading required package: qdapDictionaries\n",
      "Loading required package: qdapRegex\n",
      "Warning message:\n",
      "\"package 'qdapRegex' was built under R version 3.3.3\"Loading required package: qdapTools\n",
      "Warning message:\n",
      "\"package 'qdapTools' was built under R version 3.3.3\"Loading required package: RColorBrewer\n",
      "\n",
      "Attaching package: 'qdap'\n",
      "\n",
      "The following objects are masked from 'package:tm':\n",
      "\n",
      "    as.DocumentTermMatrix, as.TermDocumentMatrix\n",
      "\n",
      "The following object is masked from 'package:NLP':\n",
      "\n",
      "    ngrams\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    Filter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "library(qdap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;she&lt;/b&gt; woke up at       6 a.m. it\\'s so early!  she was only 10% awake and began drinking coffee in front of her computer.'</span>"
      ],
      "text/latex": [
       "'<b>she</b> woke up at       6 a.m. it\\textbackslash{}'s so early!  she was only 10\\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;she&lt;/b&gt; woke up at       6 a.m. it\\'s so early!  she was only 10% awake and began drinking coffee in front of her computer.'</span>"
      ],
      "text/plain": [
       "[1] \"<b>she</b> woke up at       6 a.m. it's so early!  she was only 10% awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer'</span>"
      ],
      "text/latex": [
       "'bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer'</span>"
      ],
      "text/plain": [
       "[1] \"bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;She&lt;/b&gt; woke up at        A.M. It\\'s so early!  She was only % awake and began drinking coffee in front of her computer.'</span>"
      ],
      "text/latex": [
       "'<b>She</b> woke up at        A.M. It\\textbackslash{}'s so early!  She was only \\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;She&lt;/b&gt; woke up at        A.M. It\\'s so early!  She was only % awake and began drinking coffee in front of her computer.'</span>"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke up at        A.M. It's so early!  She was only % awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It\\'s so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/latex": [
       "'<b>She</b> woke up at 6 A.M. It\\textbackslash{}'s so early! She was only 10\\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It\\'s so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the object: text\n",
    "text <- \"<b>She</b> woke up at       6 A.M. It\\'s so early!  She was only 10% awake and began drinking coffee in front of her computer.\"\n",
    "\n",
    "# All lowercase\n",
    "tolower(text)\n",
    "\n",
    "# Remove punctuation\n",
    "removePunctuation(text)\n",
    "\n",
    "# Remove numbers\n",
    "removeNumbers(text)\n",
    "\n",
    "# Remove whitespace\n",
    "stripWhitespace(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning with qdap\n",
    "100xp\n",
    "The qdap package offers other text cleaning functions. Each is useful in its own way and is particularly powerful when combined with the others.\n",
    "\n",
    "bracketX(): Remove all text within brackets (e.g. \"It's (so) cool\" becomes \"It's cool\")\n",
    "replace_number(): Replace numbers with their word equivalents (e.g. \"2\" becomes \"two\")\n",
    "replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. \"Sr\" becomes \"Senior\")\n",
    "replace_contraction(): Convert contractions back to their base words (e.g. \"shouldn't\" becomes \"should not\")\n",
    "replace_symbol() Replace common symbols with their word equivalents (e.g. \"$\" becomes \"dollar\")\n",
    "Instructions\n",
    "Apply the following functions to the text object from the previous exercise:\n",
    "\n",
    "bracketX()\n",
    "replace_number()\n",
    "replace_abbreviation()\n",
    "replace_contraction()\n",
    "replace_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'She woke up at 6 A.M. It\\'s so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/latex": [
       "'She woke up at 6 A.M. It\\textbackslash{}'s so early! She was only 10\\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "'She woke up at 6 A.M. It\\'s so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/plain": [
       "[1] \"She woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at six A.M. It\\'s so early! She was only ten% awake and began drinking coffee in front of her computer.'"
      ],
      "text/latex": [
       "'<b>She</b> woke up at six A.M. It\\textbackslash{}'s so early! She was only ten\\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at six A.M. It\\'s so early! She was only ten% awake and began drinking coffee in front of her computer.'"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke up at six A.M. It's so early! She was only ten% awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 AM It\\'s so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/latex": [
       "'<b>She</b> woke up at 6 AM It\\textbackslash{}'s so early! She was only 10\\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 AM It\\'s so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke up at 6 AM It's so early! She was only 10% awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/latex": [
       "'<b>She</b> woke up at 6 A.M. it is so early! She was only 10\\% awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer.'"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke up at 6 A.M. it is so early! She was only 10% awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It\\'s so early! She was only 10 percent awake and began drinking coffee in front of her computer.'"
      ],
      "text/latex": [
       "'<b>She</b> woke up at 6 A.M. It\\textbackslash{}'s so early! She was only 10 percent awake and began drinking coffee in front of her computer.'"
      ],
      "text/markdown": [
       "'&lt;b&gt;She&lt;/b&gt; woke up at 6 A.M. It\\'s so early! She was only 10 percent awake and began drinking coffee in front of her computer.'"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke up at 6 A.M. It's so early! She was only 10 percent awake and began drinking coffee in front of her computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## text is still loaded in your workspace\n",
    "\n",
    "# Remove text within brackets\n",
    "bracketX(text)\n",
    "\n",
    "# Replace numbers with words\n",
    "replace_number(text)\n",
    "\n",
    "# Replace abbreviations\n",
    "replace_abbreviation(text)\n",
    "\n",
    "# Replace contractions\n",
    "replace_contraction(text)\n",
    "\n",
    "# Replace symbols with words\n",
    "replace_symbol(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All about stop words\n",
    "100xp\n",
    "Often there are words that are frequent but provide little information. So you may want to remove these so-called stop words. Some common English stop words include \"I\", \"she'll\", \"the\", etc. In the tm package, there are 174 stop words on this common list.\n",
    "\n",
    "In fact, when you are doing an analysis you will likely need to add to this list. In our coffee tweet example, all tweets contain \"coffee\", so it's important to pull out that word in addition to the common stop words. Leaving it in doesn't add any insight and will cause it to be overemphasized in a frequency analysis.\n",
    "\n",
    "Using the c() function allows you to add new words (separated by commas) to the stop words list. For example, the following would add \"word1\" and \"word2\" to the default list of English stop words:\n",
    "\n",
    "all_stops <- c(\"word1\", \"word2\", stopwords(\"en\"))\n",
    "Once you have a list of stop words that makes sense, you will use the removeWords() function on your text. removeWords() takes two arguments: the text object to which it's being applied and the list of words to remove.\n",
    "\n",
    "Instructions\n",
    "Print out the standard stop words using stopwords(\"en\").\n",
    "Print out text with the standard stop words removed.\n",
    "Create new_stops consisting of the standard stop words plus \"coffee\" and \"bean\".\n",
    "Print out text with your customized stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n",
      "  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n",
      " [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n",
      " [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n",
      " [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n",
      " [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n",
      " [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n",
      " [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n",
      " [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n",
      " [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n",
      " [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n",
      " [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n",
      " [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n",
      " [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n",
      " [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n",
      " [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n",
      " [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n",
      " [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n",
      " [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n",
      " [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n",
      "[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n",
      "[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n",
      "[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n",
      "[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n",
      "[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n",
      "[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n",
      "[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n",
      "[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n",
      "[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n",
      "[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n",
      "[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n",
      "[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n",
      "[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n",
      "[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n",
      "[171] \"so\"         \"than\"       \"too\"        \"very\"      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;She&lt;/b&gt; woke         6 A.M. It\\'s  early!  She   10% awake  began drinking coffee  front   computer.'</span>"
      ],
      "text/latex": [
       "'<b>She</b> woke         6 A.M. It\\textbackslash{}'s  early!  She   10\\% awake  began drinking coffee  front   computer.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;She&lt;/b&gt; woke         6 A.M. It\\'s  early!  She   10% awake  began drinking coffee  front   computer.'</span>"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking coffee  front   computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;She&lt;/b&gt; woke         6 A.M. It\\'s  early!  She   10% awake  began drinking   front   computer.'</span>"
      ],
      "text/latex": [
       "'<b>She</b> woke         6 A.M. It\\textbackslash{}'s  early!  She   10\\% awake  began drinking   front   computer.'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'&lt;b&gt;She&lt;/b&gt; woke         6 A.M. It\\'s  early!  She   10% awake  began drinking   front   computer.'</span>"
      ],
      "text/plain": [
       "[1] \"<b>She</b> woke         6 A.M. It's  early!  She   10% awake  began drinking   front   computer.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## text is preloaded into your workspace\n",
    "\n",
    "# List standard English stop words\n",
    "print(stopwords(\"en\"))\n",
    "\n",
    "# Print text without standard stop words\n",
    "removeWords(text, stopwords(\"en\"))\n",
    "\n",
    "# Add \"coffee\" and \"bean\" to the list: new_stops\n",
    "new_stops <- c(\"coffee\", \"bean\", stopwords(\"en\"))\n",
    "\n",
    "# Remove stop words from text\n",
    "removeWords(text, new_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to word stemming and stem completion\n",
    "\n",
    "Still another useful preprocessing step involves word stemming and stem completion. The tm package provides the stemDocument() function to get to a word's root. This function either takes in a character vector and returns a character vector, or takes in a PlainTextDocument and returns a PlainTextDocument.\n",
    "\n",
    "For example,\n",
    "\n",
    "stemDocument(c(\"computational\", \"computers\", \"computation\"))\n",
    "returns \"comput\" \"comput\" \"comput\". But because \"comput\" isn't a real word, we want to re-complete the words so that \"computational\", \"computers\", and \"computation\" all refer to the same word, say \"computer\", in our ongoing analysis.\n",
    "\n",
    "We can easily do this with the stemCompletion() function, which takes in a character vector and an argument for the completion dictionary. The completion dictionary can be a character vector or a Corpus object. Either way, the completion dictionary for our example would need to contain the word \"computer\" for all the words to refer to it.\n",
    "\n",
    "Instructions\n",
    "- Create a vector called complicate consisting of the words \"complicated\", \"complication\", and \"complicatedly\" in that order.\n",
    "- Store the stemmed version of complicate to an object called stem_doc.\n",
    "- Create comp_dict that contains one word, \"complicate\".\n",
    "- Create complete_text by applying stemCompletion() to stem_doc. Re-complete the words using comp_dict as the reference corpus.\n",
    "- Print complete_text to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     complic      complic      complic \n",
      "\"complicate\" \"complicate\" \"complicate\" \n"
     ]
    }
   ],
   "source": [
    "# Create complicate\n",
    "complicate <- c(\"complicated\", \"complication\",\"complicatedly\" )\n",
    "\n",
    "# Perform word stemming: stem_doc\n",
    "stem_doc <- stemDocument(complicate)\n",
    "\n",
    "# Create the completion dictionary: comp_dict\n",
    "comp_dict <- \"complicate\"\n",
    "\n",
    "# Perform stem completion: complete_text \n",
    "complete_text <- stemCompletion(stem_doc, comp_dict)\n",
    "\n",
    "# Print complete_text\n",
    "print(complete_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word stemming and stem completion on a sentence\n",
    "\n",
    "Let's consider the following sentence as our document for this exercise:\n",
    "\n",
    "\"In a complicated haste, Tom rushed to fix a new complication, too complicatedly.\"\n",
    "This sentence contains the same three forms of the word \"complicate\" that we saw in the previous exercise. The difference here is that even if you called stemDocument() on this sentence, it would return the sentence without stemming any words. Take a moment and try it out in the console. Be sure to include the punctuation marks.\n",
    "\n",
    "This happens because stemDocument() treats the whole sentence as one word. In other words, our document is a character vector of length 1, instead of length n, where n is the number of words in the document. To solve this problem, we first remove the punctation marks with the removePunctuation() function you learned a few exercises back. We then strsplit() this character vector of length 1 to length n, unlist(), then proceed to stem and re-complete.\n",
    "\n",
    "Don't worry if that was confusing. Let's go through the process step by step!\n",
    "\n",
    "Instructions\n",
    "The document text_data and the completion dictionary comp_dict are loaded in your workspace.\n",
    "\n",
    "- Remove the punctuation marks in text_data using removePunctuation() and store the result into rm_punc.\n",
    "- Call strsplit() on rm_punc with the split argument set equal to \" \". Nest this inside unlist() and store the result as n_char_vec.\n",
    "- Use stemDocument() again to perform word stemming on n_char_vec and store the result as stem_doc.\n",
    "- Print stem_doc on your console.\n",
    "- Create complete_doc by re-completing your stemmed document with stemCompletion() and using comp_dict as your reference corpus.\n",
    "- Print complete_doc to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data= \"In a complicated haste, Tom rushed to fix a new complication, too complicatedly.\"\n",
    "\n",
    "comp_dict=c(\"In\",         \"a\" ,         \"complicate\" ,\"haste\"  ,    \"Tom\",\"rush\",\"to\",\"fix\",\"new\",\"too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'In'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'complic'</li>\n",
       "\t<li>'hast'</li>\n",
       "\t<li>'Tom'</li>\n",
       "\t<li>'rush'</li>\n",
       "\t<li>'to'</li>\n",
       "\t<li>'fix'</li>\n",
       "\t<li>'a'</li>\n",
       "\t<li>'new'</li>\n",
       "\t<li>'complic'</li>\n",
       "\t<li>'too'</li>\n",
       "\t<li>'complic'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'In'\n",
       "\\item 'a'\n",
       "\\item 'complic'\n",
       "\\item 'hast'\n",
       "\\item 'Tom'\n",
       "\\item 'rush'\n",
       "\\item 'to'\n",
       "\\item 'fix'\n",
       "\\item 'a'\n",
       "\\item 'new'\n",
       "\\item 'complic'\n",
       "\\item 'too'\n",
       "\\item 'complic'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'In'\n",
       "2. 'a'\n",
       "3. 'complic'\n",
       "4. 'hast'\n",
       "5. 'Tom'\n",
       "6. 'rush'\n",
       "7. 'to'\n",
       "8. 'fix'\n",
       "9. 'a'\n",
       "10. 'new'\n",
       "11. 'complic'\n",
       "12. 'too'\n",
       "13. 'complic'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"In\"      \"a\"       \"complic\" \"hast\"    \"Tom\"     \"rush\"    \"to\"     \n",
       " [8] \"fix\"     \"a\"       \"new\"     \"complic\" \"too\"     \"complic\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          In            a      complic         hast          Tom         rush \n",
      "        \"In\"          \"a\" \"complicate\"      \"haste\"        \"Tom\"       \"rush\" \n",
      "          to          fix            a          new      complic          too \n",
      "        \"to\"        \"fix\"          \"a\"        \"new\" \"complicate\"        \"too\" \n",
      "     complic \n",
      "\"complicate\" \n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation: rm_punc\n",
    "rm_punc <- removePunctuation(text_data)\n",
    "\n",
    "# Create character vector: n_char_vec\n",
    "n_char_vec <- unlist(strsplit(rm_punc, split = ' '))\n",
    "\n",
    "# Perform word stemming: stem_doc\n",
    "stem_doc <- stemDocument(n_char_vec)\n",
    "\n",
    "# Print stem_doc\n",
    "stem_doc\n",
    "\n",
    "# Re-complete stemmed document: complete_doc\n",
    "complete_doc <- stemCompletion(stem_doc, comp_dict)\n",
    "\n",
    "# Print complete_doc\n",
    "print(complete_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply preprocessing steps to a corpus\n",
    "100xp\n",
    "The tm package provides a special function tm_map() to apply cleaning functions to a corpus. Mapping these functions to an entire corpus makes scaling the cleaning steps very easy.\n",
    "\n",
    "To save time (and lines of code) it's a good idea to use a custom function like the one displayed in the editor, since you may be applying the same functions over multiple corpora. You can probably guess what the clean_corpus() function does. It takes one argument, corpus, and applies a series of cleaning functions to it in order, then returns the final result.\n",
    "\n",
    "Notice how the tm package functions do not need content_transformer(), but base R and qdap functions do.\n",
    "\n",
    "Be sure to test your function's results. If you want to draw out currency amounts, then removeNumbers() shouldn't be used! Plus, the order of cleaning steps makes a difference. For example, if you removeNumbers() and then replace_number(), the second function won't find anything to change! Check, check, and re-check!\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Edit the custom function clean_corpus() in the sample code to apply (in order):\n",
    "    - tm's stripWhitespace()\n",
    "    - tm's removePunctuation()\n",
    "    - Base R's tolower()\n",
    "    - Add \"mug\" to the stop words list c(stopwords(\"en\"), \"coffee\")\n",
    "- Create clean_corp by applying clean_corpus() to the included corpus tweet_corp.\n",
    "- Print the cleaned 227th tweet in clean_corp using indexing [[227]][1].\n",
    "- Compare it to the original tweet from tweets$text tweet using [227]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#My Codes\n",
    "tweets <- read.csv(\"tweets.csv\")\n",
    "names(tweets) <- \"text\"\n",
    "tweet_corp <- VCorpus(VectorSource(tweets$text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>text</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>@ayyytylerb that is so true drink lots of coffee                                                                                            </td></tr>\n",
       "\t<tr><td>RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?</td></tr>\n",
       "\t<tr><td>If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns+Coffee=#nosense @MomsDemand </td></tr>\n",
       "\t<tr><td>My cute coffee mug. http://t.co/2udvMU6XIG                                                                                                  </td></tr>\n",
       "\t<tr><td>RT @slaredo21: I wish we had Starbucks here... Cause coffee dates in the morning sound perff!                                               </td></tr>\n",
       "\t<tr><td>Does anyone ever get a cup of coffee before a cocktail??                                                                                    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " text\\\\\n",
       "\\hline\n",
       "\t @ayyytylerb that is so true drink lots of coffee                                                                                            \\\\\n",
       "\t RT @bryzy\\_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?\\\\\n",
       "\t If you believe in \\#gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns+Coffee=\\#nosense @MomsDemand \\\\\n",
       "\t My cute coffee mug. http://t.co/2udvMU6XIG                                                                                                  \\\\\n",
       "\t RT @slaredo21: I wish we had Starbucks here... Cause coffee dates in the morning sound perff!                                               \\\\\n",
       "\t Does anyone ever get a cup of coffee before a cocktail??                                                                                    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "text | \n",
       "|---|---|---|---|---|---|\n",
       "| @ayyytylerb that is so true drink lots of coffee                                                                                             | \n",
       "| RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ? | \n",
       "| If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns+Coffee=#nosense @MomsDemand  | \n",
       "| My cute coffee mug. http://t.co/2udvMU6XIG                                                                                                   | \n",
       "| RT @slaredo21: I wish we had Starbucks here... Cause coffee dates in the morning sound perff!                                                | \n",
       "| Does anyone ever get a cup of coffee before a cocktail??                                                                                     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  text                                                                                                                                        \n",
       "1 @ayyytylerb that is so true drink lots of coffee                                                                                            \n",
       "2 RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?\n",
       "3 If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns+Coffee=#nosense @MomsDemand \n",
       "4 My cute coffee mug. http://t.co/2udvMU6XIG                                                                                                  \n",
       "5 RT @slaredo21: I wish we had Starbucks here... Cause coffee dates in the morning sound perff!                                               \n",
       "6 Does anyone ever get a cup of coffee before a cocktail??                                                                                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$content\n",
      "[1] \"also dogs arent smart enough  dip  donut      eat  part thats  dipped ladyandthetramp\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "'Also, dogs aren\\'t smart enough to dip the donut in the coffee and then eat the part that's been dipped. #ladyandthetramp'"
      ],
      "text/latex": [
       "'Also, dogs aren\\textbackslash{}'t smart enough to dip the donut in the coffee and then eat the part that's been dipped. \\#ladyandthetramp'"
      ],
      "text/markdown": [
       "'Also, dogs aren\\'t smart enough to dip the donut in the coffee and then eat the part that's been dipped. #ladyandthetramp'"
      ],
      "text/plain": [
       "[1] \"Also, dogs aren't smart enough to dip the donut in the coffee and then eat the part that's been dipped. #ladyandthetramp\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Alter the function code to match the instructions\n",
    "clean_corpus <- function(corpus){\n",
    "  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))\n",
    "  corpus <- tm_map(corpus, removePunctuation)\n",
    "  corpus <- tm_map(corpus, removeNumbers)\n",
    "  corpus <- tm_map(corpus, removeWords, c(stopwords(\"en\"), \"coffee\", \"mug\"))\n",
    "  corpus <- tm_map(corpus, content_transformer(tolower))\n",
    "  return(corpus)\n",
    "}\n",
    "\n",
    "# Apply your customized function to the tweet_corp: clean_corp\n",
    "clean_corp <- clean_corpus(tweet_corp)\n",
    "\n",
    "# Print out a cleaned up tweet\n",
    "print(clean_corp[[227]][1])\n",
    "\n",
    "# Print out the same tweet in original form\n",
    "tweets$text[227]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$content\n",
      "[1] \"my cute   httptcoudvmuxig\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clean_corp[[4]][1]) #just for verifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TDM & DTM - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Understanding TDM and DTM\n",
    "\n",
    "When should you use the term-document matrix instead of the document-term matrix?\n",
    "\n",
    "Possible Answers\n",
    "- When you have big data.\n",
    "- When you want the documents as rows and words as columns.\n",
    "- When you want the words as rows and documents as columns.(Correct)\n",
    "- When you need to store it on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a document-term matrix\n",
    "\n",
    "Hopefully you are not too tired after all this basic text mining work! Just in case, let's revisit the coffee tweets to build a document-term matrix.\n",
    "\n",
    "Beginning with the coffee.csv file, we have used common transformations to produce a clean corpus called clean_corp.\n",
    "\n",
    "The document-term matrix is used when you want to have each document represented as a row. This can be useful if you are comparing authors within rows, or the data is arranged chronologically and you want to preserve the time series.\n",
    "\n",
    "Instructions\n",
    "- Create coffee_dtm by applying DocumentTermMatrix() to clean_corp.\n",
    "- Print the information about the coffee_dtm object.\n",
    "- Create coffee_m, a matrix version of coffee_dtm, using the as.matrix() function.\n",
    "- Print the dimensions of coffee_m to the console using the dim() function. Note the number of rows and columns.\n",
    "- Look at documents 148 through 150 and terms 2587 through 2590 (i.e. [148:150, 2587:2590])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<DocumentTermMatrix (documents: 1000, terms: 3097)>>\n",
      "Non-/sparse entries: 7765/3089235\n",
      "Sparsity           : 100%\n",
      "Maximal term length: 27\n",
      "Weighting          : term frequency (tf)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1000</li>\n",
       "\t<li>3097</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1000\n",
       "\\item 3097\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1000\n",
       "2. 3097\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1000 3097"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>stand</th><th scope=col>star</th><th scope=col>starbucks</th><th scope=col>starbuckscard</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>148</th><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>149</th><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>150</th><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & stand & star & starbucks & starbuckscard\\\\\n",
       "\\hline\n",
       "\t148 & 0 & 0 & 0 & 0\\\\\n",
       "\t149 & 0 & 0 & 0 & 0\\\\\n",
       "\t150 & 0 & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | stand | star | starbucks | starbuckscard | \n",
       "|---|---|---|\n",
       "| 148 | 0 | 0 | 0 | 0 | \n",
       "| 149 | 0 | 0 | 0 | 0 | \n",
       "| 150 | 0 | 0 | 0 | 0 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     Terms\n",
       "Docs  stand star starbucks starbuckscard\n",
       "  148 0     0    0         0            \n",
       "  149 0     0    0         0            \n",
       "  150 0     0    0         0            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the dtm from the corpus: coffee_dtm\n",
    "coffee_dtm <- DocumentTermMatrix(clean_corp)\n",
    "\n",
    "# Print out coffee_dtm data\n",
    "print(coffee_dtm)\n",
    "\n",
    "# Convert coffee_dtm to a matrix: coffee_m\n",
    "coffee_m <- as.matrix(coffee_dtm)\n",
    "\n",
    "# Print the dimensions of coffee_m\n",
    "dim(coffee_m)\n",
    "\n",
    "# Review a portion of the matrix\n",
    "coffee_m[148:150, 2587:2590]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a term-document matrix\n",
    "\n",
    "You're almost done with the not-so-exciting foundational work before we get to some fun visualizations and analyses based on the concepts you've learned so far!\n",
    "\n",
    "In this exercise, you are performing a similar process but taking the transpose of the document-term matrix. In this case, the term-document matrix has terms in the first column and documents across the top as individual column names.\n",
    "\n",
    "The TDM is often the matrix used for language analysis. This is because you likely have more terms than authors or documents and life is generally easier when you have more rows than columns. An easy way to start analyzing the information is to change the matrix into a simple matrix using as.matrix() on the TDM.\n",
    "\n",
    "Instructions\n",
    "- Create coffee_tdm by applying TermDocumentMatrix() to clean_corp.\n",
    "- Print out information about the coffee_tdm object.\n",
    "- Create coffee_m by converting coffee_tdm to a matrix using as.matrix().\n",
    "- Print the dimensions of coffee_m to the console. Note the number of rows then columns.\n",
    "- Look at terms 2587 through 2590 and documents 148 through 150 (i.e. [2587:2590, 148:150])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<TermDocumentMatrix (terms: 3098, documents: 1000)>>\n",
      "Non-/sparse entries: 7772/3090228\n",
      "Sparsity           : 100%\n",
      "Maximal term length: 27\n",
      "Weighting          : term frequency (tf)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3098</li>\n",
       "\t<li>1000</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3098\n",
       "\\item 1000\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3098\n",
       "2. 1000\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3098 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>148</th><th scope=col>149</th><th scope=col>150</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>stampedeblue</th><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>stand</th><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>star</th><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>starbucks</th><td>0</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & 148 & 149 & 150\\\\\n",
       "\\hline\n",
       "\tstampedeblue & 0 & 0 & 0\\\\\n",
       "\tstand & 0 & 0 & 0\\\\\n",
       "\tstar & 0 & 0 & 0\\\\\n",
       "\tstarbucks & 0 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 148 | 149 | 150 | \n",
       "|---|---|---|---|\n",
       "| stampedeblue | 0 | 0 | 0 | \n",
       "| stand | 0 | 0 | 0 | \n",
       "| star | 0 | 0 | 0 | \n",
       "| starbucks | 0 | 0 | 0 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "              Docs\n",
       "Terms          148 149 150\n",
       "  stampedeblue 0   0   0  \n",
       "  stand        0   0   0  \n",
       "  star         0   0   0  \n",
       "  starbucks    0   0   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a TDM from clean_corp: coffee_tdm\n",
    "coffee_tdm <- TermDocumentMatrix(clean_corp)\n",
    "\n",
    "# Print coffee_tdm data\n",
    "print(coffee_tdm)\n",
    "\n",
    "# Convert coffee_tdm to a matrix: coffee_m\n",
    "coffee_m <- as.matrix(coffee_tdm)\n",
    "\n",
    "# Print the dimensions of the matrix\n",
    "dim(coffee_m)\n",
    "\n",
    "# Review a portion of the matrix\n",
    "coffee_m[2587:2590, 148:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the number of rows and columns for the TDM is just the number of columns and rows of the DTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word clouds and more interesting visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Test your understanding of text mining\n",
    "50xp\n",
    "What is a sound business reason to create a text mining visual like a word cloud?\n",
    "\n",
    "Possible Answers\n",
    "Words clouds help decision makers come to quick conclusions. (Correct)\n",
    "Visuals can be manipulated so you can lead your audience.\n",
    "Visuals are pretty and people like colorful things.\n",
    "Millions of words can be put into a word cloud so its faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent terms with tm\n",
    "\n",
    "Now that you know how to make a term-document matrix, as well as its transpose, the document-term matrix, we will use it as the basis for some analysis. In order to analyze it we need to change it to a simple matrix like we did in chapter 1 using as.matrix.\n",
    "\n",
    "Calling rowSums() on your newly made matrix aggregates all the terms used in a passage. Once you have the rowSums(), you can sort() them with decreasing = TRUE, so you can focus on the most common terms.\n",
    "\n",
    "Lastly, you can make a barplot() of the top 5 terms of term_frequency with the following code.\n",
    "\n",
    "barplot(term_frequency[1:5], col = \"#C0DE25\")\n",
    "Of course, you could take our ggplot2 course to learn how to customize the plot even more... :)\n",
    "\n",
    "Instructions\n",
    "Create coffee_m as a matrix using the term-document matrix coffee_tdm from the last chapter.\n",
    "Create term_frequency using the rowSums() function on coffee_m.\n",
    "Sort term_frequency in descending order and store the result in term_frequency.\n",
    "Use subsetting to print out the top 10 terms from term_frequency.\n",
    "Make a barplot of the top 10 terms with col = \"tan\" and las = 2 (for vertical x-axis labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>coffee</dt>\n",
       "\t\t<dd>191</dd>\n",
       "\t<dt>like</dt>\n",
       "\t\t<dd>111</dd>\n",
       "\t<dt>cup</dt>\n",
       "\t\t<dd>103</dd>\n",
       "\t<dt>shop</dt>\n",
       "\t\t<dd>69</dd>\n",
       "\t<dt>just</dt>\n",
       "\t\t<dd>67</dd>\n",
       "\t<dt>get</dt>\n",
       "\t\t<dd>62</dd>\n",
       "\t<dt>morning</dt>\n",
       "\t\t<dd>57</dd>\n",
       "\t<dt>want</dt>\n",
       "\t\t<dd>49</dd>\n",
       "\t<dt>drinking</dt>\n",
       "\t\t<dd>47</dd>\n",
       "\t<dt>can</dt>\n",
       "\t\t<dd>45</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[coffee] 191\n",
       "\\item[like] 111\n",
       "\\item[cup] 103\n",
       "\\item[shop] 69\n",
       "\\item[just] 67\n",
       "\\item[get] 62\n",
       "\\item[morning] 57\n",
       "\\item[want] 49\n",
       "\\item[drinking] 47\n",
       "\\item[can] 45\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "coffee\n",
       ":   191like\n",
       ":   111cup\n",
       ":   103shop\n",
       ":   69just\n",
       ":   67get\n",
       ":   62morning\n",
       ":   57want\n",
       ":   49drinking\n",
       ":   47can\n",
       ":   45\n",
       "\n"
      ],
      "text/plain": [
       "  coffee     like      cup     shop     just      get  morning     want \n",
       "     191      111      103       69       67       62       57       49 \n",
       "drinking      can \n",
       "      47       45 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nj3PjkSmbAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## coffee_tdm is still loaded in your workspace\n",
    "\n",
    "# Create a matrix: coffee_m\n",
    "coffee_m <- as.matrix(coffee_tdm)\n",
    "\n",
    "# Calculate the rowSums: term_frequency\n",
    "term_frequency <- rowSums(coffee_m)\n",
    "\n",
    "# Sort term_frequency in descending order\n",
    "term_frequency <- sort(term_frequency, decreasing = TRUE)\n",
    "\n",
    "# View the top 10 most common words\n",
    "term_frequency[1:10]\n",
    "\n",
    "# Plot a barchart of the 10 most common words\n",
    "barplot(term_frequency[1:10], col=\"tan\",las=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent terms with qdap\n",
    "100xp\n",
    "If you are OK giving up some control over the exact preproccessing steps, then a fast way to get frequent terms is with freq_terms() from qdap.\n",
    "\n",
    "The function accepts a text variable, which in our case is the tweets$text vector. You can specify the top number of terms to show with the top argument, a vector of stop words to remove with the stopwords argument, and the minimum character length of a word to be included with the at.least argument. qdap has its own list of stop words that differ from those in tm. Our exercise will show you how to use either and compare their results.\n",
    "\n",
    "Making a basic plot of the results is easy. Just call plot() on the freq_terms() object.\n",
    "\n",
    "Instructions\n",
    "Create frequency using the freq_terms() function on tweets$text. Include arguments to accomplish the following:\n",
    "Limit to the top 10 terms\n",
    "At least 3 letters per term\n",
    "Use \"Top200Words\" to define stop words\n",
    "Produce a plot() of the frequency object. Compare it to the plot you produced in the previous exercise.\n",
    "Create frequency2 using the freq_terms() function on tweets$text. Include the following arguments:\n",
    "Limit to the top 10 terms\n",
    "At least 3 letters per term\n",
    "Use tm::stopwords(\"english\") to define stop words\n",
    "Produce a plot() of frequency2. Compare it to the plot of frequency. Do certain words change based on the stop words criterion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAzMzNNTU1ZWVlo\naGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD///+PE7TvAAAACXBIWXMA\nABJ0AAASdAHeZh94AAARKklEQVR4nO3ciVrbSBpAUaIJWXpL/P4vO4ANGDoIWroOlHTOTLzQ\n6foqf7htWXJydQBWu3rvDcAWCAkCQoKAkCAgJAgICQJCgoCQIJCH9An244Ih1QvChyUkCAgJ\nAkKCgJAgICQICAkCQoKAkCAgJAhcMqT/wQYJCQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKA\nkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAn1I\n03T8ISR2JA9pOv0QEnsiJAisD2majgdyx/vp5PnXhcSmrQ5pOt2c30+/+PrNqnfe+xcMl9CE\ndHhLSMel3/sXDJdwuZCm82M9IbFtlwvp2T8XElsmJAg0Ib3pZIOQ2K7VIT0/zf0YkdPf7Mf6\nkP4bIbFJQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQ\nICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAj89pDqBeHDEhIEhAQBIUFASBAQEgSEBAHXkRap\nZ8XohLRIPStGJ6RF6lkxOiEtUs+K0QlpkXpWjE5Ii9SzYnRCWqSeFaMT0iL1rBidkBapZ8Xo\nhLRIPStGJ6RF6lkxOiEtUs+K0QlpkXpWjE5Ii9SzYnRCWqSeFaMT0iL1rBidkBapZ8XohLRI\nPStGJ6RF6lkxOiEtUs+K0QlpkXpWjE5Ii9SzYnRCWqSeFaMT0iL1rBidkBapZ8XohLRIPStG\nJ6RF6lkxOiEtUs+K0QlpkXpWjE5Ii9SzYnRCWqSeFaPLQ5oel37v7/YLambFdghpkWZWbIeQ\nFmlmxXasCmm6cXt3ON7f3QmJPVoT0nS6OdV09/xY1Kc77/3dfkHN8NmO1Yd201lQp/v7pd/7\nu/2C1oycLVoX0vHYTkjs3rr3SAevSHAreY/07P5+6ff+br+glVNnc9aG9PzQbhISe7T29PeT\nVyKnv9krn7VbpJ4VoxPSIvWsGJ2QFqlnxeiEtEg9K0YnpEXqWTE6IS1Sz4rRCWmRelaMTkiL\n1LNidEJapJ4VoxPSIvWsGJ2QFqlnxeiEtEg9K0YnpEXqWTE6IS1Sz4rRCWmRelaMTkiL1LNi\ndEJapJ4VoxPSIvWsGJ2QFqlnxeiEtEg9K0YnpEXqWTE6IS1Sz4rRCWmRelaMTkiL1LNidEJa\npJ4VoxPSIvWsGJ2QFqlnxeguGVK9IHxYQoKAkCAgJAgICQJCgoCQICAkCLggO68eChslpHn1\nUNgoIc2rh8JGCWlePRQ2Skjz6qGwUUKaVw+FjRLSvHoobJSQ5tVDYaOENK8eChslpHn1UNgo\nIc2rh8JGCWlePRQ2Skjz6qGwUUKaVw+FjRLSvHoobJSQ5tVDYaOENK8eChslpHn1UNgoIc2r\nh8JGCWlePRQ2Skjz6qGwUUKaVw+FjRLSvHoobJSQ5tVDYaOENK8eChslpHn1UNgoIc2rh8JG\nCWlePRQ2Skjz6qGwUUKaVw+FjSpCml5Y+r0jKCweCvuyNqTpICQQ0isWDYX9WRnSdOMmpNvb\n+2ePS793BIWV42Uvklek08vSdP+Fw6c77x1BYf2E2YXs0O5JSMel3zuCwprZsiNpSNOTYzsh\nsR/5K9LZ0u8dQWHxYNkXIc1bPFj2xXukeWtmy46sDunu9Pfh/vSd09/sk8/azauHwkYJaV49\nFDZKSPPqobBRQppXD4WNEtK8eihslJDm1UNho4Q0rx4KGyWkefVQ2CghzauHwkYJaV49FDZK\nSPPqobBRQppXD4WNEtK8eihslJDm1UNho4Q0rx4KGyWkefVQ2CghzauHwkYJaV49FDZKSPPq\nobBRQppXD4WNEtK8eihslJDm1UNho4Q0rx4KGyWkefVQ2CghzauHwkYJaV49FDZKSPPqobBR\nlwypXhA+LCFBQEgQEBIEhAQBIUFASBAQEgSGvyBb7xqWEBIEhAQBIUFASBAQEgSEBAEhQUBI\nEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFA\nSBAQEgSKkKbp10sLid0IQvp1RkJiT4QEgfUhTdPtod3x9iaqx+M8IbEf0SvSdHpwyujTHSGx\nG3VI50sLid0QEgSEBAEhQUBIEKiuIz2c/j5bWkjshs/aQUBIEBASBIQEASFBQEgQEBIEhAQB\nIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQE\ngUuGVC8IH5aQICAkCAgJAkKCgJAgICQIDH4dqd4zLCMkCAgJAkKCgJAgICQICAkCQoKAkCAg\nJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAg\nICQICAkCQoKAkCAgJAisCWm6v51u3D+fHpcWErtRhHQf0GNIn+4Iid1YdWh3ehn6d0jHpYXE\nbggJAutONkyH8/dGQmK3hASBOqRJSOzRyutIpxN3x9Pft/dekdilJKQXlhYSuyEkCKw7/T3X\nkZDYEZ+1g4CQICAkCAgJAkKCgJAgICQICAkCvw7p6tzipYXEbggJAi8f2n25/nE4/Lj+snxp\nIbEbL4b05ern8cuLSxIS+/FiSKdDup8O7eB1L4Z0fXU8tPOKBK97MaQf0/FUw/Rj8dJCYjde\nPtnw89vnq6vP338uX1pI7IYLshB4+T3S19VLC4ndeDGkafUrlJDYjxdD+uf62+LTDKelhcRu\nzFxH8hEheCshQcBZOwgICQKvXZD9tuKC7OJ/E0ZzyY8ILd4UjObFkL7ef2h18YVZIbEfr/0x\nisOKs3ZL/0UYjpAg4NAOAk42QMDpbwiMdEG23iBkfh3S569//rN+aSGxGzN/QeSX738tP647\nCIk9+XVIP//6fn081fD1j8UvTUJiP2beI/3zx9fpQ/0xiqUbgYt75WTDP1+FBK/zigSBX4f0\n489v16tPOAiJ/Xj5rN3qU+BCYj9eCmnF37D6sLSQ2A2vSBB48T3S57v3SNff//o4f4n+ml8n\nXNTsWbsvztrBm7x2HemLkOB1riNB4JXP2q054yAk9mP2099/+/Q3vIk/jwQBf0IWAkKCgJAg\nICQICAkCQoKAkCAgJAgICQJCgoCQICAkCKwMabr//6+WFhK7sT6kg5BASBDoDu1u72+cLS0k\ndiML6aynm1XvCIndqEI6tXR+mCck9iMK6djQ9OTYTkjsRxPS9PiKdLa0kNiN/D3S2dJCYjcu\nc7LhuLSQ2A2nvyHgs3YQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQB\nIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhASBS4ZULwgflpAgICQICAkCQoKAkCAg\nJAgICQLDXJCtdwclIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSE\nBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQWBRSNPblhYSuyEkCAgJAgtD\nmqa7mKa7++n0xdPTh6WFxG4sC+k2l/uCpoeQTk9vV70jJHZj+aHd9PjkaVcPSwuJ3VgZ0uOx\n3enIbhISe7QupMcf94/OlxYSu7EqpLOghMSurQ5pms6/4j0S+7Ty0G46P1fn9De75bN2EBAS\nBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQ\nEgSEBAEhQUBIEBASBIQEASFB4JIh1QvChyUkCAgJAkKCgJAgICQICAkCY1xHqrcGMSFBQEgQ\nEBIEhAQBIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQUBI\nEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQWBVSNON27vD8WY6HJ/fLy0kdmNNSI8B\nne4fqvp0R0jsxuqQDv8O6n5pIbEbQoKAkCAgJAhUJxumJycbjksLid0oTn/fnfV2+ps9Ky/I\nTk+fCon9EBIEhAQBn7WDgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQI\nCAkCQoKAkCAgJAgICQJCgoCQICAkCAgJApcMqV4QPiwhQUBIEBASBIQEASFBQEgQEBIEXJCF\ngJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJC\ngoCQICAkCAgJAkKCgJAgICQICAkCQoKAkCCQhDQ9uXtYWkjsRvWKNB2ExI4JCQKLQ5rObm7+\nd+N4d7a0kNiNVSFNj7d3Dx5elj7dERK7sfzQbnpMaDo7tHt8SRIS+yEkCKwJ6fy4TkjsmpAg\nsOL09/RwKyT2LgtpmoTEfvmsHQSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSE\nBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQeCSIdULwoclJAgICQJCgoCQ\nICAkCAgJAkKCwIe6IFvvAH4XIUFASBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFA\nSBAQEgSEBAEhQUBIEBASBIQEASFBQEgQEBIEhAQBIUFASBAQEgSEBAEhQWBNSNMrSwuJ3RAS\nBIQEgUUhTTdu7w7H+389Py0tJHZjSUjT6eZUz7+e36x6R0jsxuKQDmcBPbu/X1pI7MbCQ7vj\n3elGSOzewpMNj0dxQoIVZ+1+EZCQ2K1VJxue3z+8TB2XFhK7se709+nG6W/2rvys3bMLtEJi\nP4QEASFBwB+jgICQICAkCAgJAkKCgJAgICQICAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAkKC\ngJAgICQICAkCQoKAkCAgJAgICQJCgsAlQ6oXhA9LSBAQEgSEBAEhQUBIEBASBIQEASFBQEgQ\nEBIELhkS7McFQ6oX/B2G3PSYux5y02/YtZBuDbnpMXc95KaF9EZDbnrMXQ+5aSG90ZCbHnPX\nQ276PUKCPRISBIQEASFBQEgQiEOabrQrXtD9ZqfTgzE2/3y3I+x6Ot/0KKM+bvD5mF/aeRvS\n9LiBj+9hs9Oz5x/bs90OsuvDcKOeHjc5vWHcQhrqd/cwbkjnGx1g09NBSP/F9LjfMTb/fLdj\n7Prw5D9ZY2xaSP/FdHbcfjgMsPnnux1j1/f7HGnUQvoPxvuWfPPv7Afzy2/LD01I/8F09mCg\nzY8a0v2jITYtpLcb73f3aLyQpicPh9i0kN5serwd5Xd31EO7AUctpLeaHu+GeQf8r92Oseuz\nkIYZ9XuGNMQV63vT88vsY2z+rZfaP5b7/2gNtOlT++/yyQbYKSFBQEgQEBIEhAQBIUFASBAQ\nEgSEBAEhQUBI4/nj+urq+s83/LwP/ymcDRHSaH5MV3euX/2ZV35zfx+zHs109fXH4fDXdPXH\naz9TSL+RWQ/mz6svd/d/Xd0euP34enXX1Sma29urqx9frqZvtw+U9PsY9WC+XP19fPDPzY+f\nd4d508+nId198ZuQfiujHsyTOL7dvlO6vvr2NKTrn4c/bl+vdPQbmfVgntTx+ermsO7H1edn\nh3YPj95ni7tk1oN5Usd5Pi894rcw68E8vEc6/C2kD8SsB3N/1u7v6eu/Du1+COndmPVoHq4j\n/XN+smG6+vPw81pI78asR/Pj8/GTDTf1nJ3+/nb74PvzkHxG6LcR0nj++jrdf9bu4YLs4dt0\n9f3Ze6Q/hPT7CAkCQoKAkCAgJAgICQJCgoCQICAkCAgJAv8HaVoEjyHdUIQAAAAASUVORK5C\nYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAzMzNNTU1ZWVlo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\nIUFASBAQEgT+Dw2FNZDSSEeDAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create frequency\n",
    "frequency <- freq_terms(tweets$text,top = 10,at.least = 3,stopwords = \"Top200Words\")\n",
    "\n",
    "# Make a frequency barchart\n",
    "plot(frequency)\n",
    "\n",
    "# Create frequency2\n",
    "frequency2 <- freq_terms(tweets$text,top = 10,at.least = 3,stopwords = tm::stopwords(\"english\"))\n",
    "\n",
    "# Make a frequency2 barchart\n",
    "plot(frequency2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to toggle between the two plots on the right to examine the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to word clouds - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coffee     like      cup     shop     just      get  morning     want \n",
      "     191      111      103       69       67       62       57       49 \n",
      "drinking      can \n",
      "      47       45 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEX/AAD/TU3/aGj/fHz/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\nICQAGICQAGAAQgKAAQgJAAb+AL3raZ/Tfi4GAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## term_frequency is loaded into your workspace\n",
    "\n",
    "# Load wordcloud package\n",
    "library(wordcloud)\n",
    "\n",
    "# Print the first 10 entries in term_frequency\n",
    "print(term_frequency[1:10])\n",
    "\n",
    "# Create word_freqs\n",
    "word_freqs <- data.frame(term=names(term_frequency), num=term_frequency)\n",
    "\n",
    "# Create a wordcloud for the values in word_freqs\n",
    "wordcloud(word_freqs$term,word_freqs$num, max.words=100, colors=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Great job! But whoa, chardonnay looks huge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words and word clouds\n",
    "100xp\n",
    "Now that you are in the text mining mindset, sitting down for a nice glass of chardonnay, we need to dig deeper. In the last word cloud, \"chardonnay\" dominated the visual. It was so dominant that you couldn't draw out any other interesting insights.\n",
    "\n",
    "Let's change the stop words to include \"chardonnay\" to see what other words are common, yet were originally drowned out.\n",
    "\n",
    "The previous word cloud used the clean_corpus() function below:\n",
    "\n",
    "clean_corpus <- function(corpus){\n",
    "  corpus <- tm_map(corpus, removePunctuation)\n",
    "  corpus <- tm_map(corpus, stripWhitespace)\n",
    "  corpus <- tm_map(corpus, removeNumbers)\n",
    "  corpus <- tm_map(corpus, content_transformer(tolower))\n",
    "  corpus <- tm_map(corpus, removeWords, c(stopwords(\"en\"), \"amp\"))\n",
    "  return(corpus)\n",
    "}\n",
    "Instructions\n",
    "Edit the clean_corpus() function in the sample code by adding \"chardonnay\", \"wine\", and \"glass\" to the existing stop words.\n",
    "Create clean_chardonnay by applying your newly edited function to chardonnay_corp, which is already loaded in your workspace.\n",
    "Create chardonnay_tdm, a term-document matrix of clean_chardonnay.\n",
    "Create chardonnay_m by converting chardonnay_tdm into a matrix.\n",
    "Create a vector of terms chardonnay_words by taking the row sums of chardonnay_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add new stop words to clean_corpus()\n",
    "clean_corpus <- function(corpus){\n",
    "  corpus <- tm_map(corpus, removePunctuation)\n",
    "  corpus <- tm_map(corpus, stripWhitespace)\n",
    "  corpus <- tm_map(corpus, removeNumbers)\n",
    "  corpus <- tm_map(corpus, content_transformer(tolower))\n",
    "  corpus <- tm_map(corpus, removeWords, \n",
    "                   c(stopwords(\"en\"), \"amp\", \"chardonnay\", \"wine\", \"glass\"))\n",
    "  return(corpus)\n",
    "}\n",
    "\n",
    "# Create clean_chardonnay\n",
    "clean_chardonnay <- clean_corpus(chardonnay_corp)\n",
    "\n",
    "# Create chardonnay_tdm\n",
    "chardonnay_tdm <- TermDocumentMatrix(clean_chardonnay)\n",
    "\n",
    "# Create chardonnay_m\n",
    "chardonnay_m <- as.matrix(chardonnay_tdm)\n",
    "\n",
    "# Create chardonnay_words\n",
    "chardonnay_words <- rowSums(chardonnay_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the better word cloud\n",
    "100xp\n",
    "Now that you've added new stop words to the clean_corpus() function, let's take a look at the improved word cloud!\n",
    "\n",
    "Your results from the previous exercise are preloaded into your workspace. Let's take a look at these new results.\n",
    "\n",
    "Instructions\n",
    "We've loaded the wordcloud package for you behind the scenes and will do so for all additional exercises requiring it.\n",
    "\n",
    "- Sort the values in chardonnay_words with decreasing = TRUE.\n",
    "- Look at the top 6 words in chardonnay_words and their values.\n",
    "- Create chardonnay_freqs, a data frame with two columns:\n",
    "    - term = names(chardonnay_words)\n",
    "    - num = chardonnay_words\n",
    "- Create a wordcloud() using chardonnay_freqs$term as the words, and chardonnay_freqs$num as the frequencies. Add the parameters max.words = 50 and colors = \"red\". Review what other words pop out now that \"chardonnay\" is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the chardonnay_words in descending order\n",
    "chardonnay_words <- sort(chardonnay_words, decreasing=TRUE)\n",
    "\n",
    "# Print the 6 most frequent chardonnay terms\n",
    "chardonnay_words[1:6]\n",
    "\n",
    "# Create chardonnay_freqs\n",
    "chardonnay_freqs <- data.frame(term = names(chardonnay_words), num = chardonnay_words)\n",
    "\n",
    "# Create a wordcloud for the values in word_freqs\n",
    "wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words=50, colors=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve word cloud colors\n",
    "100xp\n",
    "So far, you have specified only a single hexidecimal color to make your word clouds. You can easily improve the appearance of a word cloud. Instead of the #AD1DA5 in the code below, you can specify a vector of colors to make certain words stand out or to fit an existing color scheme.\n",
    "\n",
    "wordcloud(chardonnay_freqs$term, \n",
    "          chardonnay_freqs$num, \n",
    "          max.words = 100, \n",
    "          colors = \"#AD1DA5\")\n",
    "To change the colors argument of the wordcloud() function, you can use a vector of named colors like c(\"chartreuse\", \"cornflowerblue\", \"darkorange\"). The function colors() will list all 657 named colors. You can also use this PDF as a reference.\n",
    "\n",
    "Instructions\n",
    "Call thecolors() function to list all basic colors.\n",
    "Create a wordcloud() using the predefined chardonnay_freqs with the colors \"grey80\", \"darkgoldenrod1\", and \"tomato\". Include the top 100 terms using max.words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the list of colors\n",
    "colors()\n",
    "\n",
    "# Print the wordcloud with the specified colors\n",
    "wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words=100, colors=c(\"grey80\", \"darkgoldenrod1\", \"tomato\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prebuilt color palettes\n",
    "100xp\n",
    "In celebration of your text mining skills, you may have had too many glasses of chardonnay while listening to Marvin Gaye. If that's the case and you find yourself unable to pick good looking colors on your own, you can use the RColorBrewer package to help. RColorBrewer color schemes are organized into three categories:\n",
    "\n",
    "Sequential: Colors ascend from light to dark in sequence\n",
    "Qualitative: Colors are chosen for their pleasing qualities together\n",
    "Diverging: Colors have two distinct color spectra with lighter colors in between\n",
    "To change the colors parameter of the wordcloud() function you can use a select a palette from RColorBrewer such as \"Greens\". The function display.brewer.all() will list all predefined color palettes. More information on ColorBrewer (the framework behind RColorBrewer) is available on its [website](http://www.colorbrewer.org/).\n",
    "\n",
    "The function brewer.pal() allows you to select colors from a palette. Specify the number of distinct colors needed (e.g. 8) and the predefined palette to select from (e.g. \"Greens\"). Often in word clouds, very faint colors are washed out so it may make sense to remove the first couple from a brewer.pal() selection, leaving only the darkest.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "green_pal <- brewer.pal(8, \"Greens\")\n",
    "green_pal <- green_pal[-(1:2)]\n",
    "Then just add that object to the wordcloud() function.\n",
    "\n",
    "wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, colors = green_pal)\n",
    "Instructions\n",
    "Call thedisplay.brewer.all() function to list all available palettes.\n",
    "Create purple_orange selecting the first 10 colors from the PuOr palette.\n",
    "Drop the first 2 colors from purple_orange.\n",
    "Create a wordcloud for chardonnay_freqs using the purple_orange palette. Include the top 100 terms using max.words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List the available colors\n",
    "display.brewer.all()\n",
    "\n",
    "# Create purple_orange\n",
    "purple_orange<- brewer.pal(10, \"PuOr\")\n",
    "\n",
    "# Drop 2 faintest colors\n",
    "purple_orange <- purple_orange[-(1:2)]\n",
    "\n",
    "# Create a wordcloud with purple_orange palette\n",
    "wordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, colors = purple_orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find common words\n",
    "\n",
    "Say you want to visualize common words across multiple documents. You can do this with commonality.cloud().\n",
    "\n",
    "Each of our coffee and chardonnay corpora is composed of many individual tweets. To treat the coffee tweets as a single document and likewise for chardonnay, you paste() together all the tweets in each corpus along with the parameter collapse = \" \". This collapses all tweets (separated by a space) into a single vector. Then you can create a vector containing the two collapsed documents.\n",
    "\n",
    "all_coffee <- paste(coffee$tweets, collapse = \" \")\n",
    "all_chardonnay <- paste(chardonnay$tweets, collapse = \" \")\n",
    "all_tweets <- c(all_coffee, all_chardonnay)\n",
    "Once you're done with these steps, you can take the same approach you've seen before to create a VCorpus() based on a VectorSource from the all_tweets object.\n",
    "\n",
    "Instructions\n",
    "- Create all_coffee by using paste() with collapse = \" \" on coffee_tweets$text.\n",
    "- Create all_chardonnay by using paste() with collapse = \" \" on chardonnay_tweets$text.\n",
    "- Create all_tweets using c() to combine all_coffee and all_chardonnay. Make all_coffee the first term.\n",
    "- Convert all_tweets to a VectorSource().\n",
    "- Create all_corpus by using VCorpus() on all_tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create all_coffee\n",
    "all_coffee <- paste(coffee_tweets$text, collapse=\" \")\n",
    "\n",
    "# Create all_chardonnay\n",
    "all_chardonnay <- paste(chardonnay_tweets$text, collapse=\" \")\n",
    "\n",
    "# Create all_tweets\n",
    "all_tweets <- c(all_coffee, all_chardonnay)\n",
    "\n",
    "# Convert to a vector source\n",
    "all_tweets <- VectorSource(all_tweets)\n",
    "\n",
    "# Create all_corpus\n",
    "all_corpus <- VCorpus(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize common words\n",
    "\n",
    "Now that you have a corpus filled with words used in both the chardonnay and coffee tweets files, you can clean the corpus, convert it into a TermDocumentMatrix, and then a matrix to prepare it for a commonality.cloud()\n",
    "\n",
    "The commonality.cloud() function accepts this matrix object, plus additional arguments like max.words and colors to further customize the plot.\n",
    "\n",
    "commonality.cloud(tdm_matrix, max.words = 100, colors = \"springgreen\")\n",
    "Instructions\n",
    "Create all_clean by applying the predefined clean_corpus() function to all_corpus.\n",
    "Create all_tdm, a TermDocumentMatrix from all_clean.\n",
    "Create all_m by converting all_tdm to a matrix object.\n",
    "Create a commonality.cloud() from all_m with colors = \"steelblue1\" and max.words = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the corpus\n",
    "all_clean <- clean_corpus(all_corpus)\n",
    "\n",
    "# Create all_tdm\n",
    "all_tdm <- TermDocumentMatrix(all_clean)\n",
    "\n",
    "# Create all_m\n",
    "all_m <- as.matrix(all_tdm)\n",
    "\n",
    "# Print a commonality cloud\n",
    "commonality.cloud(all_m, max.words = 100, colors = \"steelblue1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize dissimilar words\n",
    "100xp\n",
    "Say you want to visualize the words not in common. To do this, you can also use comparison.cloud() and the steps are quite similar with one main difference.\n",
    "\n",
    "Like when you were searching for words in common, you start by unifying the tweets into distinct corpora and combining them into their own VCorpus() object. Next apply a clean_corpus() function and organize it into a TermDocumentMatrix.\n",
    "\n",
    "To keep track of what words belong to coffee versus chardonnay, you can set the column names of the TDM like this:\n",
    "\n",
    "colnames(all_tdm) <- c(\"chardonnay\", \"coffee\")\n",
    "Lastly, convert the object to a matrix using as.matrix() for use in comparison.cloud(). For every distinct corpora passed to the comparison.cloud() you can specify a color as in colors = c(\"red\", \"yellow\", \"green\") to make the sections distinguishable.\n",
    "\n",
    "Instructions\n",
    "all_corpus is preloaded in your workspace.\n",
    "\n",
    "Create all_clean by applying the predefined clean_corpus function to all_corpus.\n",
    "Create all_tdm, a TermDocumentMatrix from all_clean.\n",
    "Use colnames() to rename each distinct corpora within all_tdm. Name the first column \"coffee\" and the second column \"chardonnay\".\n",
    "Create all_m by converting all_tdm into matrix form.\n",
    "Create a comparison.cloud() using all_m, with colors = c(\"orange\", \"blue\") and max.words = 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the corpus\n",
    "all_clean <- clean_corpus(all_corpus)\n",
    "\n",
    "# Create all_tdm\n",
    "all_tdm <- TermDocumentMatrix(all_clean)\n",
    "\n",
    "# Give the columns distinct names\n",
    "colnames(all_tdm) <- c(\"coffee\", \"chardonnay\")\n",
    "\n",
    "# Create all_m\n",
    "all_m <- as.matrix(all_tdm)\n",
    "\n",
    "# Create comparison cloud\n",
    "comparison.cloud(all_m, colors = c(\"orange\", \"blue\"), max.words = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarized tag cloud\n",
    "100xp\n",
    "A commonality.cloud() may be misleading since words could be represented disproportionately in one corpus or the other, even if they are shared. In the commonality cloud, they would show up without telling you which one of the corpora has more term occurrences. To solve this problem, we can create a pyramid.plot() from the plotrix package.\n",
    "\n",
    "Building on what you already know, we have created a simple matrix from the coffee and chardonnay tweets using all_tdm_m <- as.matrix(all_tdm). Recall that this matrix contains two columns: one for term frequency in the chardonnay corpus, and another for term frequency in the coffee corpus. So we can use the subset() function in the following way to get terms that appear one or more times in both corpora:\n",
    "\n",
    "same_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n",
    "Once you have the terms that are common to both corpora, you can create a new column in same_words that contains the absolute difference between how often each term is used in each corpus.\n",
    "\n",
    "To identify the words that differ the most between documents, we must order() the rows of same_words by the absolute difference column with decreasing = TRUE like this:\n",
    "\n",
    "same_words <- same_words[order(same_words[, 3], decreasing = TRUE), ]\n",
    "Now that same_words is ordered by the absolute difference, let's create a small data.frame() of the 20 top terms so we can pass that along to pyramid.plot():\n",
    "\n",
    "top_words <- data.frame(\n",
    "  x = same_words[1:20, 1],\n",
    "  y = same_words[1:20, 2],\n",
    "  labels = rownames(same_words[1:20, ])\n",
    ")\n",
    "Note that top_words contains columns x and y for the frequency of the top words for each of the documents, and a third column, labels, that contains the words themselves.\n",
    "\n",
    "Finally, you can create your pyramid.plot() and get a better feel for how the word usages differ by topic!\n",
    "\n",
    "pyramid.plot(top_words$x, top_words$y,\n",
    "             labels = top_words$labels, gap = 8,\n",
    "             top.labels = c(\"Chardonnay\", \"Words\", \"Coffee\"),\n",
    "             main = \"Words in Common\", laxlab = NULL, \n",
    "             raxlab = NULL, unit = NULL)\n",
    "Instructions\n",
    "Create common_words by using subset() on all_tdm_m to get terms that appear multiple times in both corpora.\n",
    "Create difference by taking the absolute difference between the columns, using the abs() function.\n",
    "Use cbind() to add difference to common_words.\n",
    "Order common_words by the difference column (i.e. common_words[, 3]) with decreasing = TRUE.\n",
    "Create top25_df with the 25 most common words. Use \"x\", \"y\" and \"labels\" as headers.\n",
    "Plot a pyramid.plot() with the parameters from top25_df. Use the pyramid.plot() code above as a template. Just change top_words to the appropriate data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create common_words\n",
    "common_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n",
    "\n",
    "# Create difference\n",
    "difference <- abs(common_words[, 1] - common_words[, 2])\n",
    "\n",
    "# Combine common_words and difference\n",
    "common_words <- cbind(common_words, difference)\n",
    "\n",
    "# Order the data frame from most differences to least\n",
    "common_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]\n",
    "\n",
    "# Create top25_df\n",
    "top25_df <- data.frame(x = common_words[1:25, 1], \n",
    "                       y = common_words[1:25, 2], \n",
    "                       labels = rownames(common_words[1:25, ]))\n",
    "\n",
    "# Create the pyramid plot\n",
    "pyramid.plot(top25_df$x, top25_df$y,\n",
    "             labels = top25_df$labels, gap = 8,\n",
    "             top.labels = c(\"Chardonnay\", \"Words\", \"Coffee\"),\n",
    "             main = \"Words in Common\", laxlab = NULL, \n",
    "             raxlab = NULL, unit = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize word networks\n",
    "100xp\n",
    "Another way to view word connections is to treat them as a network, similar to a social network. Word networks show term association and cohesion. A word of caution: these visuals can become very dense and hard to interpret visually.\n",
    "\n",
    "In a network graph, the circles are called nodes and represent individual terms, while the lines connecting the circles are called edges and represent the connections between the terms.\n",
    "\n",
    "For the over-caffeinated text miner, qdap provides a shorcut for making word networks. The word_network_plot() and word_associate() functions both make word networks easy!\n",
    "\n",
    "The sample code constructs a word network for words associated with \"Marvin\".\n",
    "\n",
    "Instructions\n",
    "- Using word_associate() sample code, change the string to \"barista\". Change the vector to coffee_tweets$text. Remember to change \"chardonnay\" to \"coffee\" in the stopwords too.\n",
    "- Change the title to \"Barista Coffee Tweet Associations\" in the sample code for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word association\n",
    "word_associate(coffee_tweets$text, match.string = c(\"barista\"), \n",
    "               stopwords = c(Top200Words, \"coffee\", \"amp\"), \n",
    "               network.plot = TRUE, cloud.colors = c(\"gray85\", \"darkred\"))\n",
    "\n",
    "# Add title\n",
    "title(main = \"Barista Coffee Tweet Associations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teaser: simple word clustering\n",
    "100xp\n",
    "In the next chapter, we cover some miscellaneous (yet very important) text mining subjects including:\n",
    "\n",
    "TDM/DTM weighting\n",
    "Dealing with TDM/DTM sparsity\n",
    "Capturing metadata\n",
    "Simple word clustering for topics\n",
    "Analysis on more than one word\n",
    "For now, let's simply create a new visual called a dendrogram from our coffee_tweets. The next chapter will explain it in detail.\n",
    "\n",
    "Instructions\n",
    "Create a dendrogram using plot() on the hc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot a dendrogram\n",
    "plot(hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance matrix and dendrogram\n",
    "100xp\n",
    "A simple way to do word cluster analysis is with a dendrogram on your term-document matrix. Once you have a TDM, you can call dist() to compute the differences between each row of the matrix.\n",
    "\n",
    "Next, you call hclust() to perform cluster analysis on the dissimilarities of the distance matrix. Lastly, you can visualize the word frequency distances using a dendrogram and plot(). Often in text mining, you can tease out some interesting insights or word clusters based on a dendrogram.\n",
    "\n",
    "Consider the table of annual rainfall that you saw in the last video. Cleveland and Portland have the same amount of rainfall, so their distance is 0. You might expect the two cities to be a cluster and for New Orleans to be on its own since it gets vastly more rain.\n",
    "\n",
    "       city rainfall\n",
    "  Cleveland    39.14\n",
    "   Portland    39.14\n",
    "     Boston    43.77\n",
    "New Orleans    62.45\n",
    "Instructions\n",
    "The data frame rain has been preloaded in your workspace.\n",
    "\n",
    "- Create dist_rain by using the dist() function on the values in the second column of rain.\n",
    "- Print the dist_rain matrix to the console.\n",
    "- Create hc by performing a cluster analysis, using hclust() on dist_rain.\n",
    "- plot() the hc object with labels = rain$city to add the city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city\trainfall\n",
    "  Cleveland\t39.14\n",
    "   Portland\t39.14\n",
    "     Boston\t43.77\n",
    "New Orleans\t62.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in read.table(file = file, header = header, sep = sep, quote = quote, :\n",
      "\"incomplete final line found by readTableHeader on 'clipboard'\""
     ]
    }
   ],
   "source": [
    "rain <- read.delim(\"clipboard\", header=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>city</th><th scope=col>rainfall</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>  Cleveland</td><td>39.14      </td></tr>\n",
       "\t<tr><td>   Portland</td><td>39.14      </td></tr>\n",
       "\t<tr><td>     Boston</td><td>43.77      </td></tr>\n",
       "\t<tr><td>New Orleans</td><td>62.45      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " city & rainfall\\\\\n",
       "\\hline\n",
       "\t   Cleveland & 39.14      \\\\\n",
       "\t    Portland & 39.14      \\\\\n",
       "\t      Boston & 43.77      \\\\\n",
       "\t New Orleans & 62.45      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "city | rainfall | \n",
       "|---|---|---|---|\n",
       "|   Cleveland | 39.14       | \n",
       "|    Portland | 39.14       | \n",
       "|      Boston | 43.77       | \n",
       "| New Orleans | 62.45       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  city        rainfall\n",
       "1   Cleveland 39.14   \n",
       "2    Portland 39.14   \n",
       "3      Boston 43.77   \n",
       "4 New Orleans 62.45   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1     2     3\n",
      "2  0.00            \n",
      "3  4.63  4.63      \n",
      "4 23.31 23.31 18.68\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAgAElEQVR4nO3diXqqzBKF4WYQEQXv/243gyhEdCtW0XTX9z7n/EkMAalibWZ0\nVwA/c77fABADggQIIEiAAIIECCBIgACCBAggSIAAggQIIEiAAIIECCBIgACCBAggSIAAggQI\nIEiAAIIECCBIgACCBAggSIAAggQIIEiAAIIECCBIgACCBAggSIAAggQIIEiAAIIECCBIgACC\nBAggSIAAggQIIEiAAIIECCBIgACCBAggSIAAggQIIEiAAIIECCBIgACCJOh8SJ1L8tPwk3Mf\nFPfw/0HcTVbUn7+VjyYOOZRbTj4u8sm5+/GDZfmcfFB/91B9/F4I0sYot5hsssRfrh8tyx8t\n7pPRuvOnb4YgbYxyS2nXR0nZXK912S7E+VU0SN2Xpuq2Gz99NwRpY5RbyKVdym/7MJf02H13\nW5bHRXrMw7Fbc/X7Ube1TP9qkbjktg/UvlSnrriP+ZGJ9LZx92foa9WO8nAZhqkPiUvLycTH\ncVWH9ofDuHFYtz9lC4Oduu3TdBh398sydWm7GiwTl328NjSJIAkpnCvnrywGqU7GQwfTII2v\njvtWaf/7+XhaVZuEhaGLx+bk9Xwb+2Pit3GNG5759d1g983T8/Dq8HNdfLddaRFBEtIucn8O\nqi0GqV0RtCujJuti9wjSmK7kNqDrh5qPp9W0y/vy0J3+AGDy+Hk2rvuBkCFJLwZrt0qzpv9H\nIZ+OOplmEIsIkhD3tFOyGCQ35O0WidvvhuW3GULWrwia5TH33z4NnVR9QLuhTsNPVfKYeD+u\ndl3m2h245uj6rcPbYKe/g6W3fw3u77V9tdvlSy/9F+maxYTiCPkwSN2ie99RGX/Xri6a4cfb\niqBaGM/928Whm2Go/Pa31WPi/c+HccOz6Ndc42CnP4PNJ/nYwjsvziAmKI6QD4N0vG2GVX8G\nmWytuVtQFsbcf/s09GSo+7CPMTfTr9e6/0XyYrBugFORudnI/swHFlEcIeltb/9hMUjXYgzB\n/MDeZI/l7xL7+HnYIFwe+mWQ/oxjzOLiYNdT+jxqgvQJiiPkw6N2bRpOw6Gw7PFaMltGXwdp\nOGq3PPR/g3RfIyWLa6T+x25TLz2UF4L0NYoj5Pw4j3T+ex6pGX7/qHV1mC2q+WwX5XWQsn64\n5aHdbB/p9Gfpz/+7j9T/djxRRZC+RnGk3K9s6HaDJlc2tP/6F/1Vdf2P6X2nZdzBaYaDaOf+\ny2Q1dTf+fM6HP1oeevhaPh+O63/74qidW15rskb6HsWR0kzOzUx3gA6zXZp2ic7q/phDdx1B\n9yfj197S4bHJPtGwvlgc+hHbydQe43pcCPjidNM4VPE4eE6QvkBxxDT3hTWdnIzpj5O54fKD\n7sfxYEN/5cLh9k11e7GY/N3d3xwtD337ertkIX9a+sc3d5iOIvsz2Hn8h6APKUH6AsURVB2S\nbmf9tgczLnmX7rK202Mrq0/P7cBEfluym6Ld5stnx8TvxhRlxXiIemno8evTRXSzN3c/hdW/\nqeppsO7l5HCp+41TgvQFimNZ88X15HiLIFnkhsuLLhkX0EkhSBZN7kH8/KZbvEOQLLrfzDG5\n7Qk/IUgmNcfuuF5yYH0khSABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAEC\nCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAg\nAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiBggyCRVcSPIAECCBIggCABAggSIIAgAQIIEiCA\nIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggS\nIMBnkBx80O+4RV6DpD9tPKHqKgiSNVRdBUGyhqqrIEjWUHUVBMkaqq6CIFlD1VUQJGuougqC\nZA1VV0GQrKHqKgiSNVRdBUGyhqqrIEjWUHUVBMkaqq6CIFlD1VUQJGuougqCZA1VV0GQrKHq\nKgiSNVRdBUGyhqqrIEjWUHUVBMkaqq6CIFlD1VUQJGuougqCZA1VV0GQrKHqKgiSNVRdBUGy\nhqqrIEjWUHUVBMkaqq6CIFlD1VUQJGuougqCZA1VV0GQrKHqKgiSNVRdBUGyhqqrIEjWUHUV\nBMkaqq6CIFlD1VUQJGuougqCZA1VV0GQrKHqKgiSNVRdhc8gwQfaoYIgWUM7VBAka2iHCoJk\nDe1QQZCsoR0qCJI1tEMFQbKGdqhYX9bzMXedvDhrTQIKaIeKtWVtUveQqUwCKmiHirVlLVxy\nuvTf1VXiCo1JQAXtULG2rIm73L+/uERjElBBO1SsLatzr34QmwRU0A4VrJGsoR0qfthHqur+\nO/aRwkI7VKwuazY5apc2KpOABtqh4ofzSEV/HinJj5xHCgntUOHnyoYybTcJU5f+J4JQQJBU\nKJXVTT39tupeS7pfkaTNESQVq8vaHJzLqttIvjz8nbnT9eLS6+k/10RAAUFSsfoSoWS40G4Y\nyZdB6oa/dIf63v8hNFByFesPf5dtmsqkX6WsCVLuKoLkAyVXsf6EbP+lTtJ6zabdpepO4rJp\n5wFBUvHrJUJNln0fpKrbKjx2f1etnDxWI0gq1pY1deNJ2DT7/lq7crgYIj2tnDrWI0gq1pa1\ndIfbd7XLuGg1ILRDxeqyFvf0VEuniiQmAQ20Q8X6sl7y8bv6QJDCQTtU+LlE6Ji+vOoByii5\nCi9BOr65fAjKKLkKL0FKurO58IMgqfASJFZEHlF7FV6ClLu3dwJCE0FS4SVIdZJx/4QvBEmF\np007DjZ4Q8lVECRrKLkKHqJvDe1QQZCsoR0q/AbpnL/+HXQQJBV+glSwj+QNJVfhJUiPHHFj\n3+YIkgpPlwidrpmr64zHcW2PIKnwdonQsV0bXXhmw/YIkgpvQaq6C1fZR9oeJVfh6Vq707V2\n6fVMkLZHyVV4CVL/yOL+4ywOC8NDFUFS4ekO2e61g3v/uUpQQZBUcGWDNbRDBUGyhnao8BSk\nKu+f/13rTx1/ECQVfoKUDVcHuYQkbY4gqfASpNJlTRekkqN22yNIKjxdItQM52I5j7Q9Sq7C\n25UNBMkTSq7CS5DS2xqp+/hLbIwgqfC5j1TxoEgPCJIKP0ft8tvtSFz8vT2CpMLjeSSX8zlj\nHhAkFVzZYA3tUEGQrKEdKjYPkpvTnzzmKLkKgmQNJVfBpp01tEOFn1vNuaHPH4Kkgg8as4ba\nq/B2iRA8IUgqvASpyfmgMW8Ikgo+H8kaSq6CIFlDyVVw+Nsa2qGCIFlDO1T4CFJT9td+Hzl0\n5wNBUuEhSFVy2z9KuI3CA4KkYvsgVc4V3cHvS+EcSdoeQVKxeZCax8f0tZFi625zBEnF5kEq\nJk/OL9xRf/KYI0gqNg9S6h5PV615itD2CJIKD/cjvfoBm6DkKgiSNZRcBZt21hAkFRxssIYg\nqeDwtzUEScX2J2RPzhWX63BCtlr8A2giSCp8XCJ0v4eCHHlAkFR4uWj12F20mnHRqhcESQW3\nUVhDO1QQJGtohwqCZA3tUEGQrKEdKgiSNbRDhZcg8VA7jwiSCj+P40qO9cKA2AJBUuElSIfu\nNNKJ00heECQVnvaRTlmbpQNXNnhAkFR4O9hQH1PnEj7fZXMESYXHo3bNgUcWe0DJVfgK0qVb\nIbmM+5E2R5BUeAlSVSTOpQW7SD4QJBW+Po0iv+hPGEsIkgo/a6Ru76hdI3EA3AOCpOLHspbt\nnk7+n020xUmcu627Nky/TR7fI0gq1pZ1ON6WDXe6vo/Dq0mcOWrnAyVX8VOQCle0W2d14crv\nJ9F0h+1SjtptjiCp+ClIyfAUoOb94+leXtlQcPGqBwRJxU9BGrfM3m+hvbjWjoPffhAkFT8F\n6TAGKfluElz97RFBUrE+SPmxrPpPCmuK90cbuB9pVwiSivVBGvTfJm9PCC1Nosq7P81ZL22P\nIKlYXdbLpSzzvD/kULw/sbowiWzIoEtI0uYIkgovVzaULmu6IJXuoD95zBEkFUpldVNPv+0O\nm88O+2E7lFyFp4tWrwTJF0quwkuQ0tsa6cIHjW2PIKnwuY9UJe+vLYIGgqTi18Pfr3aD3k8i\nv/1ZtnLqWI8gqVhb1vKnIPXnkVx+Wjlx/IAgqVh/Hin5dHVC53aFdqhYX9bLf25DEpgEFNAO\nFT+UtXSfPXeBzu0K7VCx+VG7L45SQAMlV0GQrKHkKjw+afWSuoS7+zZHkFR4C1L3wGKe2OAB\nQVLhK0ilc9yN5AVBUuEnSOfUpdwm6wdBUuEjSHXuuMjOG4KkwkOQjs4deFixNwRJxeZBqhKX\n8gB9jwiSCs4jWUPJVRAkayi5Co8nZOEF7VBBkKyhHSoIkjW0QwVBsoZ2qCBI1tAOFQTJGtqh\ngiBZQztUECRraIcKgmQN7VBBkKyhHSoIkjW0QwVBsoZ2qCBI1tAOFQTJGtqhgiBZQztUECRr\naIcKgmQN7VBBkKyhHSoIkjW0QwVBsoZ2qCBI1tAOFQTJGtqhgiBZQztUECRraIcKgmQN7VBB\nkKyhHSoIkjW0QwVBsoZ2qCBI1tAOFQTJGtqhgiBZQztUECRraIcKgmQN7VBBkKyhHSoIkjW0\nQwVBsoZ2qCBI1tAOFQTJGtqhgiBZQztUECRraIcKgmQN7VBBkKyhHSoIkjW0QwVBsoZ2qCBI\n1tAOFQTJGtqhgiBZQztUECRraIcKgmQN7VBBkKyhHSoIkjW0QwVBsoZ2qCBI1tAOFQTJGtqh\ngiBZQztUECRraIcKgmQN7VBBkKyhHSoIkjW0QwVBsoZ2qCBI1tAOFevLej7mrpMXZ61JQAHt\nULG2rE3qHjKVSUAF7VCxtqyFS06X/ru6SlyhMQmooB0q1pY1cZf79xeXaEwCKmiHirVlde7V\nD2KTgAraoYI1kjW0Q8UP+0hV3X/HPlJYaIeK1WXNJkft0kZlEtBAO1T8cB6p6M8jJfmR80gh\noR0quLLBGtqhQqmsbkpnEliHdqhgjWQN7VBBkKyhHSoIkjW0Q8X6Kxs+3g2ic7tCO1SsLWtJ\nkAJFO1SsLusleX/zhMAkoIF2qFhf1sv7C4MkJgEFtEPFD2UtJ9etKk0C8miHCo7aWUM7VBAk\na2iHCoJkDe1QQZCsoR0qCJI1tEMFQbKGdqggSNbQDhWLDwNK3j7M5JdJwDvaoWIpSLXsvXh0\nbldoh4qxrNXsGtRUYxLYBdqh4l7W6bO80/88zmTlJLAHtEPF5w9MFZkEvKMdKjhqZw3tUEGQ\nrKEdKmZlLdNP7nj9aRLwjXaomJb1qPMoOjq3K7RDxbSsiSu1JwHvaIcKjtpZQztUTMtauLef\nKiExCXhHO1TMyppnomdilyYB32iHirGsXzzwce0ksAu0QwVBsoZ2qOCErDW0QwVBsoZ2qJgf\n/r7LPnyK6reTgHe0Q8WLIDknd5MsndsV2qFiVtZDUrX/rRJ3vuafPtn7y0nAN9qhYn5CdniY\n98Vl10buLlk6tyu0Q8XiJULdN3KHwOncrtAOFfOLVsc1UkKQokU7VMw37cZ9pOJ6cp9+jthX\nk4B3tEPFrKzZePC7WyGJ3VJB53aFdqiYl7XK2xjl3WrJHZUmAc9ohwqubLCGdqggSNbQDhXz\nI95c/R0/2qGCIFlDO1SwaWcN7VBBkKyhHSqeD39fr3mtOAl4RjtUPJ+QbV9LRJNE53aFdqiY\nlrV0WdMFqXQHrUnAO9qhYn7RajNcq8pRu4jRDhV/b6MgSLGjHSqmZU1va6QLH30ZMdqhYmEf\nqRJ+mD6d2xXaoWL+yOLHbRRak4BvtEPF0m0UJ81JwDPaoYIrG6yhHSoIkjW0QwVBsoZ2qODT\nKKyhHSoIkjW0Q8W8rCqfIkvndoV2qCBI1tAOFQTJGtqhgiBZQztUECRraIcKgmQN7VBBkKyh\nHSo4j2QN7VBBkKyhHSq41s4a2qGCIFlDO1QQJGtohwqCZA3tUEGQrKEdKgiSNbRDBUGyhnao\nIEjW0A4VBMka2qGCIFlDO1QQJGtohwqCZA3tUEGQrKEdKgiSNbRDBUGyhnaoIEjW0A4VBMka\n2qHix7KWqXN5pToJyKIdKtaWdbgbPRtuTC9UJgEVtEPFT0EqXNFcr3Xx/jNn6dyu0A4VPwUp\ncU33ffP+U9Dp3K7QDhU/BWl82tDzU4fUHkmEX9EOFT8F6TAGKdGYBFTQDhXrg5Qfy8p1H4De\nFO+PNtC5XaEdKtYH6b7Z5lzSaEwCKmiHitVlvVzKMs/7Qw7F2xzRuX2hHSq4ssEa2qGCIElz\n+I3vBq5DkKQZm11xgdaPIEkzNrviAq0fQZJmbHbFBVo/giTN2OyKC7R+BEmasdkVF2j9CJI0\nY7MrLtD6ESRpxmZXXKD1I0jSjM2uuEDrR5CkGZtdcYHWjyBJMza74gKtH0GSZmx2xQVaP4Ik\nzdjsigu0fgRJmrHZFRdo/QiSNGOzKy7Q+hEkacZmV1yg9SNI0ozNrrhA60eQpBmbXXGB1o8g\nSTM2u+ICrR9BkmZsdsUFWj+CJM3Y7IoLtH4ESZqx2RUXaP0IkjRjsysu0PoRJGnGZldcoPUj\nSNKMza64QOtHkKQZm11xgdaPIEkzNrviAq0fQZJmbHbFBVo/giTN2OyKC7R+BEmasdkVF2j9\nCJI0Y7MrLtD6ESRpxmZXXKD1I0jSjM2uuEDrR5CkGZtdcYHWjyBJMza74gKtH0GSZmx2xQVa\nP4Ikzdjsigu0fgRJmrHZFRdo/QiSNGOzKy7Q+hEkacZmV1yg9SNI0ozNrrhA60eQpBmbXXGB\n1o8gSTM2u+ICrR9BkmZsdsUFWj+ChH0JdHEhSNiXQBcXgoR9CXRxIUjYl0AXF4KEfQl0cSFI\n2JdAFxeChH0JdHEhSNiXQBcXgoR9CXRxIUjYl0AXF4KEfQl0cSFI2JdAFxeChH0JdHEhSNiX\nQBcXgoR9CXRxIUjYl0AXF4KEfQl0cSFIco6pu/H9TkIWaPEIkpijcwTpd4EWjyCJSVzp+y3E\nINDFhSCJYUUkItAqEiQxuWt8v4UYBLq4ECQxdZKdfb+HCAS6uBAkMY6DDRICLR5BEkOQRARa\nPIKEfQl0cSFI2JdAFxeCJOiUtZt1+cn32whboIsLQZKT3faQMt9vJGiBLi4ESUzpkqr9UnGF\nw08CXVwIkpjUXfqvF5d6fidBC3RxIUhi7ke9Ofz9i0CLR5DEPNZIied3ErRAFxeCJIZ9JBGB\nLi4ESQ5H7SQEurgQJEGnnPNIPwt0cSFI2JdAF5f1b/t8zPsNmbz4z80DgVYGfgS6uKx92006\nudj5/U5BoJX52v2od8JRux8EurisfduFS07D0d66SlyhMYnQjEGqOY/0i0CLt/ZtJ7eTJp3/\nnDgJtDJfqdwUVzb8INDFZe3bnv2r+/xP8Gy5WjmJoEy3dNP47jh3URKt0Mq/+2KNZEXM/2JE\nOWu7CFK7j1TV/Xf/3UdC+AiS2siyySoy5TlUnbLdN6rTGLfsCJLiyM5Ffx4pyY8RLjhrVN22\nXdLVJL6CEKRNR2Zb5k79vUinCC+2i3IxIUj71K2QLt3uYoRHHeKboytB2qsuP7mrCFIoCNI+\nZe5SdScC2LQLBEHap/7qhmO3Qqp8vxVxUS4mBGmnyuGEWhrhDUlRLiYECVuLcjEhSNhalIsJ\nQdqreB9ZHOViQpB2KuKHn0S5mBCkfYr5cVxRLiYEaZ9ifmRxlIsJQdqnmB9ZHN8cXQnSXsX8\nyOIoFxOCtE/sIwWGIO0UR+3CQpD2Kt5HFke5mBAkbC3KxYQgYWtRLiYEab/OmUuKCB8FE+Vi\nQpB26NImqLxe+oMNSXxJinIxIUj7c+4TVGTJ5dpkET7mL8rFhCDtTx+eYrg3tuGEbBgI0v4M\nVwXdrg3iEqEwEKT9IUgBIkj7Q5ACRJD2hyAFiCDtj+IH7+xCfHN0JUh7RJACRJCwtSgXE4KE\nrUW5mBAkbC3KxYQgYWtRLiYECVuLcjEhSNhalIsJQcLWolxMCBK2FuViQpCwtSgXE4KErUW5\nmBAkYG8IEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAEC\nCBIggCDhlSifw6w1U5GUBwoI0jfjFRsTopQn3cd5npOD7zciSWGmCBLeKdyl/3qJ6ROmNWaK\nIOGd+8ZPLJt2HY2Ziqg8UJDc//GO6KPaNWaKIOGdwiXn9kuVuKPvtyJHY6YIEt7Kboe3ct9v\nRJLCTBEkvHfKuyWu8v02ZMnPFEECBBAkQABBAgQQJLx1TOO6RKinMFMxlQfyjrFda9fRmKmI\nygMFiSt9vwV5GjNFkPBOTCuiO42ZirFOkJO7xvdbkKcxUwQJ79RJdvb9HsRpzBRBwjvR3djX\n4cY+bI0gfTpOsTEBhhEkQABBwkfOUd1HcSM4UwQJbxUR7iNpzFRM5YG8xyIX0R1JGjNFkPBO\n4k7XzNV15iI6naQxUwQJ73QbP8f2H+6Ly3y/FTkaM0WQ8E63zFXdNZ4x7SNpzFRE5YGCvN0K\nql16PccUJI2Ziqg8UFB1y1r/0J2InlmsMVMECW8duyXk4GJ6YrHKTBEkQABBAgQQJLyi9VFC\nXmnNVCTlgQKC9M14xcYEGEaQAAEECe9VebcBlNe+34eg+xZdwucjYSPZsCfhkoiSNAapZh8J\nGyld1nSLWxnLlQ3V7FhDKjZegoR3EtcM/4DHctTumk5zxG0U2Ea/WRdVkK48aRXbS29rpIvg\nVpB3ucKFgwQJ79z2kaqoHqbPGgmby2+7ExHdINuvZqURJLxX9Z9bfPL9NiQ1Oc/+Bn7GtXbY\nWoQf6kKQsD2XRfQ8O00ECe90py+LiB5pp4Yg4a26+wTw9BjZJt4pkz6CQpDwP3WRuLg28TL5\nY/oECR8oo7lDtlO6pPtnQfQsc0TlgZJh6y6iM0mpu/RfJa97Ikh4q09RUkR0N9LkEiEOf2Mj\n3VG7Q2xH7R5rJO6QxTZcFtEm3Yh9JGwtssPeNxy1w+YifPjJ9XoSvxKXIOGtGB9+ooEg4Z3o\nHn6ihSDhnfgeftJdp5EU4vt+sZQHOqJ7+Emd9IcZxLdUIykPlET38JNDu616bTLxLVWChHei\ne/hJ0j+voRY8FTsgSHgrtoef3DZRxbdUCRLei+zhJwQJEECQAAEECVvjoy+/Ga/YmBAbgvTN\neMXGBBhGkAABBAmv1Yf+NGyTRnI2VhFBwkt14vLua6VwbVpsCBJeSt1huEr6nMVyqZ0agoRX\nKne8f5+7aK5t0EGQ8Mph8nlcdTQX2ykhSHhldpYllvNIWigPXkkI0ucoD145uMeD86vh+B1e\nIUh45fI46F0nHGx4jyDhpcIlx+7hvpdjwrGG/yBIeO14v7iTh3H9B0HCG3XRf7Tdkesa/ocg\nAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIg\ngCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQII\nEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCAB\nAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCA\nIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggS\nIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAgAQIIEiCAIAEC\nCBIggCABAggSIIAgAQIIEiCAIAECCBIggCABAggSIIAg7Zpzw/8nqo/+CBuj5rv2HKT0/x0j\nSB5Q8117zgQp2SfasmsEKRS0Za+KxBWPTbsqcy6ruh/cnyg516Qub4fInUuK4ZXu/3XukqOP\nN24TQdqprEtMPgap7APkysUgtUMV1+MwRHEdg5R0P5KkrRCkfTq55HK9JGOQEnfpXksXNu3a\nNVXTfzl1Q7jrGKT21bL7A2yCIO1T7s7tf6sxSM6NR70XgnSe/nAd/+C8NDC0UOl9ukVgDFLR\nbsBdLpNfPA15vdbVMZsEaXFgaKHS+/QnSNdjt8uT1G+ClLlx/4kgeUCl9+lvkNrNvCJ9sY/U\nfzm4tKxqguQLld6nvN8pOs+vbFi4YGgWuStB8oZK71M1P2qXDsfk+jVSPR/yHqTz9cI+kjdU\neqfybofnMGbiNOwAnbtIuWQ24C0shbsPQpA8oNJ7dXy+sqE7on1Ol4PU7iS1A1TdNQ4EyQMq\nDQggSIAAghQi9+D7rWBAI0JEkHaHRgACCBIggCABAggSIIAgAQIIkrKl42ovj7XNn1lX5f3A\nS3++5mjd37/5+4C822jz/z84D88IkrJvgjR/Zl3tmmvdtB1qni9TFQjS0wPybqNt/l4Vi08Q\nJGXfBGn+elZ0d+vlae6yp8EEgrR0P0b/WvFncvgEQVK2Okgn1z3T5HJwh3MzHyzPu//9+k6e\n3sQ42qa7ZQNfIkjK2uW1GB8wVyQuq69Pl2cvP7Mu7VcMVVK6vzstRdH9bzK+67VMXVreRnjs\nJ1fcH81VTB531w+ZlNfJxMYXHqPNePbQ9wiSsv6xc90T6W5PVUiav0Fafmbduf+T6/FwPbx6\nON19fLfnNWT9CPsH3FXZ7SF3t5+z27SG25y6H8eJ3V94KKfPJcJnCJKyxwPmTt13h2HpngZp\n+Zl1hbv8Z8yP8Y0PwTvdJzf8N+l+nvyqu/G2/UWTuWqc2OSFu0u/LsNXCJKyxwPm+kfVNcPS\nPQ3S8jPrMtc8j2zmMb7hAQ/VsKIZJvfYghx+lY/voRtrM/44e+Gu+Xt0A/9HkJRNAzN97fH6\n8jPr/n9cbj6+p9HOJjp+O7lofBxm4Spyrin/HiVT9v8gLT+zjiCFhZIp+yBIi8+s0wrSwp8u\nWuwAAAElSURBVBt7M2Z8ipIpeyyv2fM+0nkpXIP/7yNlT/tI+VKQhmeIH8Z9pPtRhXEf6fmK\nIPaRViBIyqaHubOm3SG6HbVLXdkdLXOvnllX/PcY9GN8p/mhuXmQhl/dDtP1Q7Z/mY8Tm7xw\nd+ao3fcIkrLJkj07j1TeP/9o+Zl15z+fbbSwubV8Hmk+0fbVfkKz99DvkN0m9njh7sh5pO8R\nJGXTJbs7Pjcel+4OMRyujysbnp5Zl2YL45kbx9fGMnlc2XCdB6ndepv+qmwDdKgnE7u/cMeV\nDSsQpL2q/lyFva5TK44b1Au7TfgfgrRb2WxP5XRYNZIVQeLq7zUI0m7Vs+N2Ky737nwfJO5H\nWoUg7Ve1biU0832QDmzYrUGQAAEECRBAkAAB/wDTbuQyUwEw5wAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Cluster Dendrogram\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dist_rain\n",
    "dist_rain <- dist(rain[,2])\n",
    "\n",
    "# View the distance matrix\n",
    "print(dist_rain)\n",
    "\n",
    "# Create hc\n",
    "hc <- hclust(dist_rain)\n",
    "\n",
    "# Plot hc\n",
    "plot(hc, labels=rain$city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a distance matrix and dendrogram from a TDM\n",
    "100xp\n",
    "Now that you understand the steps in making a dendrogram, you can apply them to text. But first, you have to limit the number of words in your TDM using removeSparseTerms() from tm. Why would you want to adjust the sparsity of the TDM/DTM?\n",
    "\n",
    "TDMs and DTMs are sparse, meaning they contain mostly zeros. Remember that 1000 tweets can become a TDM with over 3000 terms! You won't be able to easily interpret a dendrogram that is so cluttered, especially if you are working on more text.\n",
    "\n",
    "A good TDM has between 25 and 70 terms. The lower the sparse value, the more terms are kept. The closer it is to 1, the fewer are kept. This value is a percentage cutoff of zeros for each term in the TDM.\n",
    "\n",
    "Instructions    \n",
    "tweets_tdm has been created using the chardonnay tweets.\n",
    "\n",
    "- Print the dimensions of tweets_tdm to the console.\n",
    "- Create tdm1 using removeSparseTerms() with sparse = 0.95 on tweets_tdm.\n",
    "- Create tdm2 using removeSparseTerms() with sparse = 0.975 on tweets_tdm.\n",
    "- Print tdm1 to the console to see how many terms are left.\n",
    "- Print tdm2 to the console to see how many terms are left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the dimensions of tweets_tdm\n",
    "dim(tweets_tdm)\n",
    "\n",
    "# Create tdm1\n",
    "tdm1 <- removeSparseTerms(tweets_tdm, sparse=0.95)\n",
    "\n",
    "# Create tdm2\n",
    "tdm2 <- removeSparseTerms(tweets_tdm, sparse=0.975)\n",
    "\n",
    "# Print tdm1\n",
    "print(tdm1)\n",
    "\n",
    "# Print tdm2\n",
    "print(tdm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together: a text based dendrogram\n",
    "\n",
    "Its time to put your skills to work to make your first text-based dendrogram. Remember, dendrograms reduce information to help you make sense of the data. This is much like how an average tells you something, but not everything, about a population. Both can be misleading. With text, there are often a lot of nonsensical clusters, but some valuable clusters may also appear.\n",
    "\n",
    "A peculiarity of TDM and DTM objects is that you have to convert them first to matrices (with as.matrix()), then to data frames (with as.data.frame()), before using them with the dist() function.\n",
    "\n",
    "For the chardonnay tweets, you may have been surprised to see the soul music legend Marvin Gaye appear in the word cloud. Let's see if the dendrogram picks up the same.\n",
    "\n",
    "Instructions\n",
    "- Create tweets_tdm2 by applying removeSparseTerms() on tweets_tdm. Use sparse = 0.975.\n",
    "- Create tdm_m by using as.matrix() on tweets_tdm2 to convert it to matrix form.\n",
    "- Create tdm_df by converting tdm_m to a data frame using as.data.frame().\n",
    "- Create tweets_dist containing the distances of tdm_df using the dist() function.\n",
    "- Create a hierarchical cluster object called hc using hclust() on tweets_dist.\n",
    "- Make a dendrogram with plot() and hc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tweets_tdm2\n",
    "tweets_tdm2 <- removeSparseTerms(tweets_tdm, sparse=0.975)\n",
    "\n",
    "# Create tdm_m\n",
    "tdm_m <- as.matrix(tweets_tdm2)\n",
    "\n",
    "# Create tdm_df\n",
    "tdm_df <- as.data.frame(tdm_m)\n",
    "\n",
    "# Create tweets_dist\n",
    "tweets_dist <- dist(tdm_df)\n",
    "\n",
    "# Create hc\n",
    "hc <- hclust(tweets_dist)\n",
    "\n",
    "# Plot the dendrogram\n",
    "plot(hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrogram aesthetics\n",
    "100xp\n",
    "So you made a dendrogram...but its not as eye catching as you had hoped!\n",
    "\n",
    "The dendextend package can help your audience by coloring branches and outlining clusters. dendextend is designed to operate on dendrogram objects, so you'll have to change the hierarchical cluster from hclust using as.dendrogram().\n",
    "\n",
    "A good way to review the terms in your dendrogram is with the labels() function. It will print all terms of the dendrogram. To highlight specific branches, use branches_attr_by_labels(). First, pass in the dendrogram object, then a vector of terms as in c(\"data\", \"camp\"). Lastly add a color such as \"blue\".\n",
    "\n",
    "After you make your plot, you can call out clusters with rect.dendrogram(). This adds rectangles for each cluster. The first argument to rect.dendrogram() is the dendrogram, followed by the number of clusters (k). You can also pass a border argument specifying what color you want the rectangles to be (e.g. \"green\").\n",
    "\n",
    "Instructions\n",
    "- Load the dendextend package.\n",
    "- Create hc as a hierarchical cluster object from the tweets_dist distance matrix.\n",
    "- Create hcd as a dendrogram using as.dendrogram() on hc.\n",
    "- Print the labels of hcd to the console.\n",
    "- Recreate the hcd object using branches_attr_by_labels() with three arguments: the hcd object, c(\"marvin\", \"gaye\"), and the color \"red\".\n",
    "- plot() the dendgrogram hcd with a title main = \"Better Dendrogram\".\n",
    "- Add rectangles to the plot using rect.dendrogram(). Specify k = 2 clusters and a border color of \"grey50\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dendextend\n",
    "library(dendextend)\n",
    "\n",
    "# Create hc\n",
    "hc <- hclust(tweets_dist)\n",
    "\n",
    "# Create hcd\n",
    "hcd <- as.dendrogram(hc)\n",
    "\n",
    "# Print the labels in hcd\n",
    "labels(hcd)\n",
    "\n",
    "# Change the branch color to red for \"marvin\" and \"gaye\"\n",
    "hcd <- branches_attr_by_labels(hcd, c(\"marvin\",\"gaye\"),\"red\")\n",
    "\n",
    "# Plot hcd\n",
    "plot(hcd, main=\"Better Dendrogram\")\n",
    "\n",
    "# Add cluster rectangles \n",
    "rect.dendrogram(hcd, k=2, border=\"grey50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word association\n",
    "100xp\n",
    "Another way to think about word relationships is with the findAssocs() function in the tm package. For any given word, findAssocs() calculates its correlation with every other word in a TDM or DTM. Scores range from 0 to 1. A score of 1 means that two words always appear together, while a score of 0 means that they never appear together.\n",
    "\n",
    "To use findAssocs() pass in a TDM or DTM, the search term, and a minimum correlation. The function will return a list of all other terms that meet or exceed the minimum threshold.\n",
    "\n",
    "findAssocs(tdm, \"word\", 0.25)\n",
    "Minimum correlation values are often relatively low because of word diversity. Don't be surprised if 0.10 demonstrates a strong pairwise term association.\n",
    "\n",
    "The coffee tweets have been cleaned and organized into tweets_tdm for the exercise. You will search for a term association, and manipulate the results with list_vect2df() from qdap and then create a plot with the ggplot2 code in the example script.\n",
    "\n",
    "Instructions\n",
    "- Create associations using findAssocs() on tweets_tdm to find terms associated with \"venti\", which meet a minimum threshold of 0.2.\n",
    "- View the terms associated with \"venti\" by printing associations to the console.\n",
    "- Create associations_df, a data frame containing the result from list_vect2df(associations)[, 2:3].\n",
    "- Use the ggplot2 code to make a dot plot of the association values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create associations\n",
    "associations <- findAssocs(tweets_tdm, \"venti\", 0.2)\n",
    "\n",
    "# View the venti associations\n",
    "associations\n",
    "\n",
    "# Create associations_df\n",
    "associations_df <- list_vect2df(associations)[, 2:3]\n",
    "\n",
    "# Plot the associations_df values (don't change this)\n",
    "ggplot(associations_df, aes(y = associations_df[, 1])) + \n",
    "  geom_point(aes(x = associations_df[, 2]), \n",
    "             data = associations_df, size = 3) + \n",
    "  theme_gdocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting past single words - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram tokenization\n",
    "\n",
    "Will increasing the n-gram length increase, decrease or make no difference for the TDM or DTM size?\n",
    "\n",
    "Possible Answers\n",
    "- Increase (Correct)\n",
    "- Decrease\n",
    "- No difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing n-grams\n",
    "100xp\n",
    "So far, we have only made TDMs and DTMs using single words. The default is to make them with unigrams, but you can also focus on tokens containing two or more words. This can help extract useful phrases which lead to some additional insights or provide improved predictive attributes for a machine learning algorithm.\n",
    "\n",
    "The function below uses the RWeka package to create trigram (three word) tokens: min and max are both set to 3.\n",
    "\n",
    "tokenizer <- function(x) \n",
    "  NGramTokenizer(x, Weka_control(min = 3, max = 3))\n",
    "Then the customized tokenizer() function can be passed into the TermDocumentMatrix or DocumentTermMatrix functions as an additional parameter:\n",
    "\n",
    "tdm <- TermDocumentMatrix(\n",
    "  corpus, \n",
    "  control = list(tokenize = tokenizer)\n",
    ")\n",
    "Instructions\n",
    "A corpus has been preprocessed as before using the chardonnay tweets. The resulting object text_corp is available in your workspace.\n",
    "\n",
    "- Create a tokenizer function like the above which creates 2-word bigrams.\n",
    "- Make unigram_dtm by calling DocumentTermMatrix() on text_corp without using the tokenizer() function.\n",
    "- Make bigram_dtm using DocumentTermMatrix() on text_corp with the tokenizer() function you just made.\n",
    "- Examine the unigram_dtm and take note of the number of terms.\n",
    "- Examine the bigram_dtm and see how the number of terms increases when using bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make tokenizer function \n",
    "tokenizer <- function(x) \n",
    "  NGramTokenizer(x, Weka_control(min = 2, max = 2))\n",
    "\n",
    "# Create unigram_dtm\n",
    "unigram_dtm <- DocumentTermMatrix(text_corp)\n",
    "\n",
    "# Create bigram_dtm\n",
    "bigram_dtm <- DocumentTermMatrix(text_corp, control = list(tokenize = tokenizer))\n",
    "\n",
    "# Examine unigram_dtm\n",
    "unigram_dtm\n",
    "\n",
    "# Examine bigram_dtm\n",
    "bigram_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do bigrams affect word clouds?\n",
    "100xp\n",
    "Now that you have made a bigram DTM, you can examine it and remake a word cloud. The new tokenization method affects not only the matrices, but also any visuals or modeling based on the matrices.\n",
    "\n",
    "Remember how \"Marvin\" and \"Gaye\" were separate and large terms in the chardonnay word cloud? Using bigram tokenization grabs all two word combinations. Observe what happens to the word cloud in this exercise.\n",
    "\n",
    "Instructions\n",
    "The chardonnay tweets have been cleaned and organized into a DTM called bigram_dtm.\n",
    "\n",
    "- Create bigram_dtm_m by converting bigram_dtm to a matrix.\n",
    "- Create an object freq consisting of the word frequencies by applying colSums() on bigram_dtm_m.\n",
    "- Extract the character vector of word combinations with names(freq) and assign the result to bi_words.\n",
    "- Examine the word vector bi_words from [2577:2587] to see all 2-word combinations that include \"marvin\" as the first term.\n",
    "- Plot a simple wordcloud() passing bi_words, freq and max.words = 15 into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create bigram_dtm_m\n",
    "bigram_dtm_m <- as.matrix(bigram_dtm)\n",
    "\n",
    "# Create freq\n",
    "freq <- colSums(bigram_dtm_m)\n",
    "\n",
    "# Create bi_words\n",
    "bi_words <- names(freq)\n",
    "\n",
    "# Examine part of bi_words\n",
    "bi_words[2577:2587]\n",
    "\n",
    "# Plot a wordcloud\n",
    "wordcloud(bi_words, freq, max.words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different frequency criteria - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing frequency weights\n",
    "100xp\n",
    "So far you have used term frequency to make the DocumentTermMatrix or TermDocumentMatrix. There are other term weights that can be helpful. The most popular weight is TfIdf, which stands for term frequency-inverse document frequency.\n",
    "\n",
    "The TfIdf score increases by term occurrence but is penalized by the frequency of appearance among all documents.\n",
    "\n",
    "From a common sense perspective, if a term appears often it must be important. This attribute is represented by term frequency (i.e. Tf), which is normalized by the length of the document. However, if the term appears in all documents, it is not likely to be insightful. This is captured in the inverse document frequency (i.e. Idf).\n",
    "\n",
    "The wiki page on TfIdf contains the mathematical explanation behind the score, but the exercise will demonstrate the practical difference.\n",
    "\n",
    "Instructions\n",
    "The coffee tweets have been cleaned and organized into the corpus text_corp, but the term \"coffee\" was not removed.\n",
    "\n",
    "Create tf_tdm, a term frequency-based TermDocumentMatrix() using text_corp.\n",
    "Make tfidf_tdm using TermDocumentMatrix() on text_corp along with TfIdf weighting. Pass control = list(weighting = weightTfIdf) as an argument to the function.\n",
    "Create tf_tdm_m by converting tf_tdm to matrix form.\n",
    "Create tfidf_tdm_m calling as.matrix() on tfidf_tdm.\n",
    "Examine the term frequency for \"coffee\" in 5 tweets by examining rows 508 and 509 and columns 5 through 10 of tf_tdm_m.\n",
    "Compare the TfIdf score for \"coffee\" in 5 tweets by examining rows 508 and 509 and columns 5 through 10 of tfidf_tdm_m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tf_tdm\n",
    "tf_tdm = TermDocumentMatrix(text_corp)\n",
    "\n",
    "# Create tfidf_tdm\n",
    "tfidf_tdm = TermDocumentMatrix(text_corp, control = list(weighting = weightTfIdf))\n",
    "\n",
    "# Create tf_tdm_m\n",
    "tf_tdm_m = as.matrix(tf_tdm)\n",
    "\n",
    "# Create tfidf_tdm_m \n",
    "tfidf_tdm_m = as.matrix(tfidf_tdm)\n",
    "\n",
    "# Examine part of tf_tdm_m\n",
    "tf_tdm_m[508:509,5:10]\n",
    "\n",
    "# Examine part of tfidf_tdm_m\n",
    "tfidf_tdm_m[508:509,5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturing metadata in tm\n",
    "100xp\n",
    "Depending on what you are trying to accomplish, you may want to keep metadata about the document when you create a TDM or DTM. This metadata can be incorporated into the corpus fairly easily by creating a readerControl list and applying it to a DataframeSource when calling VCorpus().\n",
    "\n",
    "You will need to know the column names of the data frame containing the metadata to be captured. The names() function is helpful for this.\n",
    "\n",
    "To capture the text column of the coffee tweets text along with a metadata column of unique numbers called num you would use the code below.\n",
    "\n",
    "custom_reader <- readTabular(\n",
    "  mapping = list(content = \"text\", id = \"num\")\n",
    ")\n",
    "text_corpus <- VCorpus(\n",
    "  DataframeSource(tweets), \n",
    "  readerControl = list(reader = custom_reader)\n",
    ")\n",
    "Instructions\n",
    "- Add author = \"screenName\" and date = \"created\" to the custom_reader object to get even more metadata.\n",
    "- Create text_corpus with VCorpus() and a custom_reader.\n",
    "- Now that text_corpus contains 1000 coffee tweets along with 4 pieces of metadata. Overwrite the text_corpus applying a custom clean_corpus function as you did earlier.\n",
    "- Examine the cleaned text content of the first tweet using [[1]][1].\n",
    "- Confirm that all metadata was captured correctly in the second part of the document [[1]][2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add author to custom reading list\n",
    "custom_reader <- readTabular(mapping = list(content = \"text\", \n",
    "                                            id = \"num\",\n",
    "                                            author = \"screenName\",\n",
    "                                            date = \"created\"\n",
    "                                            ))\n",
    "\n",
    "# Make corpus with custom reading\n",
    "text_corpus <- VCorpus(DataframeSource(tweets),  \n",
    "readerControl = list(reader = custom_reader))\n",
    "\n",
    "# Clean corpus\n",
    "text_corpus <- clean_corpus(text_corpus)\n",
    "\n",
    "# Print data\n",
    "print(text_corpus[[1]][1])\n",
    "\n",
    "# Print metadata\n",
    "print(text_corpus[[1]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon vs. Google - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing a text mining project\n",
    "50xp\n",
    "How many well-defined steps are in the text mining process?\n",
    "\n",
    "Possible Answers\n",
    "- 1\n",
    "- 3\n",
    "- 6\n",
    "- None...they aren't needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Problem definition\n",
    "50xp\n",
    "Which of these are NOT appropriate problem statements?\n",
    "\n",
    "Possible Answers\n",
    "- Does Amazon or Google have a better perceived pay according to online reviews?\n",
    "- Let's learn something about how employees review both Amazon and Google.\n",
    "- Does Amazon or Google have a better work-life balance according to current employees?\n",
    "- Take Hint (-15xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identifying the text sources\n",
    "\n",
    "Employee reviews can come from various sources. If your human resources department had the resources, you could have a third party administer focus groups to interview employees both internally and from your competitor.\n",
    "\n",
    "Forbes and others publish articles about the \"best places to work\", which may mention Amazon and Google. Another source of information might be anonymous online reviews from websites like Indeed, Glassdoor or CareerBliss.\n",
    "\n",
    "Here, we'll focus on a collection of anonymous online reviews.\n",
    "\n",
    "Instructions\n",
    "View the structure of amzn with str() to get its dimensions and a preview of the data.\n",
    "Create amzn_pros from the positive reviews column amzn$pros.\n",
    "Create amzn_cons from the negative reviews column amzn$cons.\n",
    "Print the structure of goog with str() to get its dimensions and a preview of the data.\n",
    "Create goog_pros from the positive reviews column goog$pros.\n",
    "Create goog_cons from the negative reviews column goog$cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(readxl)\n",
    "amzn <- read_excel(\"amzn.xlsx\", sheet=\"amzn\")\n",
    "goog <- read_excel(\"goog.xlsx\", sheet=\"goog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'pros'</li>\n",
       "\t<li>'cons'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'pros'\n",
       "\\item 'cons'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'pros'\n",
       "2. 'cons'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"pros\" \"cons\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(goog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t500 obs. of  2 variables:\n",
      " $ pros: chr  \"You're surrounded by smart people and the projects are interesting, if a little daunting.\" \"Brand name is great. Have yet to meet somebody who is unfamiliar with Amazon. Hours weren't as bad as I had previously heard. B\"| __truncated__ \"Good money.Interaction with some great minds in the world during internal conferences and sessions.Of course the pride of being\"| __truncated__ \"nice pay and overtime and different shifts\" ...\n",
      " $ cons: chr  \"You're surrounded by smart people and the projects are interesting, if a little daunting.\" \"Brand name is great. Have yet to meet somebody who is unfamiliar with Amazon. Hours weren't as bad as I had previously heard. B\"| __truncated__ \"Good money.Interaction with some great minds in the world during internal conferences and sessions.Of course the pride of being\"| __truncated__ \"nice pay and overtime and different shifts\" ...\n",
      "Classes 'tbl_df', 'tbl' and 'data.frame':\t501 obs. of  2 variables:\n",
      " $ pros: chr  \"* If you're a software engineer, you're among the kings of the hill at Google. It's an engineer-driven company without a doubt \"| __truncated__ \"1) Food, food, food. 15+ cafes on main campus (MTV) alone. Mini-kitchens, snacks, drinks, free breakfast/lunch/dinner, all day,\"| __truncated__ \"You can't find a more well-regarded company that actually deserves the hype it gets.\" \"- You drive yourself here. If you want to grow, you have to seek out opportunities and prove that your worth. This keeps you mo\"| __truncated__ ...\n",
      " $ cons: chr  \"* It *is* becoming larger, and with it comes growing pains: bureaucracy, slow to respond to market threats, bloated teams, cros\"| __truncated__ \"1) Work/life balance. What balance? All those perks and benefits are an illusion. They keep you at work and they help you to be\"| __truncated__ \"I live in SF so the commute can take between 1.5 hours to 1.75 hours each way on the shuttle - sometimes 2 hours each way on a \"| __truncated__ \"- Google is a big company. So there are going to be winners and losers when it comes to career growth. Due to the high hiring b\"| __truncated__ ...\n"
     ]
    }
   ],
   "source": [
    "# Print the structure of amzn\n",
    "str(amzn)\n",
    "\n",
    "# Create amzn_pros\n",
    "amzn_pros <- amzn$pros\n",
    "\n",
    "# Create amzn_cons\n",
    "amzn_cons <- amzn$cons\n",
    "\n",
    "# Print the structure of goog\n",
    "str(goog)\n",
    "\n",
    "# Create goog_pros\n",
    "goog_pros <- goog$pros\n",
    "\n",
    "# Create goog_cons\n",
    "goog_cons <- goog$cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text organization\n",
    "100xp\n",
    "Now that you have selected the exact text sources, you are ready to clean them up. You'll be using the two functions you just saw in the video: qdap_clean(), which applies a series of qdap functions to a text vector, and tm_clean(), which applies a series of tm functions to a corpus object. You can refer back to the video to remind yourself of how they work.\n",
    "\n",
    "In order to keep things simple, the functions have been defined for you and are available in your workspace. It's your job to apply them to amzn_pros and amzn_cons!\n",
    "\n",
    "Instructions\n",
    "- Alter amzn_pros by applying qdap_clean() to it.\n",
    "- Alter amzn_cons by applying qdap_clean() to it.\n",
    "- Apply VCorpus() to amzn_pros and store the result as az_p_corp.\n",
    "- Apply VCorpus() to amzn_cons and store the result as az_c_corp.\n",
    "- Create amzn_pros_corp by applying tm_clean() to az_p_corp.\n",
    "- Create amzn_cons_corp by applying tm_clean() to az_c_corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qdap_clean <- function(x){\n",
    "                 x <- replace_abbreviation(x)\n",
    "                 x <- replace_contraction(x)\n",
    "                 x <- replace_number(x)\n",
    "                 x <- replace_ordinal(x)\n",
    "                 x <- replace_ordinal(x)\n",
    "                 x <- replace_symbol(x)\n",
    "                 x <- tolower(x)\n",
    "                 return(x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm_clean <- function(corpus){\n",
    "  corpus <- tm_map(corpus, removePunctuation)\n",
    "  corpus <- tm_map(corpus, stripWhitespace)\n",
    "  corpus <- tm_map(corpus, removeWords, \n",
    "                   c(stopwords(\"en\"), \"Google\", \"Amazon\", \"company\"))\n",
    "  return(corpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alter amzn_pros\n",
    "amzn_pros <- qdap_clean(amzn_pros)\n",
    "    \n",
    "# Alter amzn_cons\n",
    "amzn_cons <- qdap_clean(amzn_cons)\n",
    "\n",
    "# Create az_p_corp \n",
    "az_p_corp <- VCorpus(VectorSource(amzn_pros))\n",
    "\n",
    "# Create az_c_corp\n",
    "az_c_corp <- VCorpus(VectorSource(amzn_cons))\n",
    "\n",
    "# Create amzn_pros_corp\n",
    "amzn_pros_corp <- tm_clean(az_p_corp)\n",
    "\n",
    "# Create amzn_cons_corp\n",
    "amzn_cons_corp <- tm_clean(az_c_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's clean up the Google reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Google reviews\n",
    "\n",
    "Now that the Amazon reviews have been cleaned, the same must be done for the Google reviews. qdap_clean() and tm_clean() are available in your workspace to help you clean goog_pros and goog_cons.\n",
    "\n",
    "Instructions\n",
    "- Alter goog_pros by applying qdap_clean() to it.\n",
    "- Alter goog_cons by applying qdap_clean() to it.\n",
    "- Apply VCorpus() to goog_pros and store the result as goog_p_corp.\n",
    "- Apply VCorpus() to goog_cons and store the result as goog_c_corp.\n",
    "- Create goog_pros_corp by applying tm_clean() to goog_p_corp.\n",
    "- Create goog_cons_corp by applying tm_clean() to goog_c_corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alter goog_pros\n",
    "goog_pros <- qdap_clean(goog_pros)\n",
    "    \n",
    "# Alter goog_cons\n",
    "goog_cons <- qdap_clean(goog_cons)\n",
    "\n",
    "# Create az_p_corp \n",
    "goog_p_corp <- VCorpus(VectorSource(goog_pros))\n",
    "\n",
    "# Create az_c_corp\n",
    "goog_c_corp <- VCorpus(VectorSource(goog_cons))\n",
    "\n",
    "# Create goog_pros_corp\n",
    "goog_pros_corp <- tm_clean(goog_p_corp)\n",
    "\n",
    "# Create goog_cons_corp\n",
    "goog_cons_corp <- tm_clean(goog_c_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 4 & 5: Feature extraction & analysis - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction & analysis: amzn_pros\n",
    "100xp\n",
    "amzn_pros_corp, amzn_cons_corp, goog_pros_corp and goog_cons_corp have all been preprocessed, so now you can extract the features you want to examine. Since you are using the bag of words approach, you decide to create a bigram TermDocumentMatrix for Amazon's positive reviews corpus, amzn_pros_corp. From this, you can quickly create a wordcloud() to understand what phrases people positively associate with working at Amazon.\n",
    "\n",
    "The function below uses RWeka to tokenize two terms and is used behind the scenes in this exercise.\n",
    "\n",
    "tokenizer <- function(x) \n",
    "  NGramTokenizer(x, Weka_control(min = 2, max = 2))\n",
    "Instructions\n",
    "Create amzn_p_tdm as a TermDocumentMatrix from amzn_pros_corp. Make sure to add control = list(tokenize = tokenizer) so that the terms are bigrams.\n",
    "Create amzn_p_tdm_m from amzn_p_tdm by using the as.matrix() function.\n",
    "Create amzn_p_freq to obtain the term frequencies from amzn_p_tdm_m.\n",
    "Create a wordcloud() using names(amzn_p_freq) as the words, amzn_p_freq as their frequencies, and max.words = 25 and color = \"blue\" for asthetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer <- function(x) \n",
    "  NGramTokenizer(x, Weka_control(min = 2, max = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(RWeka)\n",
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in .jcall(\"RWekaInterfaces\", \"[S\", \"tokenize\", .jcast(tokenizer, : java.lang.NullPointerException\n",
     "output_type": "error",
     "traceback": [
      "Error in .jcall(\"RWekaInterfaces\", \"[S\", \"tokenize\", .jcast(tokenizer, : java.lang.NullPointerException\nTraceback:\n",
      "1. TermDocumentMatrix(amzn_pros_corp, control = list(tokenize = tokenizer))",
      "2. TermDocumentMatrix.VCorpus(amzn_pros_corp, control = list(tokenize = tokenizer))",
      "3. mclapply(unname(content(x)), termFreq, control)",
      "4. lapply(X, FUN, ...)",
      "5. FUN(X[[i]], ...)",
      "6. .tokenize(doc)",
      "7. NGramTokenizer(x, Weka_control(min = 2, max = 2))   # at line 1-2 of file <text>",
      "8. .jcall(\"RWekaInterfaces\", \"[S\", \"tokenize\", .jcast(tokenizer, \n .     \"weka/core/tokenizers/Tokenizer\"), .jarray(as.character(control)), \n .     .jarray(as.character(x)))",
      "9. .jcheck()"
     ]
    }
   ],
   "source": [
    "# Create amzn_p_tdm\n",
    "amzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp, control=list(tokenize=tokenizer))\n",
    "\n",
    "# Create amzn_p_tdm_m\n",
    "amzn_p_tdm_m <- as.matrix(amzn_p_tdm)\n",
    "\n",
    "# Create amzn_p_freq\n",
    "amzn_p_freq <- rowSums(amzn_p_tdm_m)\n",
    "\n",
    "# Plot a wordcloud using amzn_p_freq values\n",
    "wordcloud(names(amzn_p_freq), amzn_p_freq, max.words=25, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction & analysis: amzn_cons\n",
    "\n",
    "You now decide to contrast this with the amzn_cons_corp corpus in another bigram TDM. Of course, you expect to see some different phrases in your word cloud.\n",
    "\n",
    "Once again, you will use this custom function to extract your bigram features for the visual:\n",
    "\n",
    "tokenizer <- function(x) \n",
    "  NGramTokenizer(x, Weka_control(min = 2, max = 2))\n",
    "Instructions\n",
    "- Create amzn_c_tdm by converting amzn_cons_corp into a TermDocumentMatrix and incorporating the bigram function control = list(tokenize = tokenizer).\n",
    "- Create amzn_c_tdm_m as a matrix version of amzn_c_tdm.\n",
    "- Create amzn_c_freq by using rowSums() to get term frequencies from amzn_c_tdm_m.\n",
    "- Create a wordcloud() using names(amzn_c_freq) and the values amzn_c_freq. Use the arguments max.words = 25 and color = \"red\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in .jcall(\"RWekaInterfaces\", \"[S\", \"tokenize\", .jcast(tokenizer, : java.lang.NullPointerException\n",
     "output_type": "error",
     "traceback": [
      "Error in .jcall(\"RWekaInterfaces\", \"[S\", \"tokenize\", .jcast(tokenizer, : java.lang.NullPointerException\nTraceback:\n",
      "1. TermDocumentMatrix(amzn_cons_corp, control = list(tokenize = tokenizer))",
      "2. TermDocumentMatrix.VCorpus(amzn_cons_corp, control = list(tokenize = tokenizer))",
      "3. mclapply(unname(content(x)), termFreq, control)",
      "4. lapply(X, FUN, ...)",
      "5. FUN(X[[i]], ...)",
      "6. .tokenize(doc)",
      "7. NGramTokenizer(x, Weka_control(min = 2, max = 2))   # at line 1-2 of file <text>",
      "8. .jcall(\"RWekaInterfaces\", \"[S\", \"tokenize\", .jcast(tokenizer, \n .     \"weka/core/tokenizers/Tokenizer\"), .jarray(as.character(control)), \n .     .jarray(as.character(x)))",
      "9. .jcheck()"
     ]
    }
   ],
   "source": [
    "# Create amzn_c_tdm\n",
    "amzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp, control=list(tokenize=tokenizer))\n",
    "\n",
    "# Create amzn_c_tdm_m\n",
    "amzn_c_tdm_m <- as.matrix(amzn_c_tdm)\n",
    "\n",
    "# Create amzn_c_freq\n",
    "amzn_c_freq <- rowSums(amzn_c_tdm_m)\n",
    "\n",
    "# Plot a wordcloud of negative Amazon bigrams\n",
    "wordcloud(names(amzn_c_freq), amzn_c_freq, max.words=25, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amzn_cons dendrogram\n",
    "\n",
    "It seems there is a strong indication of long working hours and poor work-life balance in the reviews. As a simple clustering technique, you decide to perform a hierarchical cluster and create a dendrogram to see how connected these phrases are.\n",
    "\n",
    "Instructions\n",
    "- Create amzn_c_tdm as a TermDocumentMatrix using amzn_cons_corp with control = list(tokenize = tokenizer).\n",
    "- Print amzn_c_tdm to the console.\n",
    "- Create amzn_c_tdm2 by applying the removeSparseTerms() function to amzn_c_tdm with the sparse argument equal to .993.\n",
    "- Create hc, a hierarchical cluster object by nesting the distance matrix dist(amzn_c_tdm2, method = \"euclidean\") inside the hclust() function. Make sure to also pass method = \"complete\" to the hclust() function.\n",
    "- Plot hc to view the clustered bigrams and see how the concepts in the Amazon cons section may lead you to a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word association\n",
    "100xp\n",
    "As expected, you see similar topics throughout the dendrogram. Switching back to positive comments, you decide to examine top phrases that appeared in the word clouds. You hope to find associated terms using the findAssocs()function from tm. You want to check for something surprising now that you have learned of long hours and a lack of work-life balance.\n",
    "\n",
    "Instructions\n",
    "The amzn_pros_corp corpus has been cleaned using the custom functions like before.\n",
    "\n",
    "- Construct a TDM called amzn_p_tdm from amzn_pros_corp and control = list(tokenize = tokenizer).\n",
    "- Create amzn_p_m by converting amzn_p_tdm to a matrix.\n",
    "- Create amzn_p_freq by applying rowSums() to amzn_p_m.\n",
    "- Create term_frequency using sort() on amzn_p_freq along with the argument decreasing = TRUE\n",
    "- Examine the first 5 bigrams using term_frequency[1:5].\n",
    "- You may be surprised to see \"fast paced\" as a top term because it could be a negative term related to \"long hours\". Look at the terms most associated with \"fast paced\". Use findAssocs() on amzn_p_tdm to examine \"fast paced\" with a 0.2 cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create amzn_p_tdm\n",
    "amzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp, control=list(tokenize=tokenizer))\n",
    "\n",
    "# Create amzn_p_m\n",
    "amzn_p_m <- as.matrix(amzn_p_tdm)\n",
    "\n",
    "# Create amzn_p_freq\n",
    "amzn_p_freq <- rowSums(amzn_p_m)\n",
    "\n",
    "# Create term_frequency\n",
    "term_frequency <- sort(amzn_p_freq, decreasing=TRUE)\n",
    "\n",
    "# Print the 5 most common terms\n",
    "term_frequency[1:5]\n",
    "\n",
    "# Find associations with fast paced\n",
    "findAssocs(amzn_p_tdm, \"fast paced\", 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the associated terms reveals that some are still somewhat negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick review of Google reviews\n",
    "100xp\n",
    "You decide to create a comparison.cloud() of Google's positive and negative reviews for comparison to Amazon. This will give you a quick understanding of top terms without having to spend as much time as you did examining the Amazon reviews in the previous exercises.\n",
    "\n",
    "We've provided you with a corpus all_goog_corpus, which has the 500 positive and 500 negative reviews for Google. Here, you'll clean the corpus and create a comparison cloud comparing the common words in both pro and con reviews.\n",
    "\n",
    "Instructions\n",
    "The all_goog_corpus object consisting of Google pro and con reviews is loaded in your workspace.\n",
    "\n",
    "- Create all_goog_corp by cleaning all_goog_corpus with the predefined tm_clean() function.\n",
    "- Create all_tdm by converting all_goog_corp to a term-document matrix.\n",
    "- Name the TDM columns in the order they were constructed by assigning c(\"Goog_Pros\", \"Goog_Cons\") to colnames(all_tdm).\n",
    "- Create all_m by converting all_tdm to a matrix.\n",
    "- Construct a comparison.cloud() from all_m. Include colors = c(\"#F44336\", \"#2196f3\") and max.words = 100 as arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create all_goog_corp\n",
    "all_goog_corp <- tm_clean(all_goog_corpus)\n",
    "\n",
    "# Create all_tdm\n",
    "all_tdm <- TermDocumentMatrix(all_goog_corp)\n",
    "\n",
    "# Name the columns of all_tdm\n",
    "colnames(all_tdm) <- c(\"Goog_Pros\", \"Goog_Cons\")\n",
    "\n",
    "# Create all_m\n",
    "all_m <- as.matrix(all_tdm)\n",
    "\n",
    "# Build a comparison cloud\n",
    "comparison.cloud(all_m,colors = c(\"#F44336\", \"#2196f3\"), max.words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cage match! Amazon vs. Google pro reviews\n",
    "100xp\n",
    "Amazon's positive reviews appear to mention bigrams such as \"good benefits\", while its negative reviews focus on bigrams such as \"work load\" and \"work-life balance\" issues.\n",
    "\n",
    "In contrast, Google's positive reviews mention \"great food\", \"perks\", \"smart people\", and \"fun culture\", among other things. Google's negative reviews discuss \"politics\", \"getting big\", \"bureaucracy\", and \"middle management\".\n",
    "\n",
    "You decide to make a pyramid plot lining up positive reviews for Amazon and Google so you can adequately see the differences between any shared birgrams.\n",
    "\n",
    "Instructions\n",
    "all_tdm_m is loaded in your workspace. It's a bigram TDM that has been preprocessed and converted to a simple matrix. Have a look with head(all_tdm_m).\n",
    "\n",
    "- Create common_words by using subset() on all_tdm_m to identify the bigrams that occur in both companies' pro reviews. Use the criteria all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0.\n",
    "- Create difference by calculating the absolute differences between columns 1 and 2 of common_words using the abs() function.\n",
    "- Add difference as a column to the common_words object using cbind(). difference should be the second argument to cbind().\n",
    "- Reorder common_words by sorting the difference column in decreasing order. Within single brackets, call the order() function with the decreasing argument set equal to TRUE. Save the result to common_words.\n",
    "- Create a data frame called top15_df of the top 15 bigrams. This is done with data.frame() by assigning the first column x = common_words[1:15, 1], the second column y = common_words[1:15, 2], and the third column labels = rownames(common_words[1:15, ]).\n",
    "- Create a pyramid plot by passing top15_df$x,top15_df$y, and labels = top15_df$labels to the pyramid.plot() function. We've added some additional arguments for you to improve the plot's appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create common_words\n",
    "common_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n",
    "\n",
    "# Create difference\n",
    "difference <- abs(common_words[, 1] - common_words[, 2])\n",
    "\n",
    "# Add difference to common_words\n",
    "common_words <- cbind(common_words, difference)\n",
    "\n",
    "# Order the data frame from most differences to least\n",
    "common_words <- common_words[order(common_words[, 3],decreasing = TRUE), ]\n",
    "\n",
    "# Create top15_df\n",
    "top15_df <- data.frame(x = common_words[1:15, 1],\n",
    "y = common_words[1:15, 2],labels =rownames(common_words[1:15, ]))\n",
    "\n",
    "# Create the pyramid plot\n",
    "pyramid.plot(top15_df$x, top15_df$y,\n",
    "             labels = top15_df$labels, gap = 12, \n",
    "             top.labels = c(\"Amzn\", \"Pro Words\", \"Google\"), \n",
    "             main = \"Words in Common\", unit = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cage match, part 2! Negative reviews\n",
    "\n",
    "Interestingly, some Amazon employees discussed \"work-life balance\" as a positive. In both organizations, people mentioned \"culture\" and \"smart people\", so there are some similar positive aspects between the two companies.\n",
    "\n",
    "You now decide to turn your attention to negative reviews and make the same visual. This time, all_tdm_m contains the negative reviews, or cons, from both organizations.\n",
    "\n",
    "Instructions\n",
    "- Create common_words using subset(). Pass in the all_tdm_m matrix, then add the logical expression all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0\n",
    "- Create difference by calculating the absolute differences by using abs() on the first column minus the second column.\n",
    "- Attach differences to common_words using cbind().\n",
    "- Reorder the common_words matrix using the third column common_words[, 3]. Save the result back to common_words.\n",
    "- Create a data frame called top15_df of the top 15 terms. This is done with data.frame() by assigning the first column x = common_words[1:15, 1], the second column y = common_words[1:15, 2], and the third column labels = rownames(common_words[1:15, ]).\n",
    "- Lastly, create another pyramid.plot() using the plotrix package. Pass in top15_df$x, top15_df$y, and labels = top15_df$labels. Add the following additional arguments to make the labels correct:\n",
    "    - gap = 12\n",
    "    - top.labels = c(\"Amzn\", \"Cons Words\", \"Google\")\n",
    "    - main = \"Words in Common\"\n",
    "    - unit = NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create common_words\n",
    "common_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n",
    "\n",
    "# Create difference\n",
    "difference <- abs(common_words[, 1] - common_words[, 2])\n",
    "\n",
    "# Add difference to common_words\n",
    "common_words <- cbind(common_words, difference)\n",
    "\n",
    "# Order the data frame from most differences to least\n",
    "common_words <- common_words[order(common_words[, 3],decreasing = TRUE), ]\n",
    "\n",
    "# Create top15_df\n",
    "top15_df <- data.frame(x = common_words[1:15, 1],\n",
    "y = common_words[1:15, 2],labels =rownames(common_words[1:15, ]))\n",
    "\n",
    "# Create the pyramid plot\n",
    "pyramid.plot(top15_df$x, top15_df$y,\n",
    "             labels = top15_df$labels, \n",
    "             gap = 12, \n",
    "             top.labels = c(\"Amzn\", \"Cons Words\", \"Google\"), \n",
    "             main = \"Words in Common\", \n",
    "             unit = NULL)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
