{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Light My Fire: Starting To Use Spark With dplyr Syntax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The connect-work-disconnect pattern\n",
    "100xp\n",
    "Working with sparklyr is very much like working with dplyr when you have data inside a database. In fact, sparklyr converts your R code into SQL code before passing it to Spark.\n",
    "\n",
    "The typical workflow has three steps:\n",
    "\n",
    "Connect to Spark using spark_connect().\n",
    "Do some work.\n",
    "Close the connection to Spark using spark_disconnect().\n",
    "In this exercise, you'll do this simplest possible piece of work: returning the version of Spark that is running, using spark_version().\n",
    "\n",
    "spark_connect() takes a URL that gives the location to Spark. For a local cluster (as you are running), the URL should be \"local\". For a remote cluster (on another machine, typically a high-performance server), the connection string will be a URL and port to connect on.\n",
    "\n",
    "spark_version() and spark_disconnect() both take the Spark connection as their only argument.\n",
    "\n",
    "One word of warning. Connecting to a cluster takes several seconds, so it is impractical to regularly connect and disconnect. While you need to reconnect for each DataCamp exercise, when you incorporate sparklyr into your own workflow, it is usually best to keep the connection open for the whole time that you want to work with Spark.\n",
    "\n",
    "Instructions\n",
    "Load the sparklyr package with library().\n",
    "Connect to Spark by calling spark_connect(), with argument master = \"local\". Assign the result to spark_conn.\n",
    "Get the Spark version using spark_version(), with argument sc = spark_conn.\n",
    "Disconnect from Spark using spark_disconnect(), with argument sc = spark_conn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load sparklyr\n",
    "library(sparklyr)\n",
    "\n",
    "# Connect to your Spark cluster\n",
    "spark_conn <- spark_connect(master=\"local\")\n",
    "\n",
    "# Print the version of Spark\n",
    "print(spark_version(sc=spark_conn))\n",
    "\n",
    "# Disconnect from Spark\n",
    "spark_disconnect(sc = spark_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying data into Spark\n",
    "100xp\n",
    "Before you can do any real work using Spark, you need to get your data into it. sparklyr has some functions such as spark_read_csv() that will read a CSV file into Spark. More generally, it is useful to be able to copy data from R to Spark. This is done with dplyr's copy_to() function. Be warned: copying data is a fundamentally slow process. In fact, a lot of strategy regarding optimizing performance when working with big datasets is to find ways of avoiding copying the data from one location to another.\n",
    "\n",
    "copy_to() takes two arguments: a Spark connection, and a data frame to copy over to Spark.\n",
    "\n",
    "Once you have copied your data into Spark, you might want some reassurance that it has actually worked. You can see a list of all the data frames stored in Spark using src_tbls(), which simply takes a Spark connection argument.\n",
    "\n",
    "Throughout the course, you will explore track metadata from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong). While Spark will happily scale well past a million rows of data, to keep things simple and responsive, you will use a thousand track subset. To clarify the terminology: a track refers to a row in the dataset. For your thousand track dataset, this is the same thing as a song (though the full million row dataset suffered from some duplicate songs).\n",
    "\n",
    "Instructions\n",
    "track_metadata, containing the song name, artist name, and other metadata for 1,000 tracks, has been pre-defined in your workspace.\n",
    "\n",
    "Use str() to explore the track_metadata dataset.\n",
    "Connect to the Spark cluster, storing the connection in spark_conn.\n",
    "Copy track_metadata to the Spark cluster using copy_to().\n",
    "See which data frames are available in Spark, using src_tbls().\n",
    "Disconnect from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dplyr\n",
    "library(dplyr)\n",
    "\n",
    "# Explore track_metadata structure\n",
    "str(track_metadata)\n",
    "\n",
    "# Connect to your Spark cluster\n",
    "spark_conn <- spark_connect(master='local')\n",
    "\n",
    "# Copy track_metadata to Spark\n",
    "track_metadata_tbl <- copy_to(spark_conn,track_metadata)\n",
    "\n",
    "# List the data frames available in Spark\n",
    "src_tbls(spark_conn)\n",
    "\n",
    "# Disconnect from Spark\n",
    "spark_disconnect(sc = spark_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I hope you didn't copy the answer! Copying data between R and Spark is slow, so it's best to minimize how many times you do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data, tiny tibble\n",
    "\n",
    "In the last exercise, when you copied the data to Spark, copy_to() returned a value. This return value is a special kind of tibble() that doesn't contain any data of its own. To explain this, you need to know a bit about the way that tidyverse packages store data. Tibbles are usually just a variant of data.frames that have a nicer print method. However, dplyr also allows them to store data from a remote data source, such as databases, and – as is the case here – Spark. For remote datasets, the tibble object simply stores a connection to the remote data. This will be discussed in more detail later, but the important point for now is that even though you have a big dataset, the size of the tibble object is small.\n",
    "\n",
    "On the Spark side, the data is stored in a variable called a DataFrame. This is a more or less direct equivalent of R's data.frame variable type. (Though the column variable types are named slightly differently – for example numeric columns are called DoubleType columns.) Throughout the course, the term data frame will be used, unless clarification is needed between data.frame and DataFrame. Since these types are also analogous to database tables, sometimes the term table will also be used to describe this sort of rectangular data.\n",
    "\n",
    "Calling tbl() with a Spark connection, and a string naming the Spark data frame will return the same tibble object that was returned when you used copy_to().\n",
    "\n",
    "A useful tool that you will see in this exercise is the object_size() function from the pryr package. This shows you how much memory an object takes up.\n",
    "\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. The track metadata for 1,000 tracks is stored in the Spark cluster in the table \"track_metadata\".\n",
    "\n",
    "Link to the track_metadata using tbl(). Assign the result to track_metadata_tbl.\n",
    "See how big the dataset is, using dim() on track_metadata_tbl.\n",
    "See how small the tibble is, using object_size() on track_metadata_tbl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link to the track_metadata table in Spark\n",
    "track_metadata_tbl <- tbl(spark_conn,\"track_metadata\")\n",
    "\n",
    "# See how big the dataset is\n",
    "dim(track_metadata_tbl)\n",
    "\n",
    "# See how small the tibble is\n",
    "object_size(track_metadata_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "It's an optical illusion! The tibble looks quite big, but it is actually really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the structure of tibbles\n",
    "100xp\n",
    "If you try to print a tibble that describes data stored in Spark, some magic has to happen, since the tibble doesn't keep a copy of the data itself. The magic is that the print method uses your Spark connection, copies some of the contents back to R, and displays those values as though the data had been stored locally. As you saw earlier in the chapter, copying data is a slow operation, so by default, only 10 rows and as many columns will fit onscreen, are printed.\n",
    "\n",
    "You can change the number of rows that are printed using the n argument to print(). You can also change the width of content to display using the width argument, which is specified as the number of characters (not the number of columns). A nice trick is to use width = Inf to print all the columns.\n",
    "\n",
    "The str() function is typically used to display the structure of a variable. For data.frames, it gives a nice summary with the type and first few values of each column. For tibbles that have a remote data source however, str() doesn't know how to retrieve the data. That means that if you call str() on a tibble that contains data stored in Spark, you see a list containing a Spark connection object, and a few other bits and pieces.\n",
    "\n",
    "If you want to see a summary of what each column contains in the dataset that the tibble refers to, you need to call glimpse() instead. Note that for remote such as those stored in a Spark cluster datasets, the number of rows is a lie! It this case, glimpse() never claims that the data has more than 25 rows.\n",
    "\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.\n",
    "\n",
    "Print the first 5 rows and all the columns of the track metadata.\n",
    "Examine the structure of the tibble using str().\n",
    "Examine the structure of the track metadata using glimpse()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print 5 rows, all columns\n",
    "print(track_metadata_tbl, n=5, width=Inf)\n",
    "\n",
    "# Examine structure of tibble\n",
    "str(track_metadata_tbl)\n",
    "\n",
    "# Examine structure of data\n",
    "glimpse(track_metadata_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hurrah! I hope you can see that you need to glimpse() at your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns\n",
    "100xp\n",
    "The easiest way to manipulate data frames stored in Spark is to use dplyr syntax. Manipulating data frames using the dplyr syntax is covered in detail in the Data Manipulation in R with dplyr and Joining Data in R with dplyr courses, but you'll spend the next chapter and a half covering all the important points.\n",
    "\n",
    "dplyr has five main actions that you can perform on a data frame. You can select columns, filter rows, arrange the order of rows, change columns or add new columns, and calculate summary statistics.\n",
    "\n",
    "Let's start with selecting columns. This is done by calling select(), with a tibble, followed by the unquoted names of the columns you want to keep. dplyr functions are conventionally used with magrittr's pipe operator, %>%. To select the x, y, and z columns, you would write the following.\n",
    "\n",
    "a_tibble %>%\n",
    "  select(x, y, z)\n",
    "Note that square bracket indexing is not currently supported in sparklyr. So you cannot do\n",
    "\n",
    "a_tibble[, c(\"x\", \"y\", \"z\")]\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.\n",
    "\n",
    "Select the artist_name, release, title, and year using select().\n",
    "Try to do the same thing using square bracket indexing. Spoiler! This code throws an error, so it is wrapped in a call to tryCatch()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_metadata_tbl has been pre-defined\n",
    "track_metadata_tbl\n",
    "\n",
    "# Manipulate the track metadata\n",
    "track_metadata_tbl %>%\n",
    "  # Select columns\n",
    "  select(artist_name, release, title, year)\n",
    "\n",
    "# Try to select columns using [ ]\n",
    "tryCatch({\n",
    "    # Selection code here\n",
    "    track_metadata_tbl[,c(\"artist_name\", \"release\", \"title\", \"year\")]\n",
    "  },\n",
    "  error = print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful work! I'm sure you noticed that was quite an intense error message. Don't worry, that's normal for sparklyr errors. tryCatch(error = print) is a nice way to see errors without them stopping the execution of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering rows\n",
    "\n",
    "As well as selecting columns, the other way to extract important parts of your dataset is to filter the rows. This is achieved using the filter() function. To use filter(), you pass it a tibble and some logical conditions. For example, to return only the rows where the values of column x are greater than zero and the values of y equal the values of z, you would use the following.\n",
    "\n",
    "a_tibble %>%\n",
    "  filter(x > 0, y == z)\n",
    "Before you try the exercise, take heed of two warnings. Firstly, don't mistake dplyr's filter() function with the stats package's filter() function. Secondly, sparklyr converts your dplyr code into SQL database code before passing it to Spark. That means that only a limited number of filtering operations are currently supported. For example, you can't filter character rows using regular expressions with code like\n",
    "\n",
    "a_tibble %>%\n",
    "  filter(grepl(\"a regex\", x))\n",
    "The help page for translate_sql() describes the functionality that is available. You are OK to use comparison operators like >, !=, and %in%; arithmetic operators like +, ^, and %%; and logical operators like &, | and !. Many mathematical functions such as log(), abs(), round(), and sin() are also supported.\n",
    "\n",
    "As before, square bracket indexing does not currently work.\n",
    "\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.\n",
    "\n",
    "As in the previous exercise, select the artist_name, release, title, and year using select().\n",
    "Pipe the result of this to filter() to get the tracks from the 1960s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_metadata_tbl has been pre-defined\n",
    "glimpse(track_metadata_tbl)\n",
    "\n",
    "# Manipulate the track metadata\n",
    "track_metadata_tbl %>%\n",
    "  # Select columns\n",
    "  select(artist_name, release, title, year) %>%\n",
    "  # Filter rows\n",
    "  filter(year >= 1960, year < 1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've certainly filtered out your bad code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arranging rows\n",
    "100xp\n",
    "Back in the days when music was stored on CDs, there was a perennial problem: how do you best order your CDs so you can find the ones you want? By order of artist? Chronologically? By genre?\n",
    "\n",
    "The arrange() function let's you reorder the rows of a tibble. It takes a tibble, followed by the unquoted names of columns. For example, to sort in ascending order of the values of column x, then (where there is a tie in x) by descending order of values of y, you would write the following.\n",
    "\n",
    "a_tibble %>%\n",
    "  arrange(x, desc(y))\n",
    "Notice the use of desc() to enforce sorting by descending order. Also be aware that in sparklyr, the order() function, used for arranging the rows of data.frames does not work.\n",
    "\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.\n",
    "\n",
    "Select the artist_name, release, title, and year fields.\n",
    "Pipe the result of this to filter on tracks from the 1960s.\n",
    "Pipe the result of this to arrange() to order by artist_name, then descending year, then title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_metadata_tbl has been pre-defined\n",
    "track_metadata_tbl\n",
    "\n",
    "# Manipulate the track metadata\n",
    "track_metadata_tbl %>%\n",
    "  # Select columns\n",
    "  select(artist_name, release, title, year) %>%\n",
    "  # Filter rows\n",
    "  filter(year >= 1960, year < 1970) %>%\n",
    "  # Arrange rows\n",
    "  arrange(artist_name, desc(year), title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've certainly filtered out your bad code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutating columns\n",
    "\n",
    "It may surprise you, but not all datasets start out perfectly clean! Often you have to fix values, or create new columns derived from your existing data. The process of changing or adding columns is called mutation in dplyr terminology, and is performed using mutate(). This function takes a tibble, and named arguments to update columns. The names of each of these arguments is the name of the columns to change or add, and the value is an expression explaining how to update it. For example, given a tibble with columns x and y, the following code would update x and create a new column z.\n",
    "\n",
    "a_tibble %>%\n",
    "  mutate(\n",
    "    x = x + y,\n",
    "    z = log(x)  \n",
    "  )\n",
    "In case you hadn't got the message already that base-R functions don't work with Spark tibbles, you can't use within() or transform() for this purpose.\n",
    "\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.\n",
    "\n",
    "Select the title, and duration fields. Note that the durations are in seconds.\n",
    "Pipe the result of this to mutate() to create a new field, duration_minutes, that contains the track duration in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_metadata_tbl has been pre-defined\n",
    "track_metadata_tbl\n",
    "\n",
    "# Manipulate the track metadata\n",
    "track_metadata_tbl %>%\n",
    "  # Select columns\n",
    "  select(title, duration)%>%\n",
    "  # Mutate columns\n",
    "  mutate(duration_minutes=duration/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Brilliant work! Mutation has two purposes: changing columns, and adding new columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing columns\n",
    "\n",
    "The mutate() function that you saw in the previous exercise takes columns as inputs, and returns a column. If you are calculating summary statistics such as the mean, maximum, or standard deviation, then you typically want to take columns as inputs but return a single value. This is achieved with the summarize() function.\n",
    "\n",
    "a_tibble %>%\n",
    "  summarize(\n",
    "    mean_x       = mean(x),\n",
    "    sd_x_times_y = sd(x * y)\n",
    "  )\n",
    "Note that dplyr has a philosophy (passed on to sparklyr) of always keeping the data in tibbles. So the return value here is a tibble with one row, and one column for each summary statistic that was calculated.\n",
    "\n",
    "Instructions\n",
    "A Spark connection has been created for you as spark_conn. A tibble attached to the track metadata stored in Spark has been pre-defined as track_metadata_tbl.\n",
    "\n",
    "Select the title, and duration fields.\n",
    "Pipe the result of this to create a new field, duration_minutes, that contains the track duration in minutes.\n",
    "Pipe the result of this to summarize() to calculate the mean duration in minutes, in a field named mean_duration_minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_metadata_tbl has been pre-defined\n",
    "track_metadata_tbl\n",
    "\n",
    "# Manipulate the track metadata\n",
    "track_metadata_tbl %>%\n",
    "  # Select columns\n",
    "  select(title, duration)%>%\n",
    "  # Mutate columns\n",
    "  mutate(duration_minutes=duration/60) %>%\n",
    "  # Summarize columns\n",
    "  summarize(mean_duration_minutes=mean(duration_minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splendid! Summarizing returns a tibble with a summary statistic in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tools of the Trade: Advanced dplyr Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
