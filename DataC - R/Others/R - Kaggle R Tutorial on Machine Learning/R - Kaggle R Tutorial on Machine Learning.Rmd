---
title: "Kaggle R Tutorial on Machine Learning"
author: "MOHAMMAD SHADAN"
date: "June 5, 2017"
output: html_document
---

```{r}
getwd()
setwd("C:/Users/Lenovo/Documents/GitHub/LearningPython/Datacamp/R - Kaggle R Tutorial on Machine Learning")

train <- read.csv("train.csv")
test <- read.csv("test.csv")
```



```{r}
# Import the training set: train
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
train <- read.csv(train_url)
  
# Import the testing set: test
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
test <- read.csv(test_url)
  
# Print train and test to the console
train
test
```

```{r}
str(train)
str(test)

dim(train)
```


Rose vs Jack, or Female vs Male
100xp
How many people in your training set survived the disaster with the Titanic? To see this, you can use the table() command in combination with the $-operator to select a single column of a data frame:

# absolute numbers
table(train$Survived) 

# proportions
prop.table(table(train$Survived))
If you run these commands in the console, you'll see that 549 individuals died (62%) and 342 survived (38%). A simple prediction heuristic could thus be "majority wins": you predict every unseen observation to not survive.

In general, the table() command can help you to explore which variables have predictive value. For example, maybe gender could play a role as well? For a two-way comparison, also including gender, you can use

table(train$Sex, train$Survived)
To get proportions, you can again wrap prop.table() around table(), but you'll have to specify whether you want row-wise or column-wise proportions: This is done by setting the second argument of prop.table(), called margin, to 1 or 2, respectively.

Instructions
Call table() on train$Survived to calculate the survival rates in absolute numbers.
Calculate the survival rates as proportions by wrapping prop.table() around the previous table() call.
Do a two-way comparison on the number of males and females that survived, in absolute numbers. Again, use the train data frame.
Convert the numbers to row-wise proportions.

```{r}
# Your train and test set are still loaded
str(train)
str(test)

# Survival rates in absolute numbers
table(train$Survived)

# Survival rates in proportions
prop.table(table(train$Survived))
  
# Two-way comparison: Sex and Survived
table(train$Sex, train$Survived)

# Two-way comparison: row-wise proportions
prop.table(table(train$Sex, train$Survived),1)
```


Does age play a role?
100xp
Another variable that could influence survival is age: it's probable children were saved first. You can test this by creating a new column with a categorical variable child.

To add this new variable you need to do two things:

Create a new column, which is done through the $ operator. To create a new column, lucky, for example:

train$lucky <- NA
Provide the values for each observation (i.e., row) based on the age of the passenger. You can use a boolean test inside square brackets for this. For example, to set the lucky column to TRUE for passengers that survived the disaster, and the others to FALSE, you could use:

train$lucky[train$Survived == 1] <- TRUE
train$lucky[train$Survived == 0] <- FALSE
Instructions
Finish the code on the right to create a new column Child
whose default value is NA,
whose value is 1 if the passenger's Age is < 18 years and
whose value is 0 when the passenger's Age is >= 18 years.
Do a two-way comparison on the number of children vs adults that survived, in row-wise proportions.

```{r}
# Your train and test set are still loaded in
str(train)
str(test)

# Create the column child, and indicate whether child or no child
train$Child <- NA
train$Child[train$Age < 18] <- 1
train$Child[train$Age >= 18] <- 0


# Two-way comparison
prop.table(table(train$Child, train$Survived),1)
```

Making your first predictions
100xp
In one of the previous exercises you discovered that in your training set, females had over a 50% chance of surviving and males had less than a 50% chance of surviving. Hence, you could use this information for your first prediction: all females in the test set survive and all males in the test set die.

You use your test set for validating your predictions. You might have seen that, contrary to the training set, the test set has no Survived column. You add such a column using your predicted values. Next, when uploading your results, Kaggle will use this column (= your predictions) to score your performance.

We already prepared a data frame test_one for you, that is a copy of the test variable.

Instructions
Add an additional column, Survived, that you initialize to zero.
Use vector subsetting like in the previous exercise to set the value of Survived to 1 for observations whose Sex equals "female".

```{r}
# Your train and test set are still loaded in
str(train)
str(test)

# Copy of test
test_one <- test

# Initialize a Survived column to 0
test_one$Survived <- 0

# Set Survived to 1 if Sex equals "female"
test_one$Survived[test_one$Sex == "female"] <- 1
```

## Section 2 

Intro to decision trees
100xp
In the previous chapter you did all the slicing and dicing yourself to find subsets that have a higher chance of surviving. A decision tree automates this process for you, and outputs a flowchart-like structure that is easy to interpret (you'll make one yourself in the next exercise).

Conceptually, the decision tree algorithm starts with all the data at the root node and scans all the variables for the best one to split on. Once a variable is chosen, you do the split and go down one level (or one node) and repeat. The final nodes at the bottom of the decision tree are known as terminal nodes, and the majority vote of the observations in that node determine how to predict for new observations that end up in that terminal node.

To create your first decision tree, you'll make use of R's rpart package. Instead of needing to writing an algo yourself you can use this package to build a decision tree.

Instructions
Let's start simple: load in the rpart package with the library() function.

```{r}
# Load in the R package
library(rpart)
```

Creating your first decision tree
100xp
Inside rpart, there is therpart() function to build your first decision tree. The function takes multiple arguments:

formula: specifying variable of interest, and the variables used for prediction (e.g. formula = Survived ~ Sex + Age).
data: The data set to build the decision tree (here train).
method: Type of prediction you want. We want to predict a categorical variable, so classification: method = "class".
Your call could look like this:

my_tree <- rpart(Survived ~ Sex + Age,
                 data = train,
                 method ="class")
To visualize the resulting tree, you can use the plot(my_tree) and text(my_tree). The resutling graphs will not be that informative, but R has packages to make it all fancier: rattle, rpart.plot, and RColorBrewer.

Instructions
Build a decision tree my_tree_two:
You want to predict Survived based on Pclass, Sex, Age, SibSp, Parch, Fare and Embarked.
Use the train data to build the tree
Use method to specify that you want to classify.
Visualize my_tree_two with plot() and text().
Load the R packages rattle, rpart.plot, and RColorBrewer.
Use fancyRpartPlot(my_tree) to create a much fancier visualization of your tree.


```{r}
library(rpart)
# Your train and test set are still loaded in
str(train)
str(test)

# Build the decision tree
my_tree_two <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = "class")

# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)

# Load in the packages to build a fancy plot
library(rattle)
library(rpart.plot)
library(RColorBrewer)

# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)
```

Interpreting your decision tree
50xp
On the right, you see the decision tree you just created. Looks nice, doesn't it? It's a very clear graph, that is easy to read and to interpret. Also, you see that thanks to the algorithm we can easily take into account more variables as opposed to creating the segments manually.

Based on your decision tree, what variables play the most important role to determine whether or not a passenger will survive?

Possible Answers
Sex, Age, Pclass, SibSp, Parch, Fare and Embarked
Sex, Age, Pclass, SibSp, Fare (Correct)
Sex, Age, Parch and Embarked
Sex, Age and Embarked
Take Hint (-15xp)


Predict and submit to Kaggle
100xp
To send a submission to Kaggle you need to predict the survival rates for the observations in the test set. In the previous chapter we created rather amateuristic predictions with manual subsetting operations. Now that we have a decision tree, we can make use of the predict() function to "generate" our answer:

predict(my_tree_two, test, type = "class")
Here, my_tree_two is the tree model you've just built, test is the data set to build the preditions for, and type = "class" specifies that you want to classify observations.

Before you can submit to Kaggle, you'll have to convert your predictions to a CSV file with exactly 418 entries and 2 columns PassengerId and Survived. Head over to the instructions to get to it!

Instructions
Use predict() as specified above to make predictions on the test set. Assign the result to my_prediction.
Finish the data.frame() call to create the my_solution data frame that is in line with Kaggle's standards:
The PassengerId column should contain the PassengerId column of test.
The Survivid column should contain the values in my_prediction.
Check that my_solution has 418 entries with nrow().
Finish the write.csv() call to write the data in my_solution to "my_solution.csv". Don't remove the row.names = FALSE argument.

```{r}
# my_tree_two and test are available in the workspace

# Make predictions on the test set
my_prediction <- predict(my_tree_two, newdata = test, type = "class")

# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)

# Use nrow() on my_solution
nrow(my_solution)

# Finish the write.csv() call
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
```

Overfitting, the iceberg of decision trees
100xp
If you submitted the solution of the previous exercise, you got a result that outperforms a solution using purely gender. Hurray!

Maybe we can improve even more by making a more complex model? In rpart, the amount of detail is defined by two parameters:

cp determines when the splitting up of the decision tree stops.
minsplit determines the minimum amount of observations in a leaf of the tree.
In the super_model on the right, cp = 0 (no stopping of splits) and minsplit = 2 (smallest leaf possible). This will create the best model! Or not? Check out the resulting plot with:

fancyRpartPlot(super_model)
Looking complex, but using this model to make predictions won't give you a good score on Kaggle. Why? Because you created very specific rules based on the data in the training set. These very detailed rules are only relevant for the training set but cannot be generalized to unknown sets. You overfitted your tree. Always be aware of this danger!

Instructions
Change the command that builds super_model:
Call the resulting tree my_tree_three instead of super_model
Use the same formula, data and method.
Set minsplit to 50 and cp to 0.
Visualize my_tree_three - your new decision tree - with fancyRpartPlot().

```{r}
# Your train and test set are still loaded in

# Change this command
super_model <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
                     data = train, method = "class", control = rpart.control(minsplit = 2, cp = 0))

# Visualize my_tree_three
fancyRpartPlot(super_model)
```

```{r}
# Your train and test set are still loaded in

# Change this command
my_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,data = train, method = "class", control = rpart.control(minsplit = 50, cp = 0))

# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
```

Re-engineering our Titanic data set
100xp
Data Science is an art that benefits from a human element. Enter feature engineering: creatively engineering your own features by combining the different existing variables.

While feature engineering is a discipline in itself, too broad to be covered here in detail, let's have have a look at a simple example and create a new predictive attribute: family_size.

A valid assumption is that larger families need more time to get together on a sinking ship, and hence have less chance of surviving. Family size is determined by the variables SibSp and Parch, which indicate the number of family members a certain passenger is traveling with. So when doing feature engineering, you add a new variable family_size, which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.

Instructions
Create a new train set train_two that differs from train only by having an extra column with your feature engineered variable family_size.
Finish the command to build my_tree_four: The formula in rpart() should include family_size and the tree should be used on the train_two data.
Visualize your new decision tree with fancyRpartPlot().

```{r}
# train and test are available

# Create train_two
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1

# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,data = train_two, method = "class")

# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
```

Well done. If you have a close look at your decision tree you see that family_size is not included. Apparently other variables played a more important role. This is part of the game as well. Sometimes things will not turn out as expected, but it is here that you can make the difference.

Passenger Title and survival rate
100xp
Was it coincidence that upper-class Rose survived and third-class passenger Jack not? Let's have a look...

You have access to a new train and test set named train_new and test_new. These data sets contain a new column with the name Title (referring to Miss, Mr, etc.). Title is another example of feature engineering: it's a new variable that possibly improves the model.

Instructions
Finish the command to create a decision tree my_tree_five: make sure to include the Title variable, and to create the tree based on train_new.
Visualize my_tree_five with fancyRpartPlot(). Notice that Title appears in one of the nodes.
Finish the predict() call to create my_prediction: the function should use my_tree_five and test_new to make predictions.
The code that creates a data frame my_solution and writes it to a CSV file is included: these steps make the solution ready for a submission on Kaggle.

```{r}
#Extract the Titles from the data
```


```{r}
# train_new and test_new are available in the workspace

# Finish the command
my_tree_five <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title,data = train_new, method = "class")

# Visualize my_tree_five
fancyRpartPlot(my_tree_five)

# Make prediction
my_prediction <- predict(my_tree_five,test_new, type = "class")

# Make results ready for submission
my_solution <- data.frame(PassengerId = test_new$PassengerId, Survived = my_prediction)
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
```

## 3Improving your predictions thr

What is a Random Forest
100xp
A detailed study of Random Forests would take this tutorial a bit too far. However, since it's an often used machine learning technique, a general understanding and an illustration in R won't hurt.

In layman's terms, the Random Forest technique handles the overfitting problem you faced with decision trees. It grows multiple (very deep) classification trees using the training set. At the time of prediction, each tree is used to come up with a prediction and every outcome is counted as a vote. For example, if you have trained 3 trees with 2 saying a passenger in the test set will survive and 1 says he will not, the passenger will be classified as a survivor. This approach of overtraining trees, but having the majority's vote count as the actual classification decision, avoids overfitting.

Before starting with the actual analysis, you first need to meet one big condition of Random Forests: no missing values in your data frame. Let's get to work.

Instructions
The code to clean your entire dataset from missing data and split it up in training and test set is provided in the sample code. Study the code chunks closely so you understand what's going on. Just click Submit Answer to continue.
If you want to know how all_data itself was built from train and test, have a look at this R script.

```{r}
# load("C:/Users/Lenovo/Documents/GitHub/LearningPython/Datacamp/R - Kaggle R Tutorial on Machine Learning/all_data.RData")
load("all_data.RData")
head(all_data)
getwd()
```


```{r}
# All data, both training and test set
all_data
names(all_data)[colSums(is.na(all_data))>0]

# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passengers embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"

# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)

# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)

# How to fill in missing Age values?
# We make a prediction of a passengers Age using the other variables and a decision tree model.
# This time you give method = "anova" since you are predicting a continuous variable.
library(rpart)
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,
                       data = all_data[!is.na(all_data$Age),], method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])

# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
```

Great, your data is cleaned up. One more important element in Random Forest is randomization to avoid the creation of the same tree from the training set. You randomize in two ways: by taking a randomized sample of the rows in your training set and by only working with a limited and changing number of the available variables for every node of the tree. Now it's time to do your first Random Forest analysis.


A Random Forest analysis in R
100xp
For a Random Forest analysis in R you make use of the randomForest() function in the randomForest package. You call the function in a similar way as rpart():

First your provide the formula. There is no argument class here to inform the function you're dealing with predicting a categorical variable, so you need to turn Survived into a factor with two levels: as.factor(Survived) ~ Pclass + Sex + Age
The data argument takes the train data frame.
When you put the importance argument to TRUE you can inspect variable importance.
The ntree argument specifies the number of trees to grow. Limit these when having only limited computational power at your disposal.
To end, since Random Forest uses randomization, you set a seed like this set.seed(111) to assure reproducibility of your results. Once the model is constructed, you can use the prediction function predict().

Instructions
Perform a Random Forest and name the model my_forest. Use the variables Passenger Class, Sex, Age, Number of Siblings/Spouses Aboard, Number of Parents/Children Aboard, Passenger Fare, Port of Embarkation, and Title (in this order).
Set the number of trees to grow to 1000 and make sure you can inspect variable importance.
Make a prediction (my_prediction) on the test set using the predict() function.
Create a data frame my_solution that contains the solution in line with the Kaggle standards.
Turn your solution into a csv file with the name my_solution.csv.

```{r}
# train and test are available in the workspace
str(train)
str(test)

# Load in the package
library(randomForest)

# Train set and test set
str(train)
str(test)

# Set seed for reproducibility
set.seed(111)

# Apply the Random Forest Algorithm
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title,data = train, ntree=1000, importance=TRUE)

# Make your prediction using the test set
my_prediction <- predict(my_forest, test)

# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)

# Write your solution away to a csv file with the name my_solution.csv
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
```

Important variables
50xp
Your Random Forest object my_forest is still loaded in. Remember you set importance = TRUE? Now you can see what variables are important using

varImpPlot(my_forest)
Type it into the console and see what happens.

When running the function, two graphs appear: the accuracy plot shows how much worse the model would perform without the included variables. So a high decrease (= high value x-axis) links to a high predictive variable. The second plot is the Gini coefficient. The higher the variable scores here, the more important it is for the model.

Based on the two plots, what variable has the highest impact on the model?

Possible Answers
Fare
Sex
Title (Correct)
Age


```{r}
varImpPlot(my_forest)
```

