---
title: "R Notebook"
output: html_notebook
---

# 1. Exploratory time series data analysis

## Welcome to the Course!

### Exploring raw time series

The most common first step when conducting time series analysis is to display your time series dataset in a visually intuitive format. The most useful way to view raw time series data in R is to use the print() command, which displays the Start, End, and Frequency of your data along with the observations.

Another useful command for viewing time series data in R is the length() function, which tells you the total number of observations in your data.

Some datasets are very long, and previewing a subset of data is more suitable than displaying the entire series. The head(___, n =___) and tail(___, n =___) functions, in which n is the number of items to display, focus on the first and last few elements of a given dataset respectively.

In this exercise, you'll explore the famous River Nile annual streamflow data, Nile. This time series dataset includes some metadata information. When calling print(Nile), note that Start = 1871 indicates that 1871 is the year of the first annual observation, and End = 1970 indicates 1970 is the year of the last annual observation.

Instructions
Use the print() function to display the River Nile data. The data object is called Nile
Use the length() function to identify the number of elements in your Nile dataset.
Use head() to display the first 10 elements of the Nile dataset. To do so, set the n argument equal to 10.
Use tail() to display the last 12 elements of the Nile dataset, again setting an appropriate value to the n argument.

```{r}
# Print the Nile dataset
print(Nile)

# List the number of observations in the Nile dataset
length(Nile)

# Display the first 10 elements of the Nile dataset
head(Nile, n=10)

# Display the last 12 elements of the Nile dataset
tail(Nile, n=12)
```

### Basic time series plots

While simple commands such as print(), length(), head(), and tail() provide crucial information about your time series data, another very useful way to explore any data is to generate a plot.

In this exercise, you will plot the River Nile annual streamflow data using the plot() function. For time series data objects such as Nile, a Time index for the horizontal axis is typically included. From the previous exercise, you know that this data spans from 1871 to 1970, and horizontal tick marks are labeled as such. The default label of "Time" is not very informative. Since these data are annual measurements, you should use the label "Year". While you're at it, you should change the vertical axis label to "River Volume (1e9 m^{3})".

Additionally, it helps to have an informative title, which can be set using the argument main. For your purposes, a useful title for this figure would be "Annual River Nile Volume at Aswan, 1871-1970".

Finally, the default plotting type for time series objects is "l" for line. Connecting consecutive observations can help make a time series plot more interpretable. Sometimes it is also useful to include both the observations points as well as the lines, and we instead use "b" for both.

Instructions   

 - Use plot() to display the Nile dataset.
 - Use a second call to plot() to display the data, but add the additional arguments: xlab = "Year", ylab = "River Volume (1e9 m^{3})".
 - Use a third call to plot() with your Nile data, but this time also add a title and include observation points in the figure by specifying the following arguments: main = "Annual River Nile Volume at Aswan, 1871-1970", type ="b".

```{r}
# Plot the Nile data
plot(Nile)

# Plot the Nile data with xlab and ylab arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})")

# Plot the Nile data with xlab, ylab, main, and type arguments

plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})",main = "Annual River Nile Volume at Aswan, 1871-1970", type ="b")
```


The plot() command is one of the most versatile commands in R. When used with time series data, this command automatically plots your data over time.

### What does the time index tell us?

Some data are naturally evenly spaced by time. The time series discrete_data shown in the top figure has 20 observations, with one observation appearing at each of the discrete time indices 1 through 20. Discrete time indexing is appropriate for discrete_data.

The time series continuous_series shown in the bottom figure also has 20 observations, it is following the same periodic pattern as discrete_data, but its observations are not evenly spaced. Its first, second, and last observations were observed at times 1.210322, 1.746137, and 20.180524, respectively. Continuous time indexing is natural for continuous_series, however, the observations are approximately evenly spaced, with about 1 observation observed per time unit. Let's investigate using a discrete time indexing for continuous_series.

Instructions
Use plot(___, ___, type = "b") to display continuous_series versus continuous_time_index, its continuous time index
Create a vector 1:20 to be used as a discrete time index.
Now use plot(___, ___, type = "b") to display continuous_series versus discrete_time_index
Note the various differences between the resulting figures, but the approximation appears reasonable because the overall trend remained preserved


```{r}
# Plot the continuous_series using continuous time indexing
par(mfrow=c(2,1))
plot(continuous_time_index,continuous_series, type = "b")

# Make a discrete time index using 1:20 
discrete_time_index <- 1:20

# Now plot the continuous_series using discrete time indexing
plot(discrete_time_index,continuous_series, type = "b")
```

## Sampling Frequency - Video

### Identifying the sampling frequency

In addition to viewing your data and plotting over time, there are several additional operations that can be performed on time series datasets.

The start() and end() functions return the time index of the first and last observations, respectively. The time() function calculates a vector of time indices, with one element for each time index on which the series was observed.

The deltat() function returns the fixed time interval between observations and the frequency() function returns the number of observations per unit time. Finally, the cycle() function returns the position in the cycle of each observation.

In this exercise, you'll practice applying these functions to the AirPassengers dataset, which reports the monthly total international airline passengers (in thousands) from 1949 to 1960.

Instructions
Begin by plotting the AirPassengers data using a simple call to plot().
Next, list the first and last time observations in AirPassengers using start() and end(), respectively.
Finally, gain some additional insight into this dataset by using the time(), deltat(), frequency(), and cycle() commands AirPassengers.


```{r}
# Plot AirPassengers
plot(AirPassengers)

# View the start and end dates of AirPassengers
start(AirPassengers)
end(AirPassengers)

# Use time(), deltat(), frequency(), and cycle() with AirPassengers 
time(AirPassengers)
deltat(AirPassengers)
frequency(AirPassengers)
cycle(AirPassengers)
```

These commands provide considerable descriptive information about the structures and patterns in your time series data. It may help to keep these commands handy when working with your own time series data.

### When is the sampling frequency exact?

The sampling frequency is often only approximate and the interval between observations is not quite a fixed unit. For example, there are usually 365 days in a year based on the Gregorian calendar. However, (almost) every four years there are 366 days (leap years). This compensates for the fact that the Earth completes a rotation around Sol, the sun, in approximately 365.2422 days, on average.

As a simplifying assumption, we often ignore these small discrepancies and proceed as though the sampling frequency and observation intervals are fixed constants. Typically, our results will not be sensitive to approximation when the underlying process is not changing too quickly.

Which one of the following has an exact sampling frequency?

Possible Answers
Monthly observations recorded on the last day of the month for several years
Weekly observations recorded every Friday for several years
Daily observations recorded every "business day" for several weeks
Hourly observations recorded each hour for several days (Correct)

There are always the exact same number of hours in each day without exception.

### Missing values

Sometimes there are missing values in time series data, denoted NA in R, and it is useful to know their locations. It is also important to know how missing values are handled by various R functions. Sometimes we may want to ignore any missingness, but other times we may wish to impute or estimate the missing values.

Let's again consider the monthly AirPassengers dataset, but now the data for the year 1956 are missing. In this exercise, you'll explore the implications of this missing data and impute some new data to solve the problem.

The mean() function calculates the sample mean, but it fails in the presence of any NA values. Use mean(___, na.rm = TRUE) to calculate the mean with all missing values removed. It is common to replace missing values with the mean of the observed values. Does this simple data imputation scheme appear adequate when applied the the AirPassengers dataset?

Instructions
Use plot() to display a simple plot of AirPassengers. Note the missing data for 1956.
Use mean() to calculate the sample mean of AirPassengers with the missing data removed (na.rm = TRUE).
Run the pre-written code to impute the mean values into your missing data.
Use another call to plot() to replot your newly imputed AirPassengers data.
Run the pre-written code to add the complete AirPassengers data to your plot.

```{r}
AirPassengers
AirPassengers[85:96] <- NA
AirPassengers
```



```{r}
# Plot the AirPassengers data
plot(AirPassengers)

# Compute the mean of AirPassengers
mean(AirPassengers, na.rm=TRUE)

# Impute mean values to NA in AirPassengers
AirPassengers[85:96] <- mean(AirPassengers, na.rm = TRUE)

# Generate another plot of AirPassengers
plot(AirPassengers)

# Add the complete AirPassengers data to your plot
rm(AirPassengers)
points(AirPassengers, type = "l", col = 2, lty = 3)

```

Based on your plot, it seems that simple data imputation using the mean is not a great method to approximate what's really going on in the AirPassengers data.
 
## Basic Time Series Objects - Video

### Creating a time series object with ts()

The function ts() can be applied to create time series objects. A time series object is a vector (univariate) or matrix (multivariate) with additional attributes, including time indices for each observation, the sampling frequency and time increment between observations, and the cycle length for periodic data. Such objects are of the ts class, and represent data that has been observed at (approximately) equally spaced time points. Now you will create time series objects yourself.

The advantage of creating and working with time series objects of the ts class is that many methods are available for utilizing time series attributes, such as time index information. For example, as you've seen in earlier exercises, calling plot() on a ts object will automatically generate a plot over time.

In this exercise, you'll familiarize yourself with the ts class by encoding some time series data (saved as data_vector) into ts and exploring the result. Your time series data_vector starts in the year 2004 and has 4 observations per year (i.e. it is quarterly data).

Instructions
Apply print() and plot() to data_vector. Note that, by default, your plot does not contain time information.
Use ts() with data_vector to convert your data to a ts object. Set the start argument equal to 2004 and the frequency argument equal to 4. Assign the result to time_series.
Use print() and plot() to view your time_series object.

```{r}
 data_vector <- 
  c(2.0521941073,  4.2928852797,  3.3294132944,  3.5085950670,  0.0009576938,
 1.9217186345,  0.7978134128,  0.2999543435,  0.9435687536,  0.5748283388,
-0.0034005903,  0.3448649176,  2.2229761136,  0.1763144576,  2.7097622770,
 1.2501948965, -0.4007164754,  0.8852732121, -1.5852420266, -2.2829278891,
-2.5609531290, -3.1259963754, -2.8660295895, -1.7847009207, -1.8894912908,
-2.7255351194, -2.1033141800, -0.0174256893, -0.3613204151, -2.9008403327,
-3.2847440927, -2.8684594718, -1.9505074437, -4.8801892525, -3.2634605353,
-1.6396062522, -3.3012575840, -2.6331245433, -1.7058354022, -2.2119825061,
-0.5170595186,  0.0752508095, -0.8406994716, -1.4022683487, -0.1382114230,
-1.4065954703, -2.3046941055,  1.5073891432,  0.7118679477, -1.1300519022)
```


```{r}
# Use print() and plot() to view data_vector
print(data_vector)
plot(data_vector)

# Convert data_vector to a ts object with start = 2004 and frequency = 4
time_series <- ts(data_vector, start=2004, frequency=4)

# Use print() and plot() to view time_series
print(time_series)
plot(time_series)
```

```{r}
#xts
library(xts)
time_series_xts <- as.xts(time_series)
start(time_series_xts)
end(time_series_xts)
frequency(time_series_xts)
cycle(time_series_xts)
cycle(time_series)
```

As you can see, ts objects are treated differently by commands such as print() and plot(). For example, automatic use of the time-index in your calls to plot() requires a ts object.

### Testing whether an object is a time series

When you work to create your own datasets, you can build them as ts objects. Recall the dataset data_vector from the previous exercise, which was just a vector of numbers, and time_series, the ts object you created from data_vector using the ts() function and information regarding the start time and the observation frequency. As a reminder, data_vector and time_series are shown in the plot on the right.

When you use datasets from others, such as those included in an R package, you can check whether they are ts objects using the is.ts() command. The result of the test is either TRUE when the data is of the ts class, or FALSE if it is not.

In this exercise, you'll explore the class of the datasets you've been using throughout this chapter.

Instructions
 - Use is.ts() on the data_vector and time_series objects from the previous exercise.
 - Use another call to is.ts() to check the class of the Nile dataset used in earlier exercises.
 - Continue the trend by using is.ts() on the AirPassengers dataset.

```{r}
# Check whether data_vector and time_series are ts objects
is.ts(data_vector)
is.ts(time_series)


# Check whether Nile is a ts object
is.ts(Nile)

# Check whether AirPassengers is a ts object
is.ts(AirPassengers)
```
is.ts() is a simple command for determining whether or not you're working with a ts object. As you can see, the Nile and AirPassengers datasets you worked with earlier in the chapter are both encoded as ts objects.


### Plotting a time series object

It is often very useful to plot data we are analyzing, as is the case when conducting time series analysis. If the dataset under study is of the ts class, then the plot() function has methods that automatically incorporate time index information into a figure.

Let's consider the eu_stocks dataset (available in R by default as EuStockMarkets). This dataset contains daily closing prices of major European stock indices from 1991-1998, specifically, from Germany (DAX), Switzerland (SMI), France (CAC), and the UK (FTSE). The data were observed when the markets were open, so there are no observations on weekends and holidays. We will proceed with the approximation that this dataset has evenly spaced observations and is a four dimensional time series.

To conclude this chapter, this exercise asks you to apply several of the functions you've already learned to this new dataset.

Instructions
Use is.ts() to check whether eu_stocks is a ts object.
View the start, end, and frequency of eu_stocks using the start(), end(), and frequency() functions, respectively.
Generate a simple plot of your eu_stocks data using the plot() command.
Generate a more complex time series plot of your eu_stocks data using the ts.plot() command. Input the eu_stocks dataset into the pre-written code, but leave the other arguments as they are.
Use the pre-written code to add a legend to your time series plot.

```{r}
#my code
data(EuStockMarkets)
eu_stocks <- EuStockMarkets
```

```{r}
# Check whether eu_stocks is a ts object
is.ts(eu_stocks)

# View the start, end, and frequency of eu_stocks
start(eu_stocks)
end(eu_stocks)
frequency(eu_stocks)

# Generate a simple plot of eu_stocks
plot(eu_stocks)

# Use ts.plot with eu_stocks
ts.plot(eu_stocks, col = 1:4, xlab = "Year", ylab = "Index Value", main = "Major European Stock Indices, 1991-1998")

# Add a legend to your ts.plot
legend("topleft", colnames(eu_stocks), lty = 1, col = 1:4, bty = "n")
```

You've mastered the basics of the ts class in R, including encoding ts objects, viewing basic time series qualities, and plotting time series data. Now let's move on to the next chapter!

# 2. Predicting the future


## Random or not random?

Some time series exhibit predictability when strong periodic or seasonal patterns are present. Other time series exhibit predictability when positive autocorrelation - or correlation among neighboring observations - induces what appear as short-term trends.

Examine the time series plots on the right. Which pair of time series do NOT appear predictable?

Possible Answers  

 - A and C
 - B and E
 - D and F (Correct)

Time series D and F do not show any obvious predictability. While these series may be predictable, the underlying relationships are not as immediately obvious as the other series.


## Name that trend

Common types of trends in time series data include persistent growth or decay over time, increasing variability over time, and periodic or seasonal patterns, that is regularly spaced peaks and troughs appearing throughout the data.

Which time series exhibits an approximately linear growth pattern over time?

Possible Answers   

Click or Press Ctrl+1 to focus
 - A
 - B (Correct)
 - C
 - D


### Removing trends in variability via the logarithmic transformation

The logarithmic function log() is a data transformation that can be applied to positively valued time series data. It slightly shrinks observations that are greater than one towards zero, while greatly shrinking very large observations. This property can stabilize variability when a series exhibits increasing variability over time. It may also be used to linearize a rapid growth pattern over time.

The time series rapid_growth has already been loaded, and is shown in the figure on the right. Note the vertical range of the data.

Instructions
 - Apply the log() function to rapid_growth, saving the result as linear_growth.
 - Use ts.plot() to show the transformed series linear_growth and note the condensed vertical range of the transformed data.
 
 
 
```{r}
rapid_growth <-  c(
  505.9547,  447.3556,  542.5831,  516.0634,  506.9599,  535.0162,  496.9291,
 497.5626,  577.2483,  536.8560,  541.2459,  473.4978,  550.9890,  569.4106,
 522.9152,  487.2002,  594.6108,  591.1740,  615.9868,  621.3175,  607.1250,
 587.0367,  554.1554,  644.1172,  509.7000,  607.0943,  603.5512,  613.6216,
 544.9143,  670.8118,  687.1316,  615.5817,  711.1873,  694.2979,  681.9293,
 659.1403,  642.7021,  601.5301,  666.7623,  650.9657,  606.0913,  696.6788,
 641.6025,  855.7719,  667.3291,  573.4914,  791.7333,  751.5914,  610.7948,
 624.6503,  833.2990,  639.8867,  736.8283,  772.2923,  686.8865,  667.7631,
 712.9415,  918.1838,  656.1089,  700.4972,  683.4933,  781.7380,  715.6843,
 808.2875,  820.7795,  656.8856,  733.3400,  773.5387,  641.2027,  932.2119,
 680.6766,  988.2828,  664.8986,  813.5283,  883.4088,  924.2749,  969.4321,
 777.3293,  880.9984,  971.3583,  902.9584, 1020.7457, 1075.1483,  886.1707,
 889.6322,  950.3908,  878.0395, 1043.7676,  901.1090, 1079.6584,  933.9054,
 921.9433,  870.8071,  811.1398, 1004.2677, 1008.1758, 1189.4893,  751.9706,
 947.4753,  886.5153, 1074.8943, 1101.1307, 1130.1855,  975.8495,  948.1610,
1177.8227, 1227.1271,  976.9957,  836.7089, 1323.6047,  852.3532, 1200.8262,
1274.4788, 1349.2614, 1102.6334, 1324.8566, 1268.7187, 1058.2289, 1204.0872,
1084.6503, 1284.4305, 1195.2843, 1058.4262, 1188.0577, 1166.5934, 1064.6946,
1429.0685, 1070.8528, 1539.3305, 1467.1571, 1127.7058, 1296.0717, 1555.2741,
1332.9037, 1315.4236, 1189.2462, 1482.4339, 1240.9287, 1237.7720, 1468.6083,
1328.5457, 1589.5078, 1373.1630, 1503.5563, 1659.9376, 1704.6137, 1550.4638,
1625.8026, 1873.8582, 1370.6209, 1439.7114, 1447.4369, 1579.9158, 1681.2571,
1661.6059, 1311.8468, 1326.0308, 1323.0995, 1550.4863, 1606.2042, 1768.5401,
1509.8368, 1592.1086, 1627.6188, 1544.6329, 1439.5234, 1682.3518, 1850.7097,
1673.3801, 1832.4272, 1672.2672, 1781.5768, 1659.2899, 1970.0389, 2044.7124,
1929.0902, 1891.7042, 1487.1577, 2013.8722, 1796.7886, 1977.0183, 1516.9552,
1650.6039, 1523.2834, 1696.6181, 1627.2609, 1787.2968, 1567.2874, 1881.9963,
2318.9833, 1941.9879, 1820.2797, 2154.8123, 2261.5471, 2052.2214, 2079.1710,
2010.0609, 2145.2606, 1775.3008, 2013.4070)
```

```{r}
ts.plot(rapid_growth)
```


```{r}
# Log rapid_growth
linear_growth <- log(rapid_growth)
  
# Plot linear_growth using ts.plot()
 ts.plot(linear_growth)

```

As you can see, the logarithmic transformation helps stabilize your data by inducing linear growth over time. Remember to adjust your interpetation of the data accordingly.

### Removing trends in level by differencing

The first difference transformation of a time series z[t]z[t] consists of the differences (changes) between successive observations over time, that is z[t]???z[t???1]z[t]???z[t???1].

Differencing a time series can remove a time trend. The function diff() will calculate the first difference or change series. A difference series lets you examine the increments or changes in a given time series. It always has one fewer observations than the original series.

The time series z has already been loaded, and is shown in the figure on the right.

Instructions
Apply the diff() function to z, saving the result as dz.
Use ts.plot() to view a time series plot of the transformed series dz.
Use two calls of length() to calculate the number of observations in z and dz, respectively.


```{r}
z <- 
c(6.226447, 6.103354, 6.296341, 6.246230, 6.228432, 6.282297, 6.208447, 6.209721,
6.358273, 6.285730, 6.293874, 6.160147, 6.311715, 6.344602, 6.259419, 6.188675,
6.387907, 6.382110, 6.423225, 6.431842, 6.408735, 6.375087, 6.317445, 6.467881,
6.233822, 6.408684, 6.402831, 6.419378, 6.300629, 6.508489, 6.532526, 6.422568,
6.566936, 6.542901, 6.524926, 6.490936, 6.465681, 6.399477, 6.502434, 6.478457,
6.407031, 6.546325, 6.463969, 6.752004, 6.503283, 6.351743, 6.674225, 6.622193,
6.414761, 6.437192, 6.725393, 6.461291, 6.602355, 6.649363, 6.532169, 6.503933,
6.569399, 6.822398, 6.486327, 6.551790, 6.527217, 6.661520, 6.573239, 6.694918,
6.710255, 6.487510, 6.597609, 6.650976, 6.463346, 6.837560, 6.523087, 6.895969,
6.499635, 6.701381, 6.783788, 6.829010, 6.876710, 6.655864, 6.781056, 6.878695,
6.805677, 6.928289, 6.980214, 6.786910, 6.790808, 6.856873, 6.777692, 6.950592,
6.803626, 6.984400, 6.839375, 6.826484, 6.769420, 6.698440, 6.912014, 6.915898,
7.081279, 6.622697, 6.853801, 6.787298, 6.979978, 7.004093, 7.030137, 6.883308,
6.854524, 7.071423, 7.112431, 6.884482, 6.729476, 7.188114, 6.748001, 7.090765,
7.150293, 7.207313, 7.005457, 7.189060, 7.145763, 6.964352, 7.093477, 6.989013,
7.158071, 7.086139, 6.964538, 7.080075, 7.061843, 6.970443, 7.264778, 6.976211,
7.339103, 7.291082, 7.027941, 7.167093, 7.349407, 7.195115, 7.181914, 7.081075,
7.301441, 7.123615, 7.121068, 7.292071, 7.191840, 7.371180, 7.224872, 7.315588,
7.414535, 7.441094, 7.346309, 7.393757, 7.535755, 7.223019, 7.272198, 7.277550,
7.365127, 7.427297, 7.415540, 7.179191, 7.189945, 7.187732, 7.346324, 7.381629,
7.477910, 7.319757, 7.372815, 7.394873, 7.342542, 7.272067, 7.427948, 7.523324,
7.422601, 7.513397, 7.421936, 7.485254, 7.414145, 7.585809, 7.623012, 7.564804,
7.545233, 7.304622, 7.607815, 7.493756, 7.589345, 7.324460, 7.408897, 7.328623,
7.436392, 7.394653, 7.488460, 7.357102, 7.540088, 7.748884, 7.571467, 7.506745,
7.675459, 7.723804, 7.626678, 7.639725, 7.605920, 7.671016, 7.481725, 7.607584)
```

```{r}
ts.plot(z)
```


```{r}
# Generate the first difference of z
dz <- diff(z)
  
# Plot dz
ts.plot(dz)

# View the length of z and dz, respectively
length(z)
length(dz)

```

By removing the long-term time trend, you can view the amount of change from one observation to the next.


### Removing seasonal trends with seasonal differencing

For time series exhibiting seasonal trends, seasonal differencing can be applied to remove these periodic patterns. For example, monthly data may exhibit a strong twelve month pattern. In such situations, changes in behavior from year to year may be of more interest than changes from month to month, which may largely follow the overall seasonal pattern.

The function diff(..., lag = s) will calculate the lag s difference or length s seasonal change series. For monthly or quarterly data, an appropriate value of s would be 12 or 4, respectively. The diff() function has lag = 1 as its default for first differencing. Similar to before, a seasonally differenced series will have s fewer observations than the original series.

Instructions
 - The time series x has already been loaded, and is shown in the adjoining figure ranging below -10 to above +10. Apply the diff(..., lag = 4) function to x, saving the result as  - dx.
 - Use ts.plot() to show the transformed series dx and note the condensed vertical range of the transformed data.
 - Use two calls of length() to calculate the number of observations in x and dx, respectively.
 

```{r}
x<- 
c(-4.198033,   9.569009,   5.175143,  -9.691646,  -3.215294,  10.843669,
 6.452159, -10.833559,  -2.235351,  10.119833,   6.579646,  -8.656565,
-2.515001,   9.837434,   7.386194,  -8.243504,  -4.264033,   8.898861,
 8.544336,  -8.066913,  -4.023025,   9.822679,   7.772852,  -6.587777,
-3.459171,  10.613851,   7.374450,  -5.798715,  -1.204711,  11.429236,
 7.570047,  -4.968384,  -2.003787,  11.941348,   9.406672,  -4.396585,
-1.555579,  12.599877,   8.502916,  -3.728968,  -2.827000,  13.375981,
 8.128941,  -3.149249,  -2.799473,  13.710570,   6.755217,  -3.779744,
-3.768274,  13.625336,   6.537931,  -3.249098,  -5.024191,  13.355373,
 6.931161,  -3.527354,  -5.197329,  11.579791,   7.162449,  -1.894607,
-5.777797,  12.482695,   6.208088,  -3.434038,  -7.080721,  11.413656,
 6.741990,  -3.532376,  -8.393542,  12.507261,   6.473175,  -3.745246,
-9.426209,  12.380817,   8.048243,  -2.831528,  -7.301893,  12.765838,
 8.223699,  -4.448131,  -6.963558,  12.034005,   7.574925,  -5.402218,
-6.568198,  10.896482,   7.276571,  -4.037873,  -6.723013,  12.180815,
 8.285162,  -4.159342,  -6.360670,  12.753018,   8.665912,  -5.440538,
-4.874932,  12.600197,   8.162589,  -6.539572)

ts.plot(x)
ts.plot(diff(x))
```



```{r}
# Generate a diff of x with lag = 4. Save this to dx
dx <- diff(x, lag=4)
  
# Plot dx
ts.plot(dx)

# View the length of x and dx, respectively 
length(x)
length(dx)
```

Once again differencing allows you to remove the longer-term time trend - in this case, seasonal volatility - and focus on the change from one period to another.

## The White Noise (WN) Model - Video

### Simulate the white noise model

The white noise (WN) model is a basic time series model. It is also a basis for the more elaborate models we will consider. We will focus on the simplest form of WN, independent and identically distributed data.

The arima.sim() function can be used to simulate data from a variety of time series models. ARIMA is an abbreviation for the autoregressive integrated moving average class of models we will consider throughout this course.

An ARIMA(p, d, q) model has three parts, the autoregressive order p, the order of integration (or differencing) d, and the moving average order q. We will detail each of these parts soon, but for now we note that the ARIMA(0, 0, 0) model, i.e., with all of these components zero, is simply the WN model.

In this exercise, you will practice simulating a basic WN model.

Instructions
 - Use arima.sim() to simulate from the WN model with list(order = c(0, 0, 0)). Set the n argument equal to 100 to produce 100 observations. Save this data as white_noise.
 - Plot your white_noise object using ts.plot().
 - Replicate your original call to arima.sim() but this time set the mean argument to 100 and the sd argument to 10. Save this data as white_noise_2.
 - Plot your white_noise_2 object with another call to ts.plot().


```{r}
# Simulate a WN model with list(order = c(0, 0, 0))
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

# Plot your white_noise data
ts.plot(white_noise)

# Simulate from the WN model with: mean = 100, sd = 10
white_noise_2 <- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 100, sd = 10)

# Plot your white_noise_2 data
ts.plot(white_noise_2)
```

The arima.sim() command is a useful way to quickly simulate time series data with the qualities you specify.

### Estimate the white noise model

For a given time series y we can fit the white noise (WN) model using the arima(..., order = c(0, 0, 0)) function. Recall that the WN model is an ARIMA(0,0,0) model. Applying the arima() function returns information or output about the estimated model. For the WN model this includes the estimated mean, labeled intercept, and the estimated variance, labeled sigma^2.

In this exercise, you'll explore the qualities of the WN model. What is the estimated mean? Compare this with the sample mean using the mean() function. What is the estimated variance? Compare this with the sample variance using the var() function.

The time series y has already been loaded, and is shown in the adjoining figure.

Instructions
Use arima() to estimate the WN model for y. Be sure to include the order = c(0, 0, 0) argument after specifying your data.
Calculate the mean and variance of y using mean() and var(), respectively. Compare the results with the output of your arima() command.

```{r}
y <- 
  c(109.76134,  98.31610, 100.63295,  88.74340, 101.87238, 104.62836,  96.64462,
102.86194, 112.76247,  82.23219,  88.94434,  94.60318, 105.61113, 113.82776,
104.82319, 101.29629,  82.98459,  88.96058, 100.60046,  91.03525, 100.19286,
 95.32537,  94.58643, 121.35830,  87.37874,  96.89007,  90.51310,  99.90843,
102.56934, 104.96144, 104.73464,  88.00465, 100.77943, 121.64776,  85.77083,
 79.15428,  98.18847,  99.90006,  98.91084, 101.64422, 102.79526,  84.65112,
 96.47870, 105.81547,  98.51869, 105.24366, 109.61264,  85.14201,  82.81442,
103.03629,  93.56967,  98.08922,  81.25461, 109.18554,  80.43181, 103.55953,
 80.22269,  84.97477, 107.78363,  92.61288,  99.80293, 107.25085,  98.66378,
 91.92275,  98.32642, 112.73402,  96.02811,  92.64909,  83.08484,  97.22196,
106.61361,  97.36943, 108.78465, 104.91858,  84.44343,  85.60786,  96.51529,
 94.18105,  85.02851,  63.26622,  87.22137, 103.52295, 105.60216, 103.26039,
101.11519, 108.45697,  97.67631, 103.01081, 100.61756, 105.58108,  98.72722,
 98.43996,  90.78219,  92.74599, 102.93762,  83.95306, 110.15937, 104.17578,
 99.27876, 103.25115)

ts.plot(y)
```

```{r}
# Fit the WN model to y using the arima command
arima(y, order = c(0, 0, 0))

# Calculate the sample mean and sample variance of y
mean(y)
var(y)

```

## The Random Walk (RW) Model - Video

### Simulate the random walk model

The random walk (RW) model is also a basic time series model. It is the cumulative sum (or integration) of a mean zero white noise (WN) series, such that the first difference series of a RW is a WN series. Note for reference that the RW model is an ARIMA(0, 1, 0) model, in which the middle entry of 1 indicates that the model's order of integration is 1.

The arima.sim() function can be used to simulate data from the RW by including the model = list(order = c(0, 1, 0)) argument. We also need to specify a series length n. Finally, you can specify a sd for the series (increments), where the default value is 1.

Instructions
Use arima.sim() to generate a RW model. Set the model argument equal to list(order = c(0, 1, 0)) to generate a RW-type model and set n equal to 100 to produce 100 observations. Save this to random_walk.
Use ts.plot() to plot your random_walk data.
Use diff() to calculate the first difference of your random_walk data. Save this as random_walk_diff.
Use another call to ts.plot() to plot random_walk_diff.


```{r}
# Generate a RW model using arima.sim
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)

# Plot random_walk
ts.plot(random_walk)

# Calculate the first difference series
random_walk_diff <- diff(random_walk) 

# Plot random_walk_diff
ts.plot(random_walk_diff)
```

As you can see, the first difference of your random_walk data is white noise data. This is because a random walk is simply recursive white noise data. By removing the long-term trend, you end up with simple white noise.

### Simulate the random walk model with a drift

A random walk (RW) need not wander about zero, it can have an upward or downward trajectory, i.e., a drift or time trend. This is done by including an intercept in the RW model, which corresponds to the slope of the RW time trend.

For an alternative formulation, you can take the cumulative sum of a constant mean white noise (WN) series, such that the mean corresponds to the slope of the RW time trend.

To simulate data from the RW model with a drift you again use the arima.sim() function with the model = list(order = c(0, 1, 0)) argument. This time, you should add the additional argument mean = ... to specify the drift variable, or the intercept.

Instructions
Use arima.sim() to generate another RW model. Set the model argument equal to list(order = c(0, 1, 0)) to generate a RW-type model and set n equal to 100 to produce 100 observations. Set the mean argument to 1 to produce a drift. Save this to rw_drift.
Use ts.plot() to plot your rw_drift data.
Use diff() to calculate the first difference of your rw_drift data. Save this as rw_drift_diff.
Use another call to ts.plot() to plot rw_drift_diff.


```{r}
# Generate a RW model with a drift uing arima.sim
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1)

# Plot rw_drift
ts.plot(rw_drift)

# Calculate the first difference series
rw_drift_diff <- diff(rw_drift) 

# Plot rw_drift_diff
ts.plot(rw_drift_diff)
```

Once again, taking the first difference of your random walk data transformed it back into white noise data, regardless of the presence of your long-term drift.

### Estimate the random walk model

For a given time series y we can fit the random walk model with a drift by first differencing the data, then fitting the white noise (WN) model to the differenced data using the arima() command with the order = c(0, 0, 0)) argument.

The arima() command displays information or output about the fitted model. Under the Coefficients: heading is the estimated drift variable, named the intercept. Its approximate standard error (or s.e.) is provided directly below it. The variance of the WN part of the model is also estimated under the label sigma^2.

Instructions
The time series random_walk has already been loaded, and is shown in the adjoining figure. Use diff() to generate the first difference of the data. Save this to rw_diff.
Use ts.plot() to plot your differenced data
Use arima() to fit the WN model for the differenced data. To do so, set the x argument to rw_diff and set the order argument to c(0, 0, 0). Store the model in model_wn.
Store the intercept value of model_wn in int_wn. You can obtain this value using model_wn$coef.
Use ts.plot() to reproduce your original plot of random_walk.
Add the estimated time trend to the adjoining plot with the function abline(). You can use int_wn as the second argument.


```{r}
ts.plot(rw_drfit)
```


```{r}
# Difference your random_walk data
rw_diff <- diff(random_walk)

# Plot rw_diff
ts.plot(rw_diff)

# Now fit the WN model to the differenced data
model_wn <- arima(rw_diff, order = c(0, 0, 0))

# Store the value of the estimated time trend (intercept)
int_wn <- model_wn$coef

# Plot the original random_walk data
ts.plot(random_walk)

# Use abline(0, ...) to add time trend to the figure
abline(0, int_wn)

```

The arima() command correctly identified the time trend in your original random-walk data.

## Stationary Processes - Video

Stationary or not?
50xp
There are many commonly encountered departures from stationarity, including time trends, periodicity, and a lack of mean reversion.

Which of the three time series plots on your right appear to be from a stationary model?

Possible Answers
 - A
 - B (Correct)
 - C
 
 Time series B is not periodic but does appear to be mean reverting. Of the options, this is the most likely to be stationary.
 
### Are the white noise model or the random walk model stationary?

The white noise (WN) and random walk (RW) models are very closely related. However, only the RW is always non-stationary, both with and without a drift term. This is a simulation exercise to highlight the differences.

Recall that if we start with a mean zero WN process and compute its running or cumulative sum, the result is a RW process. The cumsum() function will make this transformation for you. Similarly, if we create a WN process, but change its mean from zero, and then compute its cumulative sum, the result is a RW process with a drift.

Instructions 

- Use arima.sim() to generate a WN model. Set the model argument equal to list(order = c(0, 0, 0)) to generate a WN-type model and set n equal to 100 to produce 100 observations. Save this to white_noise.
- Use the cumsum() function on white_noise to quickly convert your WN model to RW data. Save this to random_walk.
- Use a similar call toarima.sim() to generate a second WN model. Keep all arguments the same, but this time set the mean argument to 0.4. Save this to wn_drift.
Use another call to cumsum() to convert your wn_drift dat


```{r}
# Use arima.sim() to generate WN data
white_noise <- arima.sim(model=list(order=c(0,0,0)), n=100)

# Use cumsum() to convert your WN data to RW
random_walk <- cumsum(white_noise)
  
# Use arima.sim() to generate WN drift data
wn_drift <- arima.sim(model=list(order=c(0,0,0)), n=100, mean=0.4)
  
# Use cumsum() to convert your WN drift data to RW
rw_drift <- cumsum(wn_drift)

# Plot all four data objects
plot.ts(cbind(white_noise, random_walk, wn_drift, rw_drift))

```


As you can see, it is easy to reverse-engineer the RW data by simply generating a cumulative sum of white noise data.

# 3. Correlation analysis and the autocorrelation function

## Scatterplots - Video

### Asset prices vs. asset returns

The goal of investing is to make a profit. The revenue or loss from investing depends on the amount invested and changes in prices, and high revenue relative to the size of an investment is of central interest. This is what financial asset returns measure, changes in price as a fraction of the initial price over a given time horizon, for example, one business day.

Let's again consider the eu_stocks dataset. This dataset reports index values, which we can regard as prices. The indices are not investable assets themselves, but there are many investable financial assets that closely track major market indices, including mutual funds and exchange traded funds.

Log returns, also called continuously compounded returns, are also commonly used in financial time series analysis. They are the log of gross returns, or equivalently, the changes (or first differences) in the logarithm of prices.

The change in appearance between daily prices and daily returns is typically substantial, while the difference between daily returns and log returns is usually small. As you'll see later, one advantage of using log returns is that calculating multi-period returns from individual periods is greatly simplified - you just add them together!

In this exercise, you'll further explore the eu_stocks dataset, including plotting prices, converting prices to (net) returns, and converting prices to log returns.

Instructions
Use plot() to generate a plot of the eu_stocks data.
Use the pre-written code to convert daily prices in the eu_stocks data to daily net returns.
Use ts() to convert returns to a ts object. Set the start argument equal to c(1991, 130) and set the frequency argument equal to 260.
Use another call to plot() to view daily net returns.
Use the pre-written code combining diff() and log() to generate logreturns.
Use a final call to plot() to view daily log returns.


```{r}
# Plot eu_stocks
plot(eu_stocks)

# Use this code to convert prices to returns
returns <- eu_stocks[-1,] / eu_stocks[-1860,] - 1

# Convert returns to ts
returns <- ts(returns, start = c(1991, 130), frequency = 260)

# Plot returns
plot(returns)

# Use this code to convert prices to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)
```

Daily net returns and daily log returns are two valuable metrics for financial data.

### Characteristics of financial time series

Daily financial asset returns typically share many characteristics. Returns over one day are typically small, and their average is close to zero. At the same time, their variances and standard deviations can be relatively large. Over the course of a few years, several very large returns (in magnitude) are typically observed. These relative outliers happen on only a handful of days, but they account for the most substantial movements in asset prices. Because of these extreme returns, the distribution of daily asset returns is not normal, but heavy-tailed, and sometimes skewed. In general, individual stock returns typically have even greater variability and more extreme observations than index returns.

In this exercise, you'll work with the eu_percentreturns dataset, which is the percentage returns calculated from your eu_stocks data. For each of the four indices contained in your data, you'll calculate the sample mean, variance, and standard deviation.

Notice that the average daily return is about 0, while the standard deviation is about 1 percentage point. Also apply the hist() and qqnorm() functions to make histograms and normal quantile plots, respectively, for each of the indices.

Instructions
Use colMeans() to calculate the sample mean for each column in your eu_percentreturns data.
Use apply() to calculate the sample variance for each index. Leave the MARGIN argument at 2 and set the FUN argument to var.
Use another call to apply() to calculate the standard deviation for each index. Keep the MARGIN argument at 2 but this time set the FUN argument to sd.
Use a third call to apply() to display a histogram of percent returns for each index.
Use a final call to apply() to display normal quantile plots for each index.

```{r}
eu_percentreturns <- eu_stocks[-1,] / eu_stocks[-1860,] - 1
eu_percentreturns <- eu_percentreturns*100
```

```{r}
# Generate means from eu_percentreturns
colMeans(eu_percentreturns)

# Use apply to calculate sample variance from eu_percentreturns
apply(eu_percentreturns, MARGIN = 2, FUN = var)

# Use apply to calculate standard deviation from eu_percentreturns
apply(eu_percentreturns, MARGIN = 2, FUN = sd)

# Display a histogram of percent returns for each index
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = hist, main = "", xlab = "Percentage Return")

# Display normal quantile plots of percent returns for each index
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = qqnorm, main = "")
qqline(eu_percentreturns)

```
Note that the vast majority of returns are near zero, but some daily returns are greater than 5 percentage points in magnitude. Similarly, note the clear departure from normality, especially in the tails of the distributions, as evident in the normal quantile plots.

### Plotting pairs of data

Time series data is often presented in a time series plot. For example, the index values from the eu_stocks dataset are shown in the adjoining figure. Recall, eu_stocks contains daily closing prices from 1991-1998 for the major stock indices in Germany (DAX), Switzerland (SMI), France (CAC), and the UK (FTSE).

It is also useful to examine the bivariate relationship between pairs of time series. In this exercise we will consider the contemporaneous relationship, that is matching observations that occur at the same time, between pairs of index values as well as their log returns. The plot(a, b) function will produce a scatterplot when two time series names a and b are given as input.

To simultaneously make scatterplots for all pairs of several assets the pairs() function can be applied to produce a scatterplot matrix. When shared time trends are present in prices or index values it is common to instead compare their returns or log returns.

In this exercise, you'll practice these skills on the eu_stocks data. Because the DAX and FTSE returns have similar time coverage, you can easily make a scatterplot of these indices. Note that the normal distribution has elliptical contours of equal probability, and pairs of data drawn from the multivariate normal distribution form a roughly elliptically shaped point cloud. Do any of the pairs in the scatterplot matrices exhibit this pattern, before or after log transformation?

Instructions
Use plot() to make a scatterplot of DAX and FTSE.
Use pairs() to make a scatterplot matrix of the four indices in eu_stocks.
Generate logreturns from eu_stocks using diff(log(___)).
Use another call to plot() to generate a simple time series plot of logreturns.
Use another call to pairs() to generate a scatterplot matrix for logreturns.

```{r}
#my code
plot(eu_stocks)
class(eu_stocks)
colnames(eu_stocks)
DAX <- eu_stocks[,"DAX"]
FTSE <- eu_stocks[,"FTSE"]
par(mfrow=c(1,1))
```

```{r}
# Make a scatterplot of DAX and FTSE
plot(DAX, FTSE)

# Make a scatterplot matrix of eu_stocks
pairs(eu_stocks)

# Convert eu_stocks to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)

# Make a scatterplot matrix of logreturns
pairs(logreturns)
```

pairs() command is a useful way to quickly check for relationships between your indices.

## Covariance and Correlation - Video

Calculating sample covariances and correlations
100xp
Sample covariances measure the strength of the linear relationship between matched pairs of variables. The cov() function can be used to calculate covariances for a pair of variables, or a covariance matrix when a matrix containing several variables is given as input. For the latter case, the matrix is symmetric with covariances between variables on the off-diagonal and variances of the variables along the diagonal. On the right you can see the scatterplot matrix of your logreturns data.

Covariances are very important throughout finance, but they are not scale free and they can be difficult to directly interpret. Correlation is the standardized version of covariance that ranges in value from -1 to 1, where values close to 1 in magnitude indicate a strong linear relationship between pairs of variables. The cor() function can be applied to both pairs of variables as well as a matrix containing several variables, and the output is interpreted analogously.

In this exercise, you'll use cov() and cor() to explore your logreturns data.

Instructions
Use cov() to calculate the sample covariance between DAX_logreturns and FTSE_logreturns.
Use another call to cov() to calculate the sample covariance matrix for logreturns.
Use cor() to calculate the sample correlation between DAX_logreturns and FTSE_logreturns.
Use another call to cor() to calculate the sample correlation matrix for logreturns.

```{r}
DAX_logreturns <- diff(log(DAX))
FTSE_logreturns <- diff(log(FTSE))
```

```{r}
# Use cov() with DAX_logreturns and FTSE_logreturns
cov(DAX_logreturns , FTSE_logreturns)

# Use cov() with logreturns
cov(logreturns)

# Use cor() with DAX_logreturns and FTSE_logreturns
cor(DAX_logreturns , FTSE_logreturns)

# Use cor() with logreturns
cor(logreturns)
```

The cov() and cor() commands provide a simple and intuitive output for comparing the relationships between your indices, especially when a scatterplot matrix is difficult to interpret.


### Guess the correlation coefficient

Which scatterplot has a sample correlation of approximately 0.90?

Possible Answers
Click or Press Ctrl+1 to focus
 - A
 - B
 - C (Correct)
 - D

## Autocorrelation - Video



```{r}
x <-
c(2.06554379,  1.29963803,  0.03357800, -0.34258065,  0.23256126,  0.46812008,
 4.34111562,  2.82007636,  2.90799984,  2.33495061,  1.15989954,  0.82008659,
-0.24338927, -0.03355907, -1.53548216, -0.69363797, -1.41731648, -0.76623179,
 0.83536060,  0.04395345,  1.07447506,  1.50200360, -0.21238609,  0.32996521,
-0.75033470, -0.10522038,  0.20471918, -0.17170595,  0.87181378,  1.47213721,
 0.84112591,  0.96430157,  0.66829027, -0.25752691,  0.08193916, -1.46057000,
-1.26726830, -2.19329186, -2.21008902,  0.42338945, -1.01513893, -1.54446229,
-0.72524036,  0.70352378, -0.36108456, -0.77422092, -0.50023603,  1.31369378,
 1.15621723,  0.68782375, -0.79475183,  0.32563325,  2.00955556,  1.70614293,
 0.99910640,  0.68932712,  0.65764259,  1.51403467,  0.85806413,  1.96951273,
 2.98268339,  3.01781322,  1.30009671,  0.71140225,  0.40782908, -0.53429804,
-0.21147251,  1.72814428, -0.75541665, -1.34178777, -1.72317007, -2.78147841,
-1.72572507, -3.49466071, -2.41789449, -0.13744248, -0.15805310, -0.27865357,
-0.97493500, -1.52666608, -1.04093146, -1.26059748, -1.44067012, -1.23902633,
-0.44668174,  1.12562870,  3.25518488,  1.13570549,  0.98992411,  0.38244269,
 2.71124649,  2.42216865,  1.78509981, -1.03092109, -1.06607323, -2.63465306,
-2.66808169, -1.30411399, -1.04269885,  0.40215260, -0.48928251, -0.49381470,
-1.08457733, -0.27456945, -1.84390881, -2.09907629, -1.88923578, -1.84534263,
-0.33812159, -1.20911695, -0.50157701, -0.58298734, -1.66575871, -1.41327839,
-2.55380296, -0.86895290, -2.16915012, -2.60202618, -2.05678159, -0.87654110,
 1.32919650,  1.07620974, -0.96432698, -1.81480027, -2.05757608, -2.34353353,
-0.01467163,  0.77321454,  0.03106214,  1.16999559,  2.67732293,  4.57761736,
 4.90582958,  4.13300371,  4.04398099,  1.35081333,  0.61429043,  1.42969023,
 0.79231154,  1.34178061,  2.22016551,  2.82502290,  2.43279283,  1.89023418,
 0.46877402, -1.30680558, -1.45910588,  0.21169330,  1.10203354,  1.42360646)

x <- as.ts(x, start=1, end=150, frequency=1)
x
plot(x)
n<-150
```

```{r}
# Define x_t0 as x[-1]
x_t0 <- x[-1]

# Define x_t1 as x[-n]
x_t1 <- x[-n]

# Confirm that x_t0 and x_t1 are (x[t], x[t-1]) pairs  
head(cbind(x_t0, x_t1))
  
# Plot x_t0 and x_t1
plot(x_t0, x_t1)

# View the correlation between x_t0 and x_t1
cor(x_t0, x_t1)

# Use acf with x
acf(x, lag.max = 1, plot = FALSE)

# Confirm that difference factor is (n-1)/n
cor(x_t1, x_t0) * (n-1)/n

```
As you can see, the acf() command is a helpful shortcut for calculating autocorrelation. In the next few exercises, you'll explore additional features of this command.


### The autocorrelation function

Autocorrelations can be estimated at many lags to better assess how a time series relates to its past. We are typically most interested in how a series relates to its most recent past.

The acf(..., lag.max = ..., plot = FALSE) function will estimate all autocorrelations from 0, 1, 2,..., up to the value specified by the argument lag.max. In the previous exercise, you focused on the lag-1 autocorrelation by setting the lag.max argument to 1.

In this exercise, you'll explore some further applications of the acf() command. Once again, the time series x has been preloaded for you and is shown in the plot on the right.

Instructions
Use acf() to view the autocorrelations of series x from 0 to 10. Set the lag.max argument to 10 and keep the plot argument as FALSE.
Copy and paste the autocorrelation estimate (ACF) at lag-10.
Copy and paste the autocorrelation estimate (ACF) at lag-5.

```{r}
# Generate ACF estimates for x up to lag-10
acf(x, lag.max = 10, plot = FALSE)

# Type the ACF estimate at lag-10 
0.100

# Type the ACF estimate at lag-5
acf(x, lag.max = 5, plot = FALSE)
0.198
```

Since autocorrelations may vary by lag, we often consider autocorrelations as a function of the time lag. Taking this view, we have now estimated the autocorrelation function (ACF) of x from lags 0 to 10.

### Visualizing the autocorrelation function

Estimating the autocorrelation function (ACF) at many lags allows us to assess how a time series x relates to its past. The numeric estimates are important for detailed calculations, but it is also useful to visualize the ACF as a function of the lag.

In fact, the acf() command produces a figure by default. It also makes a default choice for lag.max, the maximum number of lags to be displayed.

Three time series x, y, and z have been loaded into your R environment and are plotted on the right. The time series x shows strong persistence, meaning the current value is closely relatively to those that proceed it. The time series y shows a periodic pattern with a cycle length of approximately four observations, meaning the current value is relatively close to the observation four before it. The time series z does not exhibit any clear pattern.

In this exercise, you'll plot an estimated autocorrelation function for each time series. In the plots produced by acf(), the lag for each autocorrelation estimate is denoted on the horizontal axis and each autocorrelation estimate is indicated by the height of the vertical bars. Recall that the ACF at lag-0 is always 1.

Finally, each ACF figure includes a pair of blue, horizontal, dashed lines representing lag-wise 95% confidence intervals centered at zero. These are used for determining the statistical significance of an individual autocorrelation estimate at a given lag versus a null value of zero, i.e., no autocorrelation at that lag.

Instructions
Use three calls of the function acf() to display the estimated ACFs of each of your three time series (x, y, and z). There is no need to specify additional arguments in your calls to acf().


```{r}
x <- 
c(-0.037194501, -0.677433681, -0.734627392, -1.530753268, -2.270474608,
-1.966437800, -0.964217164, -0.525467600, -0.894389375, -0.588577268,
 1.173990730,  0.237214322,  0.495339401,  0.451069901, -0.074932360,
 0.394447503,  1.693747554,  0.128914228, -0.377837870,  0.683405464,
 1.725439015,  1.441053061,  0.601169075,  0.056994730,  0.065822886,
-1.115337753, -0.637718763, -2.108544712, -1.634415325, -0.973879182,
-3.366323549, -3.008531119, -4.468204287, -4.133150702, -5.637990138,
-5.004450904, -3.228336203, -2.901650268, -2.652243606, -2.294641059,
-3.405546558, -2.195885228, -0.020223871,  0.008109970, -1.067319445,
-0.586266076,  0.361776066, -0.791487797, -0.724304302, -0.237560152,
-0.006248334, -0.887176947, -1.354322231, -2.612805891, -1.703890685,
-0.967298696,  0.407238148,  1.216422286,  2.584758701,  4.094657375,
 1.322900354,  2.301040000,  1.051197538,  1.034542353,  0.328192908,
-0.254407390,  0.115365920, -0.095501070, -1.291159946, -2.435269825,
-0.340221863, -0.161246116, -0.193856207,  0.013028505,  0.669988138,
 0.258337585,  0.408097524,  0.634652500,  0.787217886,  0.210862379,
 0.571223692,  1.451818580,  1.149416530,  3.409950799,  0.329495342,
 0.494241044, -0.782309694, -1.250713411, -2.174554967, -1.332112529,
-0.257980715,  0.696176974,  1.802774679,  1.133650234,  0.340899367,
 1.205502299,  2.517728443,  1.459118786, -0.076640633, -1.048398286,
 0.459194455, -0.119216184,  0.019025972,  0.481456920,  0.529825545,
 3.183668968,  2.544618736,  3.263691190,  1.888815173,  1.813036930,
 0.151505241, -0.588788733,  0.689931482, -0.720430786, -0.857841030,
-1.287168802, -1.528266568, -1.206549591, -2.332675975, -2.767496901,
-3.079377674, -1.889327045, -1.804902569, -1.724744275, -2.020248888,
-1.884500588, -1.856917035, -0.569246079,  0.450422614, -0.684973024,
 0.144127886, -0.458534128, -0.716389570,  0.009236847, -0.269221755,
 0.407551169,  1.515129352,  1.917948529,  2.316384191,  0.863659606,
 0.867735821, -0.244412447, -1.637805103, -2.346180265, -0.933662561,
-0.702843105, -1.651374177, -1.455613228, -0.165745533, -0.329520215)

x <- as.ts(x, start=1, end=150, frequency=1)

y<-
c(-1.36312061, -2.00706364,  1.45862211,  5.73596121, -0.60382327, -1.29457456,
 1.26073028,  5.43833855, -1.15856432, -2.09237657,  1.03041627,  5.79199689,
-0.52857197,  0.49945465,  0.93698101,  4.71218783,  2.55730575,  1.31872442,
 2.03319549,  4.46472001,  1.99539841,  1.53993613, -0.41050999,  4.89124243,
 0.48171008,  2.58247253, -0.76327207,  5.17655207,  0.56927510,  3.99778743,
 0.47932186,  3.46193333, -0.74241167,  3.58169587, -1.83431881,  3.30728790,
 0.89369984,  4.39325954, -0.53474517,  3.21540770,  0.60495783,  4.75421555,
 0.36398808,  2.09948327,  2.12138476,  4.17727349,  1.05270740,  2.48130175,
 3.87802330,  4.34323438,  2.66268151,  1.74358744,  6.08305141,  4.76219086,
 1.74355060,  2.01715054,  6.51292831,  5.34522316,  0.63267285,  3.04333011,
 5.87230692,  4.10617194,  0.14317627,  2.81641043,  5.29614428,  3.71768120,
 1.70310196,  2.25183188,  4.08823196,  3.57623779,  1.08373353,  0.59246619,
 2.83002532,  3.03413999,  1.84497378,  0.25482385,  3.19478754,  1.86707192,
 0.60803419,  2.62445725,  3.10411047,  2.17015920, -0.08714403,  3.05889950,
 3.75102535,  1.83216352,  0.93291214,  4.72282190,  2.82083762,  1.33198603,
 0.23962404,  4.43311811,  3.37442222,  0.92800165,  2.10145008,  4.94258268,
 3.51720883,  1.84184773,  0.58222673,  4.26214368,  2.34723348,  0.12252827,
 0.03479805,  5.62552268,  4.22520184,  0.69489327,  0.84611684,  6.52341813,
 2.92615502,  0.76643124,  0.24164560,  5.07212453,  2.15602964,  0.56907973,
-1.05187563,  4.85023508,  1.20440866,  2.72916194,  0.82844328,  1.48136662,
-1.80317587,  2.22258833,  0.81637669,  1.57200513, -1.60120489,  0.09871604,
 1.69373402,  1.61517455, -2.15836207,  0.27236347,  1.63620429,  1.47704362,
-2.18305137,  0.72164683,  1.85062964,  0.81439654, -1.24802828,  0.49619477,
 2.98246697,  1.45211614, -1.67283377,  0.22936477,  2.82805064,  2.40730918,
-0.04598149,  1.62622152,  5.61024673,  2.94489352, -0.77068115,  0.44378786)

y <- as.ts(y, start=1, end=150, frequency=1)

z<-
 c(0.315737982,  1.735435794, -0.009264601,  0.813628825, -0.928560124,
-1.153110489,  0.862803689,  0.531286714, -1.165746953, -1.812795089,
 1.612365119,  0.027197561, -0.441448129,  0.521515748,  0.669594188,
 0.660542458, -0.602768657,  0.310652090, -0.494678887, -1.106559735,
 0.570986432, -1.001931054,  0.257165196,  0.329057029, -1.938901124,
-0.856635089, -1.363198805, -0.571609991,  0.805302779, -0.496244427,
 0.173629336, -0.504015663,  0.131110158,  0.420891975, -0.229286900,
-0.578447634, -0.469345203,  0.364004211, -0.865851014,  0.422678125,
 0.463833244, -0.792281756, -0.764270102, -0.550418202,  0.566427460,
 0.145399530,  0.482729502,  0.474884833, -0.169909270,  1.205351065,
 0.775557894, -0.033469793,  0.117892591,  0.233605621,  0.127379648,
 0.949562978,  0.447922677, -0.958977828,  1.425274554,  0.501950210,
-2.396122620,  0.046683748, -0.167688833,  0.663482053,  0.181327339,
 0.220006847, -1.990310779,  1.079236931, -0.867702624,  0.686075605,
 0.481551351, -2.112520117,  1.367933771,  1.464156025,  0.071579135,
 0.302393657, -1.100968196,  0.116053587, -0.042550203,  0.137278303,
 0.361569682, -0.192487807, -0.305112479,  3.129497948, -0.378200092,
 0.716723267, -0.711138060,  0.181078209,  0.688958814,  0.815627106,
-0.799253104,  0.044462384,  0.540432742, -0.621667266,  0.544608472,
-0.365283608, -0.759197648, -1.492074332, -1.170489426, -1.567302671,
-1.612972464,  1.254711018, -0.322366918,  1.430948267, -0.315613777,
 0.166029069,  0.194490822, -0.799452619, -1.251505298, -2.430421725,
 0.179806635, -0.307539344,  0.504349645, -0.442137594, -0.363753810,
-2.189014968,  0.525841943, -0.484855644,  0.211211543, -0.097135106,
-0.965549103,  0.016006815, -0.059986409, -0.155469144,  0.100895284,
 0.061932849, -0.734558776, -0.318335104,  1.038486462,  1.085413328,
 0.690501645,  0.859616760,  0.432452904,  1.346333552,  1.928117389,
 0.014782533,  0.971231160,  0.304937611, -0.771593550, -1.538396903,
-1.304015359, -0.640125563,  1.133903483,  0.029736623,  0.738796828,
 1.925339777,  0.988289753,  1.009784424, -0.213503262,  1.477741635)

z <- as.ts(z, start=1, end=150, frequency=1)

```


```{r}
# View the ACF of x
acf(x)

# View the ACF of y
acf(y)

# View the ACF of z
acf(z)
```

Plotting the estimated ACF of x shows large positive correlations for several lags which quickly decay towards zero. Plotting the estimated ACF of y shows large positive correlations at lags which are multiples of four, although these also decay towards zero as the lag multiple increases. Finally, the estimated ACF of z is near zero at all lags. It appears the series z is not linearly related to its past, at least through lag 20.

# 4. Autoregression

## Autoregressive Model

### Simulate the autoregressive model

The autoregressive (AR) model is arguably the most widely used time series model. It shares the very familiar interpretation of a simple linear regression, but here each observation is regressed on the previous observation. The AR model also includes the white noise (WN) and random walk (RW) models examined in earlier chapters as special cases.

The versatile arima.sim() function used in previous chapters can also be used to simulate data from an AR model by setting the model argument equal to list(ar = phi) , in which phi is a slope parameter from the interval (-1, 1). We also need to specify a series length n.

In this exercise, you will use this command to simulate and plot three different AR models with slope parameters equal to 0.5, 0.9, and -0.75, respectively.

Instructions
Use arima.sim() to simulate 100 observations of an AR model with slope equal to 0.5. To do so, set the model argument equal to list(ar = 0.5) and set the n argument equal to 100. Save this simulated data to x.
Use a similar call to arima.sim() to simulate 100 observations of an AR model with slope equal to 0.9. Save this data to y.
Use a third call to arima.sim() to simulate 100 observations of an AR model with slope equal to -0.75 Save this data to z.
Use plot.ts() with cbind() to plot your three ts objects (x, y, z).




```{r}
# Simulate an AR model with 0.5 slope
x <- arima.sim(model = list(ar = 0.5), n = 100)

# Simulate an AR model with 0.9 slope
y <- arima.sim(model = list(ar = 0.9), n = 100)

# Simulate an AR model with -0.75 slope
z <- arima.sim(model = list(ar = -0.75), n = 100)

# Plot your simulated data
plot.ts(cbind(x,y,z))
```


As you can see, your x data shows a just a moderate amount of autocorrelation while your y data shows a large amount of autocorrelation. Alternatively, your z data tends to oscillate considerably from one observation to the next.

### Estimate the autocorrelation function (ACF) for an autoregression

What if you need to estimate the autocorrelation function from your data? To do so, you'll need the acf() command, which estimates autocorrelation by exploring lags in your data. By default, this command generates a plot of the relationship between the current observation and lags extending backwards.

In this exercise, you'll use the acf() command to estimate the autocorrelation function for three new simulated AR series (x, y, and z). These objects have slope parameters 0.5, 0.9, and -0.75, respectively, and are shown in the adjoining figure.

Instructions
Use three calls to acf() to calculate the ACF for x, y, and z, respectively.

```{r}
# Calculate the ACF for x
acf(x)

# Calculate the ACF for y
acf(y)

# Calculate the ACF for z
acf(z)

```

The plots generated by the acf() command provide useful information about each lag of your time series. The first series x has positive autocorrelation for the first couple lags, but they quickly approach zero. The second series y has positive autocorrelation for many lags, but they also decay to zero. The last series z has an alternating pattern, as does its autocorrelation function (ACF), but its ACF still quickly decays to zero in magnitude.


### Persistence and anti-persistence

Autoregressive processes can exhibit varying levels of persistence as well as anti-persistence or oscillatory behavior. Persistence is defined by a high correlation between an observation and its lag, while anti-persistence is defined by a large amount of variation between an observation and its lag.

The four plots on the right show varying degrees of persistence and anti-persistence. Which series exhibits the greatest persistence?

Possible Answers
A
B
C
D (Correct)

Plot D shows a very high degree of persistence, with a clear downward trend over time.

### Compare the random walk (RW) and autoregressive (AR) models

The random walk (RW) model is a special case of the autoregressive (AR) model, in which the slope parameter is equal to 1. Recall from previous chapters that the RW model is not stationary and exhibits very strong persistence. Its sample autocovariance function (ACF) also decays to zero very slowly, meaning past values have a long lasting impact on current values.

The stationary AR model has a slope parameter between -1 and 1. The AR model exhibits higher persistence when its slope parameter is closer to 1, but the process reverts to its mean fairly quickly. Its sample ACF also decays to zero at a quick (geometric) rate, indicating that values far in the past have little impact on future values of the process.

In this exercise, you'll explore these qualities by simulating and plotting additional data from an AR model.

Instructions
 - Use arima.sim() to simulate 200 observations from an AR model with slope 0.9. Save this to x.
 - Use ts.plot() to plot x and use acf() to view its sample ACF.
 - Now do the same from an AR model with slope 0.98. Save this to y.
Now do the same from a RW model (z), and compare the time series and sample ACFs generated by these three models.

```{r}
# Simulate and plot AR model with slope 0.9 
x <- arima.sim(model = list(ar = 0.9), n = 200)
ts.plot(x)
acf(x)

# Simulate and plot AR model with slope 0.98
y <- arima.sim(model = list(ar = 0.98), n = 200)
ts.plot(y)
acf(y)

# Simulate and plot RW model
z <- arima.sim(model = list(order = c(0, 1, 0)), n = 200)
ts.plot(z)
acf(z)
```

As you can see, the AR model represented by series y exhibits greater persistence than series x, but the ACF continues to decay to 0. By contrast, the RW model represented by series z shows considerable persistence and relatively little decay in the ACF.

## AR Model Estimation and Forecasting - Video

Estimate the autoregressive (AR) model
100xp
For a given time series x we can fit the autoregressive (AR) model using the arima() command and setting order equal to c(1, 0, 0). Note for reference that an AR model is an ARIMA(1, 0, 0) model.

In this exercise, you'll explore additional qualities of the AR model by practicing the arima() command on a simulated time series x as well as the AirPassengers data. This command allows you to identify the estimated slope (ar1), mean (intercept), and innovation variance (sigma^2) of the model.

Both xand the AirPassengers data are preloaded in your environment. The time series x is shown in the figure on the right.

Instructions
Use arima() to fit the AR model to the series x. Closely examine the output from this command.
What are the slope (ar1), mean (intercept), and innovation variance (sigma^2) estimates from your previous command? Type them into your R workspace.
Now, fit the AR model to AirPassengers, saving the results as AR. Use print() to display the fitted model AR.
Finally, use the commands provided to plot the AirPassengers, calculate the fitted values, and add them to the figure.

```{r}
#  0.82934122  0.45827452  0.05278152  0.06302386 -0.73644893 -0.56805492
# -0.05563039 -0.14757696 -0.46071786 -0.75699438 -1.57092782 -0.23143325
# -1.26147738 -0.73773592 -0.75028753 -1.92100782 -2.47286807 -3.55173257
# -1.91249277 -4.19513658 -2.81785994 -3.13869993 -1.29584598 -0.79640828
#  0.83047167 -0.21016064 -0.31346170  0.05866471  1.52705544  3.76088847
#  3.25450918  2.58562423  1.21400134  1.49023414  2.38877470  3.56561519
#  3.84286330  4.94002290  4.68469270  3.24650595  2.39817013  2.10741933
#  1.64408812 -0.18478060 -1.97232159 -0.34283939 -2.11707044 -2.69276747
# -2.26053239 -2.45553511 -2.07961804 -2.38469163 -1.55285213 -2.66500950
# -3.95567413 -2.09065332 -1.69197443 -1.30314985 -2.69825581 -2.09330095
# -2.65812196 -2.57208874 -1.59851941 -1.71253922 -1.58704322 -1.10272266
# -1.19439780 -1.33265837 -0.30011772 -0.21845395  1.67545978  1.19913602
#  1.16528149  1.65699378 -0.53121627 -0.92268706 -0.91208664 -0.69114086
# -0.51678891 -0.81106737  1.78528941  3.08153265  1.49790696  1.81441753
#  2.77403381  2.59228332  2.43284863  0.69862688 -0.31540054 -1.04872127
#  1.06241883  1.69360044  2.75453477  1.54577888  0.90824757  2.49094442
#  1.92578525 -0.29636244 -0.73137478 -1.39532217
```

### Estimate the autoregressive (AR) model

For a given time series x we can fit the autoregressive (AR) model using the arima() command and setting order equal to c(1, 0, 0). Note for reference that an AR model is an ARIMA(1, 0, 0) model.

In this exercise, you'll explore additional qualities of the AR model by practicing the arima() command on a simulated time series x as well as the AirPassengers data. This command allows you to identify the estimated slope (ar1), mean (intercept), and innovation variance (sigma^2) of the model.

Both xand the AirPassengers data are preloaded in your environment. The time series x is shown in the figure on the right.

Instructions
Use arima() to fit the AR model to the series x. Closely examine the output from this command.
What are the slope (ar1), mean (intercept), and innovation variance (sigma^2) estimates from your previous command? Type them into your R workspace.
Now, fit the AR model to AirPassengers, saving the results as AR. Use print() to display the fitted model AR.
Finally, use the commands provided to plot the AirPassengers, calculate the fitted values, and add them to the figure.

```{r}
# Fit the AR model to x
arima(x, order = c(1, 0, 0))

# Copy and paste the slope (ar1) estimate
0.8575

# Copy and paste the slope mean (intercept) estimate
-0.0948

# Copy and paste the innovation variance (sigma^2) estimate
1.022

# Fit the AR model to AirPassengers
AR <- arima(AirPassengers, order = c(1, 0, 0))
print(AR)

# Run the following commands to plot the series and fitted values
ts.plot(AirPassengers)
AR_fitted <- AirPassengers - residuals(AR)
points(AR_fitted, type = "l", col = 2, lty = 2)

```


By fitting an AR model to the AirPassengers data, you've succesfully modeled the data in a reproducible fashion. This allows you to predict future observations based on your AR_fitted data.

```{r}
# Fit an AR model to Nile
AR_fit <-arima(Nile, order  = c(1, 0, 0))
print(AR_fit)

# Use predict() to make a 1-step forecast
predict_AR <- predict(AR_fit)

# Obtain the 1-step forecast using $pred[1]
predict_AR$pred[1]

# Use predict to make 1-step through 10-step forecasts
predict(AR_fit, n.ahead = 10)

# Run to plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(Nile, xlim = c(1871, 1980))
AR_forecast <- predict(AR_fit, n.ahead = 10)$pred
AR_forecast_se <- predict(AR_fit, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2*AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2*AR_forecast_se, type = "l", col = 2, lty = 2)
```

Your predictions of River Nile flow from 1971 to 1980 make sense based on the data you have. The relatively wide band of confidence (represented by the dotted lines) is a result of the low persistence in your Nile data.

# 5. A simple moving average

## The Simple Moving Average Model

### Simulate the simple moving average model

The simple moving average (MA) model is a parsimonious time series model used to account for very short-run autocorrelation. It does have a regression like form, but here each observation is regressed on the previous innovation, which is not actually observed. Like the autoregressive (AR) model, the MA model includes the white noise (WN) model as special case.

As with previous models, the MA model can be simulated using the arima.sim() command by setting the model argument to list(ma = theta), where theta is a slope parameter from the interval (-1, 1). Once again, you also need to specifcy the series length using the n argument.

In this exercise, you'll simulate and plot three MA models with slope parameters 0.5, 0.9, and -0.5, respectively.

Instructions
Use arima.sim() to simulate a MA model with the slope parameter set to 0.5, and series length 100. Save this model to x.
Use another call to arima.sim() to simulate a MA model with the slope parameter set to 0.9. Save this model to y.
Use a third call to arima.sim() to simulate a final MA model with the slope paramter set to -0.5. Save this model to z.
Use plot.ts() to display all three models.

```{r}
# Generate MA model with slope 0.5
x <- arima.sim(model = list(ma = 0.5), n = 100)

# Generate MA model with slope 0.9
y <- arima.sim(model = list(ma = 0.9), n = 100)

# Generate MA model with slope -0.5
z <- arima.sim(model = list(ma = -0.5), n = 100)

# Plot all three models together
plot.ts(cbind(x,y,z))
```

Note that there is some very short-run persistence for the positive slope values (x and y), and the series has a tendency to alternate when the slope value is negative (z).

### Estimate the autocorrelation function (ACF) for a moving average

Now that you've simulated some MA data using the arima.sim() command, you may want to estimate the autocorrelation functions (ACF) for your data. As in the previous chapter, you can use the acf() command to generate plots of the autocorrelation in your MA data.

In this exercise, you'll use acf() to estimate the ACF for three simulated MA series, x, y, and z. These series have slope parameters of 0.4, 0.9, and -0.75, respectively, and are shown in the figure on the right.

Instructions
Use three calls to acf() to estimate the autocorrelation functions for x, y, and z, respectively.

```{r}
# Calculate ACF for x
acf(x)

# Calculate ACF for y
acf(y)

# Calculate ACF for z
acf(z)

```

As you can see from your ACF plots, the series x has positive sample autocorrelation at the first lag, but it is approximately zero at other lags. The series y has a larger sample autocorrelation at its first lag, but it is also approximately zero for the others. The series z has an alternating pattern, and its sample autocorrelation is negative at the first lag. However, similar to the others, it is approximately zero for all higher lags.

## MA Model Estimation and Forecasting - Video

### xEstimate the simple moving average model

Now that you've simulated some MA models and calculated the ACF from these models, your next step is to fit the simple moving average (MA) model to some data using the arima() command. For a given time series x we can fit the simple moving average (MA) model using arima(..., order = c(0, 0, 1)). Note for reference that an MA model is an ARIMA(0, 0, 1) model.

In this exercise, you'll practice using a preloaded time series (x, shown in the plot on the right) as well as the Nile dataset used in earlier chapters.

Instructions
Use arima() to fit the MA model to the series x.
What are the slope (ma1), mean (intercept), and innovation variance (sigma^2) estimates produced by your arima() output? Paste these into your workspace.
Use a similar call to arima() to fit the MA model to the Nile data. Save the results as MA and use print() to display the output.
Finally, use the pre-written commands to plot the Nile data and your fitted MA values.

```{r}
x <- 
c(-0.29104202,  0.37763081, -0.41268279,  0.79060856,  2.62597032,  1.95541100,
 1.32147721, -0.56276017, -1.00480197, -1.94491932, -1.30029115, -0.96819303,
-1.62074777, -0.24723380, -0.91119582, -0.03614666,  0.20326806,  0.32343468,
 1.03223134, -0.06602086,  1.10417816,  3.57665937,  1.92456285,  0.25506223,
 0.09188144,  0.83193435,  0.57777123, -1.18909258, -0.92723871, -0.28826927,
 0.09173721, -0.24812302, -1.73867120,  0.59943768,  1.40413384,  1.94216369,
 2.00154846,  2.47319616,  2.00534596, -0.54680647, -0.08531535,  0.05460297,
 1.08002979,  0.09135667,  0.03767858,  1.06184834, -0.57062663, -0.14874580,
-0.29691240, -2.91564128, -0.89162136,  0.06409581, -1.89441014, -0.82051653,
 0.29616018,  1.24526044,  2.07560937,  0.81950605, -0.44521801, -0.61898486,
-0.30817850, -0.77920197, -0.61910425,  0.54089324,  0.31254753, -0.41565724,
-0.63745705, -1.19774636,  0.38173220,  0.01097415, -0.55027499,  0.27150915,
-1.32344173, -1.86513516, -1.99571623,  0.09053037, -1.31823319, -1.26874080,
 0.25936070,  0.98662057,  1.74593866,  1.88010691,  0.43493006, -0.98614375,
 0.22889483,  1.78095941,  3.71337005,  2.01759428, -0.46069542, -1.42215110,
-0.60412149,  1.40543362,  2.35852554,  1.90795579,  2.05187460,  1.57197343,
-0.75491260, -1.39643096, -0.52225049, -0.29842969)

ts.plot(x)
plot.ts(x)
```

```{r}
# Fit the MA model to x
arima(x, order = c(0,0,1))

# Paste the slope (ma1) estimate below
0.7928

# Paste the slope mean (intercept) estimate below
0.1589

# Paste the innovation variance (sigma^2) estimate below
0.9576

# Fit the MA model to Nile
MA <- arima(Nile, order = c(0,0,1))
print(MA)

# Plot Nile and MA_fit 
ts.plot(Nile)
MA_fit <- Nile - resid(MA)
points(MA_fit, type = "l", col = 2, lty = 2)
```

By fitting an MA model to your Nile data, you're able to capture variation in the data for future prediction. Based on the plot you've generated, does the MA model appear to be a strong fit for the Nile data?

### Simple forecasts from an estimated MA model

Now that you've estimated a MA model with your Nile data, the next step is to do some simple forecasting with your model. As with other types of models, you can use the predict() function to make simple forecasts from your estimated MA model. Recall that the $pred value is the forecast, while the $se value is a standard error for that forecast, each of which is based on the fitted MA model.

Once again, to make predictions for several periods beyond the last observation you can use the n.ahead = h argument in your call to predict(). The forecasts are made recursively from 1 to h-steps ahead from the end of the observed time series. However, note that except for the 1-step forecast, all forecasts from the MA model are equal to the estimated mean (intercept).

In this exercise, you'll use the MA model derived from your Nile data to make simple forecasts about future River Nile flow levels. Your MA model from the previous exercise is available in your environment.

Instructions
Use predict() to make a forecast for River Nile flow level in 1971. Store the forecast in predict_MA.
Use predict_MA along with $pred[1] to obtain the 1-step forecast.
Use another call to predict() to make a forecast from 1971 through 1980. To do so, set the n.ahead argument equal to 10.
Run the pre-written code to plot the Nile time series plus the forecast and 95% prediction intervals.

```{r}
# Make a 1-step forecast based on MA
predict_MA <- predict(MA)

# Obtain the 1-step forecast using $pred[1]
predict_MA$pred[1]

# Make a 1-step through 10-step forecast based on MA
predict(MA, n.ahead=10)

# Plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(Nile, xlim = c(1871, 1980))
MA_forecasts <- predict(MA, n.ahead = 10)$pred
MA_forecast_se <- predict(MA, n.ahead = 10)$se
points(MA_forecasts, type = "l", col = 2)
points(MA_forecasts - 2*MA_forecast_se, type = "l", col = 2, lty = 2)
points(MA_forecasts + 2*MA_forecast_se, type = "l", col = 2, lty = 2)
```

Note that the MA model can only produce a 1-step forecast. For additional forecasting periods, the predict() command simply extends the original 1-step forecast. This explains the unexpected horizontal lines after 1971.

## Compare the AR and MA Models - Video

### AR vs MA models

As you've seen, autoregressive (AR) and simple moving average (MA) are two useful approaches to modeling time series. But how can you determine whether an AR or MA model is more appropriate in practice?

To determine model fit, you can measure the Akaike information criterion (AIC) and Bayesian information criterion (BIC) for each model. While the math underlying the AIC and BIC is beyond the scope of this course, for your purposes the main idea is these these indicators penalize models with more estimated parameters, to avoid overfitting, and smaller values are preferred. All factors being equal, a model that produces a lower AIC or BIC than another model is considered a better fit.

To estimate these indicators, you can use the AIC() and BIC() commands, both of which require a single argument to specify the model in question.

In this exercise, you'll return to the Nile data and the AR and MA models you fitted to this data. These models and their predictions for the 1970s (AR_fit) and (MA_fit) are depicted in the plot on the right.

Instructions
 - As a first step in comparing these models, use cor() to measure the correlation between  - AR_fit and MA_fit.
 - Use two calls to AIC() to calculate the AIC for AR and MA, respectively.
 - Use two calls to BIC() to calculate the BIC for AR and MA, respectively.
 
```{r}
# Find correlation between AR_fit and MA_fit
cor(AR_fit, MA_fit)

# Find AIC of AR
AIC(AR)

# Find AIC of MA
AIC(MA)

# Find BIC of AR
BIC(AR)

# Find BIC of MA
BIC(MA)

```

Although the predictions from both models are very similar (indeed, they have a correlation coeffiicent of 0.94), both the AIC and BIC indicate that the AR model is a slightly better fit for your Nile data.

Name that model by time series plot
50xp
Now that you've simulated, fitted, and generated forecasts based on several models, you should have a good idea of what each model looks like and where it is most useful.

A time series from each of the four models we have considered in this course was simulated and they are shown in one of the four panels in the plot on the right. They include the white noise (WN), random walk (RW), autoregressive (AR), and simple moving average (MA) models.

Match each time series plot with one of our models WN, RW, AR, MA.

Possible Answers
(A) WN, (B) RW, (C) AR, (D) MA
(A) MA, (B) AR, (C) RW, (D) WN (Correct)
(A) MA, (B) RW, (C) AR, (D) WN
(A) RW, (B) MA, (C) WN, (D) AR
Take Hint (-15xp)

**Remember that the WN model should display no obvious pattern, the RW and AR models should show strong persistence, and the MA model should show short-run dependence.**

### Name that model by ACF plot

Finally, the ACF plot for each model provides additional useful information about how each model operates (and, therefore, when you should use each model). It is also helpful to be able to identify the model based on the ACF plot.

A time series from each of the four models you have considered in this course was simulated and their sample autocorrelation functions (ACF) are shown in one of the four panels in the adjoining figure. They include the white noise (WN), random walk (RW), autoregressive (AR), and simple moving average (MA) models.

Match each sample ACF plot with one of our models WN, RW, AR, MA.

Possible Answers
(A) MA, (B) RW, (C) AR, (D) WN
(A) RW, (B) MA, (C) WN, (D) AR
(A) WN, (B) RW, (C) AR, (D) MA
(A) MA, (B) AR, (C) RW, (D) WN

Remember that the RW ACF plot is likely to show large autocorrelation for many lags without quick decay to zero.

Remember that the WN ACF plot should show virtually no autocorrelation at all lags.

 Remember that the MA ACF plot should show strong autocorrelation at a lag of 1 but virtually no autocorrelation at all other lags.
 
Plot A shows autocorrelation for the first lag only, which is consistent with the expectations of the MA model. Plot B shows dissipating autocorrelation across several lags, consistent with the AR model. Plot C is consistent with a RW model with considerable autocorrelation for many lags. Finally. Plot D shows virtually no autocorrelation with any lags, consistent with a WN model. Understanding the logic behind these ACF plots is crucial for understanding how each model operates.

```{r}
![Caption for the picture.]("ACF (A) MA, (B) AR, (C) RW, (D) WN.png")
```

